[{"content":"Note: The cover image is from https://xkcd.com/2634/ Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed https://openwrt.org/docs/guide-user/services/tls/acmesh Terminal access to the \u0026lsquo;Plug.\nSteps I knew this process was going to be awkward but I didn\u0026rsquo;t expect it to consume so much of my time.\nEverywhere I read led me to manually install and run an acme.sh script.\nThat I could not get to work.\nHowever with a combination of luci-app-acme and Terminal commands* I finally got there.\n(* I may be doing this all wrong but I think that luci-app-acme doesn\u0026rsquo;t write some config settings correctly; hence the Terminal to amend this and then run the initial certificate generation.)\nInstall The SSL module is not installed by default:\nopkg update\nopkg install lighttpd-mod-openssl\nAdd the acme modules and the LuCI bits:\nopkg install acme luci-app-acme\nThe symlink that acme created in the .well-known directory is wrong, recreate:\nrm -R /www/.well-known/\nmkdir /www/.well-known/\nln -s /var/run/acme/challenge/ /www/.well-known/acme-challenge\nEnable the acme-challenge:\nnano /etc/lighttpd/lighttpd.conf\n# ACME-challenge $HTTP[\u0026#34;url\u0026#34;] =~ \u0026#34;^/[.]well-known/acme-challenge/\u0026#34; { alias.url += ( \u0026#34;/.well-known/acme-challenge\u0026#34; =\u0026gt; \u0026#34;/www/.well-known/acme-challenge/\u0026#34; ) } Restart:\nservice lighttpd restart\nservice lighttpd status\nCheck that the acme-challenge is working as intended:\nnano /var/run/acme/challenge/README.txt\n(Insert some text here, like \u0026lsquo;Oh, hello.\u0026rsquo;.)\nIn a different Terminal:\ncurl -L -v http://\u0026lt;DOMAIN_NAME\u0026gt;/.well-known/acme-challenge/README.txt\nIn LuCI:\nhttp://\u0026lt;PLUG_IP\u0026gt;/luci\nServices | ACME certificates\nACME global config\nAccount email - \u0026lt;EMAIL_ADDRESS\u0026gt;\nSave \u0026amp; Apply\nCertificate config\n\u0026lt;DOMAIN_NAME\u0026gt; | Add\n(Needs underscores, not full stops.)\nPop-up box:\nACME certificates - \u0026lt;DOMAIN_NAME\u0026gt;\nTab - General Settings\nEnabled - tick\nDomain names - \u0026lt;DOMAIN_NAME\u0026gt;\nValidation method - Webroot\nTab - Webroot Challenge Validation\nWebroot directory - /www/ (Same as \u0026lsquo;var.server_root\u0026rsquo; in the lighttpd.conf file.)\nTab - Advanced Settings\nKey type - RSA 2048 bits\nSave\nSave \u0026amp; Apply\nRestart the web server and acme processes:\nservice lighttpd restart\n/etc/init.d/acme restart\nAt this point it might be wise to cast an eye over these files, especially the e-mail address:\nnano /etc/config/acme\nnano /etc/acme/ca/acme-v02.api.letsencrypt.org/directory/ca.conf\nnano /etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/\u0026lt;DOMAIN_NAME\u0026gt;.conf\nRestart the server and acme process if any changes are made.\nTrigger a renewal process:\n/etc/init.d/acme renew\nNote: If a certificate has previously been generated, using the same details, then that certificate will be retrieved and a new one will not be created.\nResult:\nCert success. Your cert is in: /etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/\u0026lt;DOMAIN_NAME\u0026gt;.cer Your cert key is in: /etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/\u0026lt;DOMAIN_NAME\u0026gt;.key The intermediate CA cert is in: /etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/ca.cer And the full-chain cert is in: /etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/fullchain.cer Running renew hook: \u0026#39;/usr/lib/acme/notify renewed\u0026#39; Add the newly created certificate to the vHost configuration.\nnano /etc/lighttpd/vhosts.d/\u0026lt;DOMAIN_NAME\u0026gt;.conf\nFull working example below:\n# Site \u0026lt;DOMAIN_NAME\u0026gt; $HTTP[\u0026#34;host\u0026#34;] == \u0026#34;\u0026lt;DOMAIN_NAME\u0026gt;\u0026#34; { server.document-root = \u0026#34;/www/sites/\u0026lt;DOMAIN_NAME\u0026gt;\u0026#34; server.errorlog = \u0026#34;/var/log/lighttpd/\u0026lt;DOMAIN_NAME\u0026gt;.error.log\u0026#34; accesslog.filename = \u0026#34;/var/log/lighttpd/\u0026lt;DOMAIN_NAME\u0026gt;.access.log\u0026#34; $SERVER[\u0026#34;socket\u0026#34;] == \u0026#34;:443\u0026#34; { ssl.engine = \u0026#34;enable\u0026#34; ssl.pemfile = \u0026#34;/etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/combined.cer\u0026#34; } } Restart the web server and acme processes:\nservice lighttpd restart\n/etc/init.d/acme restart\ncrontab Run the renew process daily at midnight to update any certs as needed:\ncrontab -e\n# Renew the acme certs daily at midnight 0 0 * * * /etc/init.d/acme renew Redirecting from HTTP to HTTPS Add mod_redirect.\nnano /etc/lighttpd/lighttpd.conf\n# Added by me # Modules server.modules += ( \u0026#34;mod_alias\u0026#34;, \u0026#34;mod_redirect\u0026#34; ) Add the redirect to the vHosts.\nnano /etc/lighttpd/vhosts.d/\u0026lt;DOMAIN_NAME\u0026gt;.conf\nFull working example below:\n# Site \u0026lt;DOMAIN_NAME\u0026gt; $HTTP[\u0026#34;host\u0026#34;] == \u0026#34;\u0026lt;DOMAIN_NAME\u0026gt;\u0026#34; { server.document-root = \u0026#34;/www/sites/\u0026lt;DOMAIN_NAME\u0026gt;\u0026#34; server.errorlog = \u0026#34;/var/log/lighttpd/\u0026lt;DOMAIN_NAME\u0026gt;.error.log\u0026#34; accesslog.filename = \u0026#34;/var/log/lighttpd/\u0026lt;DOMAIN_NAME\u0026gt;.access.log\u0026#34; $SERVER[\u0026#34;socket\u0026#34;] == \u0026#34;:443\u0026#34; { ssl.engine = \u0026#34;enable\u0026#34; ssl.pemfile = \u0026#34;/etc/acme/\u0026lt;DOMAIN_NAME\u0026gt;/combined.cer\u0026#34; } $SERVER[\u0026#34;socket\u0026#34;] == \u0026#34;:80\u0026#34; { $HTTP[\u0026#34;host\u0026#34;] =~ \u0026#34;(.*)\u0026#34; { url.redirect = ( \u0026#34;^/(.*)\u0026#34; =\u0026gt; \u0026#34;https://%1/$1\u0026#34; ) } } } Conclusion # BEWARE #\nToo many unsuccessful certificate creation attempts will result in being locked out for an hour.\nhttps://letsencrypt.org/docs/rate-limits/#authorization-failures-per-identifier-per-account Too many successful (5) certificate creations will result in a lockout of a day.\nhttps://letsencrypt.org/docs/rate-limits/#new-certificates-per-exact-set-of-identifiers So, don\u0026rsquo;t do what I did and generate a working cert, then delete / uninstall everything and repeatedly try again to ensure the documented steps were correct.\nNotes (stuff I found out later) Removing acme:\nservice acme stop\nopkg remove acme* --force-depends\nopkg remove luci-app-acme\nopkg remove lighttpd-mod-openssl\nrm -R /etc/acme*\nrm /etc/config/acme\nEnsure any rows regarding $SERVER[\u0026ldquo;socket\u0026rdquo;] == \u0026ldquo;:443\u0026rdquo; are either removed or commented out in:\nnano /etc/lighttpd/vhosts.d/\u0026lt;DOMAIN_NAME\u0026gt;.conf\nservice lighttpd restart\nservice lighttpd status\n","permalink":"https://boffinsblog.github.io/posts/configuring-lighttpd-tls-and-acme-on-a-sheevaplug/","summary":"Configuring lighttpd on a SheevaPlug.","title":"Configuring lighttpd TLS and ACME on a SheevaPlug"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed As I was tinkering once again, ruminating on how to use my SheevaPlug as a web server*, I thought I\u0026rsquo;d put some notes together around the process.\n* This is actually something I\u0026rsquo;ve been thinking about for a while, for a couple of reasons:\nA separate dedicated web server is never a bad idea\nIt means that I can take the main server offline and perform updates without bringing my websites down\nResources https://openwrt.org/docs/guide-user/services/webserver/lighttpd https://openwrt.org/docs/guide-user/luci/luci.on.lighttpd Steps Stop uhttpd service uhttpd stop\nInstall lighttpd Install lighttpd and the LuCI packages:\nsu\nopkg update\nopkg install lighttpd lighttpd-mod-cgi luci-mod-admin-full\nAdd a LuCI theme:\nopkg install luci-theme-openwrt\nAdd some optional modules.\nFor logging:\nopkg install lighttpd-mod-accesslog\nStart lighttpd service lighttpd start\nservice lighttpd status\nCreate some directories Content needs a place to live.\nI put all my content under /www/sites/:\nmkdir -p /www/sites/sheevaplug/ mkdir /www/sites/sheevaplug/blog/ mkdir /www/sites/sheevaplug/code/ Configure Configure the landing pages This is the default landing page; the page that will be served when typing the \u0026lsquo;Plug IP into a browser.\nCheck that the cgi-module has been loaded:\nnano /etc/lighttpd/conf.d/30-cgi.conf\nEnsure that this line is there:\nserver.modules += ( \u0026quot;mod_cgi\u0026quot; )\nIn the same file, tell lighttpd to process LuCI CGI requests.\nI always keep original comments and add to them; that way I can revert to a known good prior state easily:\n# Changed by me #cgi.assign = ( \u0026#34;.pl\u0026#34; =\u0026gt; \u0026#34;/usr/bin/perl\u0026#34;, # \u0026#34;.cgi\u0026#34; =\u0026gt; \u0026#34;/usr/bin/perl\u0026#34;, # \u0026#34;.rb\u0026#34; =\u0026gt; \u0026#34;/usr/bin/ruby\u0026#34;, # \u0026#34;.erb\u0026#34; =\u0026gt; \u0026#34;/usr/bin/eruby\u0026#34;, # \u0026#34;.py\u0026#34; =\u0026gt; \u0026#34;/usr/bin/python\u0026#34; ) # Added by me cgi.assign = ( \u0026#34;.pl\u0026#34; =\u0026gt; \u0026#34;/usr/bin/perl\u0026#34;, \u0026#34;.cgi\u0026#34; =\u0026gt; \u0026#34;/usr/bin/perl\u0026#34;, \u0026#34;.rb\u0026#34; =\u0026gt; \u0026#34;/usr/bin/ruby\u0026#34;, \u0026#34;.erb\u0026#34; =\u0026gt; \u0026#34;/usr/bin/eruby\u0026#34;, \u0026#34;.py\u0026#34; =\u0026gt; \u0026#34;/usr/bin/python\u0026#34;, \u0026#34;/cgi-bin/luci\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-backup\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-download\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-exec\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-upload\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, ) LuCI needs root privileges to access configuration, so lighttpd needs to run as root too:\nnano /etc/lighttpd/lighttpd.conf\n# Changed by me #server.username = \u0026#34;http\u0026#34; #server.groupname = \u0026#34;www-data\u0026#34; At the bottom:\n# Added by me # Modules server.modules += ( \u0026#34;mod_alias\u0026#34; ) Check and restart Check the configuration syntax:\nlighttpd -t -f /etc/lighttpd/lighttpd.conf\nRestart:\nservice lighttpd restart\nCheck the status:\nservice lighttpd status\nAdd vHosts As with nginx I am serving several sites on my \u0026lsquo;Plug so will use vHosts.\nCreate a home for the conf files:\nmkdir -p /etc/lighttpd/vhosts.d/\nnano /etc/lighttpd/vhosts.d/\u0026lt;DOMAIN_NAME\u0026gt;.conf\n# Site \u0026lt;DOMAIN_NAME\u0026gt; $HTTP[\u0026#34;host\u0026#34;] == \u0026#34;\u0026lt;DOMAIN_NAME\u0026gt;\u0026#34; { server.document-root = \u0026#34;/www/sites/\u0026lt;DOMAIN_NAME\u0026gt;\u0026#34; server.errorlog = \u0026#34;/var/log/lighttpd/\u0026lt;DOMAIN_NAME\u0026gt;.error.log\u0026#34; accesslog.filename = \u0026#34;/var/log/lighttpd/\u0026lt;DOMAIN_NAME\u0026gt;.access.log\u0026#34; } Load the vHosts nano /etc/lighttpd/lighttpd.conf\n# Include the vHosts include_shell \u0026#34;cat /etc/lighttpd/vhosts.d/*.conf\u0026#34; Check and restart Add PHP opkg update opkg install php8 opkg install php8-cgi opkg install php8-mod-session Edit / update:\nnano /etc/lighttpd/lighttpd.conf\nChange (comment out):\nstatic-file.exclude-extensions = ( \u0026quot;.php\u0026quot;, \u0026quot;.pl\u0026quot;, \u0026quot;.fcgi\u0026quot; )\nTo:\n# Changed by me #static-file.exclude-extensions = ( \u0026#34;.php\u0026#34;, \u0026#34;.pl\u0026#34;, \u0026#34;.fcgi\u0026#34; ) Edit / update:\nnano /etc/php.ini\nChange (comment out):\ndoc_root = \u0026quot;/www\u0026quot;\nTo:\n; Changed by me ;doc_root = \u0026#34;/www\u0026#34; Edit / update:\nnano /etc/lighttpd/conf.d/30-cgi.conf\n# Added by me cgi.assign = ( \u0026#34;.pl\u0026#34; =\u0026gt; \u0026#34;/usr/bin/perl\u0026#34;, \u0026#34;.cgi\u0026#34; =\u0026gt; \u0026#34;/usr/bin/perl\u0026#34;, \u0026#34;.rb\u0026#34; =\u0026gt; \u0026#34;/usr/bin/ruby\u0026#34;, \u0026#34;.erb\u0026#34; =\u0026gt; \u0026#34;/usr/bin/eruby\u0026#34;, \u0026#34;.py\u0026#34; =\u0026gt; \u0026#34;/usr/bin/python\u0026#34;, \u0026#34;.php\u0026#34; =\u0026gt; \u0026#34;/usr/bin/php-cgi\u0026#34;, \u0026#34;/cgi-bin/luci\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-backup\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-download\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-exec\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, \u0026#34;/cgi-bin/cgi-upload\u0026#34; =\u0026gt; \u0026#34;\u0026#34;, ) Check and restart Move LuCI This took far longer and more digging than I anticipated.\nI don\u0026rsquo;t want the \u0026lsquo;Plug homepage to be the LuCI logon page.\n(Yes, the easiest way would be to rename /www/index.html to something like /www/luci.html but what seemed like a simple change was proving to be more challenging and that was annoying me.)\nI wanted http://\u0026lt;PLUG_IP\u0026gt;/luci to be the LuCI logon page.\nCreate the new LuCI home directory:\nmkdir /www/luci/\nMove the existing file:\nmv /www/index.html /www/luci/index.html\nRetain a copy for when I break it:\ncp /www/luci/index.html /www/luci/index.html.old\nThe original LuCI index.html file can be found here:\nhttps://github.com/openwrt/luci/blob/master/modules/luci-base/root/www/index.html Edit:\nnano /www/luci/index.html\nChange:\n\u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0; URL=cgi-bin/luci/\u0026quot; /\u0026gt;\nTo:\n\u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0; URL=/cgi-bin/luci/\u0026quot; /\u0026gt;\nChange:\n\u0026lt;a href=\u0026quot;cgi-bin/luci/\u0026quot;\u0026gt;LuCI - Lua Configuration Interface\u0026lt;/a\u0026gt;\nTo:\n\u0026lt;a href=\u0026quot;/cgi-bin/luci/\u0026quot;\u0026gt;LuCI - Lua Configuration Interface\u0026lt;/a\u0026gt;\nSome of the LuCI controller configurations are hard coded so need changing manually:\nnano /usr/lib/opkg/info/luci-base.list\nChange:\n/www/index.html\nTo:\n/www/luci/index.html\nCreate a new /www/index.html file:\nnano /www/index.html\nThe default lighttpd welcome page can be found here:\nhttps://raw.githubusercontent.com/lighttpd/lighttpd2/refs/heads/master/contrib/default.html Check and restart Restricting LuCI To restrict which IP addresses can now access the http://\u0026lt;PLUG_IP\u0026gt;/luci directory:\nnano /etc/lighttpd/lighttpd.conf\nAdd:\n# Restrict access to /luci $HTTP[\u0026#34;remoteip\u0026#34;] !~ \u0026#34;192.168.0.xx|192.168.0.yy\u0026#34; { $HTTP[\u0026#34;url\u0026#34;] =~ \u0026#34;^/luci/\u0026#34; { url.access-deny = ( \u0026#34;\u0026#34; ) } } Check and restart Hardening (or not as the case may be) The \u0026lsquo;Plug has one job: a web server.\nBy default the \u0026lsquo;Plug firewall only allows traffic through port 22 (SSH), port 80 (HTTP) and port 443 (HTTPS).\nThe \u0026lsquo;Plug sits behind a router which only has ports 80 and 443 open (the web traffic ports).\nFrom outside the LAN the \u0026lsquo;Plug looks like this:\nnmap -F \u0026lt;MY_FIXED_IP\u0026gt; PORT STATE SERVICE 80/tcp open http 443/tcp open https From inside the LAN, the \u0026lsquo;Plug looks like this:\nnmap -F \u0026lt;SHEEVAPLUG_IP\u0026gt; PORT STATE SERVICE 22/tcp open ssh 53/tcp open domain 80/tcp open http dnsmasq Port 53/tcp was being used by dnsmasq, which I stopped and disabled as I have no need of it:\n/etc/init.d/dnsmasq stop\n/etc/init.d/dnsmasq disable\n/etc/init.d/dnsmasq status\nResult!\nnmap -F \u0026lt;SHEEVAPLUG_IP\u0026gt; PORT STATE SERVICE 22/tcp open ssh 80/tcp open http Permissions Take ownership of all of the websites under /www/sites/:\nchown http:www-data -R /www/sites/\nOr if you prefer (depending on rights):\nchown foo:foo -R /www/sites/\nSet file and folder permissions for everything under /www/sites/:\nfind /www/sites/ -type f -exec chmod 644 {} + find /www/sites/ -type d -exec chmod 755 {} + server.tag OK, this is juvenile but it made me smile; changing the server\u0026rsquo;s response header:\nnano /etc/lighttpd/lighttpd.conf\n# Change the server type if queried server.tag = \u0026#34;nunyaBidniss v1.0\u0026#34; Check and restart Now have a look at:\ncurl -I http://\u0026lt;DOMAIN_NAME\u0026gt; | grep -i '^server:'\nRemove uhttpd Remove the now redundant uhttpd:\nopkg remove uhttpd --force-removal-of-dependent-packages\nAnd any orphaned config file:\nrm /etc/config/uhttpd\nConclusion OpenWrt does a spectacular job of running lighttpd and hosting a couple of simple sites.\nNothing fancy; flat HTML, CSS and some PHP.\nTime to add SSL to the mix.\nNotes (stuff I found out later) To see the output of all the config files parsed together:\nlighttpd -p -f /etc/lighttpd/lighttpd.conf\n","permalink":"https://boffinsblog.github.io/posts/installing-lighttpd-on-a-sheevaplug-running-openwrt/","summary":"Installing lighttpd on a SheevaPlug Running OpenWrt.","title":"Installing lighttpd on a SheevaPlug Running OpenWrt"},{"content":"So, I used to have a website.\nA proper domain, mail server, everything.\nThen, one day I decided the upkeep was all a bit too much .\nSo I binned it and decided to host my humble pages on my ISP provided fixed IP.\nIt kinda worked but not very well.\n(Some things just won\u0026rsquo;t work without a domain.)\nSo now I have a new plan.\nI bought a cheap £5 (for the first year) domain.\nGrabbed a matching email.\nOpened a matching GitHub account.\nNow, when the domain comes up for renewal and it\u0026rsquo;s silly money, I can relocate the domain content to GitHub Pages .\n(I\u0026rsquo;d already moved from WordPress to Hugo some time ago and apparently Hugo can be hosted on GitHub Pages quite easily.)\nI realised that I had been getting hung up on naming; losing a memorable domain name.\n(I still kinda regret binning that last one; it was a good name.)\nAnd then I thought, what if I just leave the GitHub Pages there?\nUntouched, abandoned?\nMaybe even delete the GitHub keys and associated email account.\nI would never be able to update that bit of the Internet again.\nIt would be left, like a forgotten building in Microsoft\u0026rsquo;s city.\nI could start afresh, get another £5 (for the first year) domain.\nCreate the email and GitHub accounts.\nRinse and repeat.\nI would become a blogging vandal.\nLeaving rubble on other peoples infra.\n","permalink":"https://boffinsblog.github.io/posts/digital-rubble/","summary":"Becoming Acquainted with_the Interwebs Again.","title":"Digital Rubble"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed Steps Ensure that the Debian Bookworm system is fully updated.\nI ran through this process immediately after installing Debian Bookworm so I didn\u0026rsquo;t expect there would be much to upgrade but did it anyway.\nAs this upgrade was done on top of a fresh installation there was nothing for me to backup, nor were there additional PPA\u0026rsquo;s and repositories to worry about updating.\nYMMV following my steps.\nNote: During the upgrade processes there are in-Terminal messages, keep an eye on the screen.\nBecome a superuser:\nsu\nPerform a full upgrade:\napt update\napt upgrade\napt full-upgrade\nClean any leftover packages and reboot:\napt --purge autoremove\nreboot\nUpdate the Bookworm repositories to Trixie:\nsed -i 's/bookworm/trixie/g' /etc/apt/sources.list\nUpdate the repository use the newly added repositories:\napt update\nThe Debian documentation says the recommended way to handle the upgrades is in two (2) separate steps:\nMinimal system upgrade\nFull system upgrade\nFirst, upgrade all currently installed packages to their latest versions but without installing any new packages:\napt upgrade --without-new-pkgs\nThen, once the minimal system upgrade completes, perform a full upgrade:\napt full-upgrade\nThere were quite a few warning messages:\ndpkg: warning: unable to delete old directory\nNothing catastrophic that prevents the \u0026lsquo;Plug from booting and running.\nConclusion It may be prudent to wait until there are Trixie installer images for the \u0026lsquo;Plug but in the meantime this seems to be running OK.\n","permalink":"https://boffinsblog.github.io/posts/upgrading-debian-bookworm-to-trixie-on-a-sheevaplug-mmc/","summary":"Upgrading Debian Bookworm to Trixie on a SheevaPlug MMC.","title":"Upgrading Debian Bookworm to Trixie on a SheevaPlug MMC"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed To install Debian Bookworm (which has Long Term Support (LTS), until June 30th, 2028) to the SheevaPlug MMC and boot.\nTFTP server A TFTP server is a lot easier that messing about with files on a USB.\nTo install a TFTP server run the following in a Terminal:\nsudo apt install tftpd-hpa\nThen, check that it is running:\nsudo systemctl status tftpd-hpa\nThen, take ownership of the TFTP server directory:\nsudo chown -R $USER /srv/tftp\nAny files now placed in this directory will be available to pull onto the ‘Plug.\nA working u-boot The following process assumes that there is a working u-boot in place, and that the SheevaPlug can actually boot.\nThere is a post about unbricking a SheevaPlug if that needs to be done prior to following these steps.\nThere is also a post about installing u-boot if the u-boot needs updating.\nResources https://ftp.debian.org/debian/dists/bookworm/main/installer-armel/current/images/kirkwood/netboot/marvell/sheevaplug/ The following two (2) files need to be downloaded and placed in the TFTP server directory:\nhttps://ftp.debian.org/debian/dists/bookworm/main/installer-armel/current/images/kirkwood/netboot/marvell/sheevaplug/uImage https://ftp.debian.org/debian/dists/bookworm/main/installer-armel/current/images/kirkwood/netboot/marvell/sheevaplug/uInitrd Steps Boot the ‘Plug Power on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nprintenv\nIf needed set:\nsetenv serverip \u0026lt;Host_IP\u0026gt; setenv ipaddr \u0026lt;SheevaPlug_IP\u0026gt; setenv ethaddr \u0026lt;SheevaPlug_MAC\u0026gt; setenv macaddr \u0026lt;SheevaPlug_MAC\u0026gt; Install Tell the ‘Plug to use the files on the TFTP server:\ntftpboot 0x00800000 uImage tftpboot 0x01100000 uInitrd Set the environment variables and start the installer:\nsetenv bootargs console=ttyS0,115200n8 base-installer/initramfs-tools/driver-policy=most bootm 0x00800000 0x01100000 The installation itself is fairly straightforward and anyone who has used a text based Debian installer will feel at home going through the steps.\nAt the end of the installation process the \u0026lsquo;Plug will need rebooting.\nConfigure u-boot - Part I - boot from MMC Power on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nThen set the environment variables:\nsetenv bootargs_console console=ttyS0,115200 setenv bootcmd_mmc \u0026#39;mmc init; ext2load mmc 0:1 0x00800000 /uImage; ext2load mmc 0:1 0x01100000 /uInitrd\u0026#39; setenv bootcmd \u0026#39;setenv bootargs $(bootargs_console); run bootcmd_mmc; bootm 0x00800000 0x01100000\u0026#39; saveenv Reboot:\nrun bootcmd\nConfigure u-boot - Part II - boot from NAND. Power on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nThe easiest way I found revert to booting from NAND was to reset all the environment variables:\nenv default -a\nThen set the environment variables:\nsetenv serverip \u0026lt;Host_IP\u0026gt; setenv ipaddr \u0026lt;SheevaPlug_IP\u0026gt; setenv ethaddr \u0026lt;SheevaPlug_MAC\u0026gt; setenv macaddr \u0026lt;SheevaPlug_MAC\u0026gt; saveenv Reboot:\nrun bootcmd\nConclusion Being able to boot from an SD card has its advantages (usable space not being the least of them).\nThe Debian installation can also be updated to the latest release, Trixie .\nHowever OpenWrt is significantly faster than Debian.\nSo even though Debian has a greater breadth of repositories and software, OpenWrt is a clear winner when looking for an OS to perform simple tasks 24/7 (like a web server ).\n","permalink":"https://boffinsblog.github.io/posts/installing-debian-bookworm-on-a-sheevaplug-mmc/","summary":"Installing Debian Bookworm on a SheevaPlug MMC.","title":"Installing Debian Bookworm on a SheevaPlug MMC"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed These are some basic configuration steps; setting a hostname, a fixed IP and creating a hardened login using SSH keys.\nAssumptions: The \u0026lsquo;Plug boots and is wired to the network.\nSteps Find the SheevaPlug IP on the network, I ‘ Nmap ‘:\nsudo nmap -sn \u0026lt;ROUTER_IP\u0026gt;/24\nNmap scan report for \u0026lt;SHEEVAPLUG_IP\u0026gt; Host is up (0.0016s latency). MAC Address: 00:50:43:01:63:EA (Marvell Semiconductor) Login via SSH ssh root@\u0026lt;SHEEVAPLUG_IP\u0026gt;\nChange the password passwd\nUpdate the firmware cd /tmp\nDownload:\nwget https://downloads.openwrt.org/releases/24.10.2/targets/kirkwood/generic/openwrt-24.10.2-kirkwood-generic-globalscale_sheevaplug-squashfs-sysupgrade.bin\nInstall:\nsysupgrade openwrt-24.10.2-kirkwood-generic-globalscale_sheevaplug-squashfs-sysupgrade.bin\nThe \u0026lsquo;Plug will reboot after the update.\nUpdate existing packages OpenWrt uses the opkg package management system.\nOpkg is a full package manager for the root file system, including kernel modules and drivers\nhttps://openwrt.org/docs/guide-user/additional-software/opkg Run the following in a Terminal to update:\nopkg update\nThen, update all of the installed packages:\nopkg list-upgradable | cut -f 1 -d ' ' | xargs -r opkg upgrade\nNote: It\u0026rsquo;s also possible to update everything from the LuCI web interface (http://\u0026lt;SHEEVAPLUG_IP\u0026gt;) but as there is a connection open and available I choose to update here.\nInstall a few extra packages These are pretty much the packages that I use all the time:\nbtop for viewing running processes mc or Midnight Commander for a visual file manager nano for editing text files openssh-sftp-server for connecting to the SheevaPlug and mounting the device rsync for receiving files on the SheevaPlug Run the following in a Terminal to update:\nopkg update\nThen, install the packages:\nopkg install btop\nopkg install mc\nopkg install nano\nopkg install openssh-sftp-server\nopkg install rsync\nSet the hostname Run the following in a Terminal to edit the config file:\nnano /etc/config/system\nChange:\noption hostname 'OpenWrt'\nTo:\noption hostname 'sheevaplug'\nThen restart:\nreboot\nSet a fixed IP Run the following in a Terminal to edit the config file:\nnano /etc/config/network\nChange where needed / appropriate for your network:\nconfig interface \u0026#39;lan\u0026#39; option device \u0026#39;br-lan\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;\u0026lt;SheevaPlug_IP\u0026gt;\u0026#39; option netmask \u0026#39;\u0026lt;Netmask_IP\u0026gt;\u0026#39; option gateway \u0026#39;\u0026lt;Gateway_IP\u0026gt;\u0026#39; option broadcast \u0026#39;\u0026lt;Broadcast_IP\u0026gt;\u0026#39; list dns \u0026#39;\u0026lt;DNS_IP\u0026gt;\u0026#39; Restart networking:\n/etc/init.d/network restart\nCheck using:\nifconfig\nInstall the packages needed to create new users https://openwrt.org/docs/guide-user/additional-software/create-new-users This is to be able to add a non-root user foo who can su.\nRun the following in a Terminal to update:\nopkg update\nThen, install the packages:\nopkg install shadow-useradd shadow-su\nCreate a new foo user useradd -m -s /bin/ash foo\nSet the new user password passwd foo\nExit the root account The following ssh steps need to be executed as the newly created foo user, so exit the root account.\nsu foo\ncd\nAdd the SSH Keys https://openwrt.org/docs/guide-user/security/dropbear.public-key.auth Firstly, make a .ssh dir for foo:\nmkdir -m700 /home/foo/.ssh\nAdd the key SHEEVAPLUG_SSH_KEY.pub:\nnano /home/foo/.ssh/authorized_keys\nSet the authorized_keys permissions:\nchmod 600 /home/foo/.ssh/authorized_keys\nRestart the SSH service su\nservice log restart; service dropbear restart\nTest that you can connect to the ‘Plug from another machine ssh -o PasswordAuthentication=no sheevaplug\nAnd:\nssh -o PasswordAuthentication=no -o PubkeyAcceptedKeyTypes=ssh-rsa sheevaplug\nShould connect to the \u0026lsquo;Plug.\nHarden security Disable password authentication uci set dropbear.@dropbear[0].PasswordAuth=\u0026quot;0\u0026quot;\nDisable logging in with root privileges uci set dropbear.@dropbear[0].RootPasswordAuth=\u0026quot;0\u0026quot;\nCommit the changes uci commit dropbear\nThen, restart service dropbear restart\nTest SSH Security Logging in as a normal user:\nssh foo@\u0026lt;SheevaPlug_IP\u0026gt;\nShould fail with a message like this:\nfoo@\u0026lt;SheevaPlug_IP\u0026gt;: Permission denied (publickey).\nLogging in as a root user:\nssh root@\u0026lt;SheevaPlug_IP\u0026gt;\nShould fail with a message like this:\nroot@\u0026lt;SheevaPlug_IP\u0026gt;: Permission denied (publickey).\nHowever, logging in using the SSH key should work:\nssh sheevaplug\nUser foo can then su with a password.\nTweak the new user profile I\u0026rsquo;m not a fan of vim as the editor so I change the default to nano.\nhttps://openwrt.org/docs/guide-user/base-system/user.beginner.cli#profile_customization su\nnano /etc/profile\n# Added by me # Change the default editor export EDITOR=\u0026#34;nano\u0026#34; # Set the LOCALE (btop complains if this is not set) export LC_CTYPE=en_GB.UTF-8 Conclusion The \u0026lsquo;Plug now has a working, up-to-date, OpenWrt installation with a hostname, a fixed IP and a non-root login using SSH keys.\nTime to install a web server .\n","permalink":"https://boffinsblog.github.io/posts/configuring-openwrt-on-a-sheevaplug/","summary":"Configuring OpenWrt on a SheevaPlug.","title":"Configuring OpenWrt on a SheevaPlug"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed To install the lightweight Operating System OpenWrt to the SheevaPlug NAND and boot.\nTFTP server A TFTP server is a lot easier that messing about with files on a USB.\nTo install a TFTP server run the following in a Terminal:\nsudo apt install tftpd-hpa\nThen, check that it is running:\nsudo systemctl status tftpd-hpa\nThen, take ownership of the TFTP server directory:\nsudo chown -R $USER /srv/tftp\nAny files now placed in this directory will be available to pull onto the ‘Plug.\nA working u-boot The following process assumes that there is a working u-boot in place, and that the SheevaPlug can actually boot.\nThere is a post about unbricking a SheevaPlug if that needs to be done prior to following these steps.\nThere is also a post about installing u-boot if the u-boot needs updating.\nResources https://downloads.openwrt.org/releases/24.10.2/targets/kirkwood/generic/ https://downloads.openwrt.org/releases/24.10.2/targets/kirkwood/generic/openwrt-24.10.2-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin https://downloads.openwrt.org/releases/24.10.2/targets/kirkwood/generic/u-boot-sheevaplug/u-boot.kwb Steps Boot the ‘Plug Power on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nThen boot the ‘Plug from the OpenWrt .bin file:\ntftpboot openwrt-24.10.2-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin\n=\u0026gt; tftpboot openwrt-24.10.2-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin Using egiga0 device TFTP from server \u0026lt;Host_IP\u0026gt;; our IP address is \u0026lt;SheevaPlug_IP\u0026gt; Filename \u0026#39;openwrt-24.10.2-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin\u0026#39;. Load address: 0x800000 Loading: ################################################################# ################################################################# ################################################################# ################################################################# ################################################################# ################################################################# ####################################### 550.8 KiB/s done Bytes transferred = 6291456 (600000 hex) Now, erase the part of NAND where OpenWrt is to be installed:\nnand erase.part ubi\n=\u0026gt; nand erase.part ubi NAND erase.part: device 0 offset 0x100000, size 0x1ff00000 Skipping bad block at 0x169c0000 Erasing at 0x1ffe0000 -- 100% complete. OK Then, write to those sectors:\nnand write 0x800000 ubi 0x600000\n=\u0026gt; nand write 0x800000 ubi 0x600000 NAND write: device 0 offset 0x100000, size 0x600000 6291456 bytes written: OK Finally, reboot the ‘Plug (without interrupting):\nreset\nAfter booting there should be the OpenWrt prompt.\nOpenWrt BusyBox Prompt\nConclusion Following the above will produce a vanilla OpenWrt installation.\nNext, configuration tweaks such as setting the hostname, creating a fixed IP etc.\nNotes (stuff I found out later) To factory reset:\nfirstboot \u0026amp;\u0026amp; reboot\n","permalink":"https://boffinsblog.github.io/posts/installing-openwrt-on-a-sheevaplug-nand/","summary":"Installing OpenWrt on a SheevaPlug NAND.","title":"Installing OpenWrt on a SheevaPlug NAND"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed The NAND contains three partitions:\nMTD0 – u-boot, think of it as the grub of embedded systems MTD1 – uImage, the kernel MTD2 – rootfs, the filesystem u-boot, processes the kernel, which then boots the filesystem (and boots).\nTFTP server A TFTP server is a lot easier that messing about with files on a USB.\nTo install a TFTP server run the following in a Terminal:\nsudo apt install tftpd-hpa\nThen, check that it is running:\nsudo systemctl status tftpd-hpa\nThen, take ownership of the TFTP server directory:\nsudo chown -R $USER /srv/tftp\nAny files now placed in this directory will be available to pull onto the ‘Plug.\nThe u-boot files Das U-Boot (subtitled “the Universal Boot Loader” and often shortened to U-Boot; see History for more about the name) is an open-source boot loader used in embedded devices to perform various low-level hardware initialization tasks and boot the device’s operating system kernel.\nhttps://en.wikipedia.org/wiki/Das_U-Boot This process assumes that the new u-boot files are on the TFTP server.\nThe OpenWrt version is here .\n(This is the one that I use.)\nThe Bookworm version provided by Debian is here .\nSteps Power on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nThen, type the following to set the TFTP environment variables:\nsetenv serverip \u0026lt;Host_IP\u0026gt; setenv ipaddr \u0026lt;SheevaPlug_IP\u0026gt; Set the MAC address:\nsetenv ethaddr \u0026lt;SheevaPlug_MAC\u0026gt; setenv macaddr \u0026lt;SheevaPlug_MAC\u0026gt; Now:\nsaveenv saveenv stores the variables so they will not need setting again.\nBoot from the updated u-boot on the TFTP server:\ntftpboot 0x0800000 u-boot.kwb Then:\nnand erase 0x0 0x80000 nand write 0x0800000 0x0 0x80000 This erases a the portion of NAND reserved for u-boot and writes the new version.\nNow restart the \u0026lsquo;Plug:\nreset The ‘Plug will now reboot with an updated u-boot.\nConclusion Now that the \u0026lsquo;Plug is booting and has an up to date u-boot, it\u0026rsquo;s time to install an OS.\nI\u0026rsquo;ve documented two (2) routes:\nInstalling OpenWrt on NAND Installing Debian Bookworm on MMC Notes (stuff I found out later) To reset the envvars:\nenv default -a\n","permalink":"https://boffinsblog.github.io/posts/installing-das-u-boot-on-a-sheevaplug-nand/","summary":"Installing Das U-Boot on a SheevaPlug NAND.","title":"Installing Das U-Boot on a SheevaPlug NAND"},{"content":"Introduction A short series of SheevaPlug posts around the installation and configuration of different Operating Systems and additional software.\nUnbricking and booting\nUnbricking Installing Das U-Boot Using OpenWrt\nInstalling OpenWrt on the NAND Configuring OpenWrt Using Debian\nInstalling Debian Bookworm on an MMC Upgrading Debian Bookworm to Trixie Additional OpenWrt software\nInstalling a lighttpd web server Configuring lighttpd TLS and ACME Files and Tools Needed You may think your ‘Plug is dead, but if you can connect it into a computer you can resurrect it.\nOpen On-Chip Debugger To install openoc run the following in a Terminal:\nsudo apt install openocd\nOther Tools Install some needed tools:\nsudo apt install telnet screen\nu-boot.elf A valid u-boot.elf needs to be in the host home (~/) directory.\nI have an ancient version archived away but there may be a newer version on the Internet somewhere.\nSteps Open three (3) Terminals.\nI use Terminator which allows for window tiling.\nIn Terminal 1 run the following:\nscreen /dev/ttyUSB0 115200\nThen, in Terminal 2 run the following:\nsudo openocd -f /usr/share/openocd/scripts/board/sheevaplug.cfg -s /usr/share/openocd/scripts\nNow, in Terminal 3 run the following:\ntelnet localhost 4444\nThen:\nreset;sheevaplug_init;load_image u-boot.elf;resume 0x00600000\nBack in Terminal 1 something is happening!\nConclusion The \u0026lsquo;Plug should now boot but u-boot may need updating .\n","permalink":"https://boffinsblog.github.io/posts/unbricking-a-sheevaplug/","summary":"Unbricking a SheevaPlug.","title":"Unbricking a SheevaPlug"},{"content":"My AIY box stopped working.\nGoogle stopped talking to me.\nFor a short while I could regenerate a temporary token but the token lasted a mere seven days.\nSo, inevitably, when I shouted at the box to set a timer or something equally trivial, it didn\u0026rsquo;t respond.\nRegularly resetting the token was a faff and frequently forgotten, so the box fell into disuse.\nGetting the box running properly again was added to my list of things to do.\nThe Past Back in the distant past authorising Google AIY was a simple-ish process:\nGo to the Google cloud console\nCreate a Project\nEnable the Google Assistant API\nCreate some API credentials\nDownload a JSON file and save it to the Pi as assistant.json\nLaunch the python aiy script in a Terminal on the Pi (and wait)\nThe Terminal will prompt visiting a URL\nGo to the URL, agree and wait for the Authorisation response code\nPaste said code in the waiting Terminal\nGo through the Authorisation acceptance steps\n(OK, not that simple but a fairly straightforward set of steps.)\nThe above process is known as OOB or \u0026lsquo;Out of Band\u0026rsquo; .\nIt has been deprecated in favour of a more secure OAuth flow.\n(I think it has something to do with an access token being part of the response URL but I could be wrong about that.)\nThe Present Google has stopped using OOB completely (even temporary tokens are unavailable) and they now enforce OAuth.\nThe authorisation process is similar but somewhat more involved:\nGo to the Google cloud console\nCreate a Project\nAdd tld.domain to \u0026lsquo;Branding | Authorised domains\u0026rsquo; Enable the Google Assistant API\nCreate some API credentials:\nIn Clients\nCreate Client (Create OAuth client ID)\nApplication Type: Desktop app\nName: SomeName\nClick \u0026lsquo;Create\u0026rsquo;\nCopy the Client ID from the pop-up dialogue\nCopy the Client Secret from the pop-up dialogue\nDownload a JSON file and save it to the Pi as assistant.json\nCreate a tld.domain/auth folder on a web server\nCreate a tld.domain/auth/index.php file from the OAuth code below\nCopy:\nClient ID\nClient Secret\nInto tld.domain/auth/index.php\nLaunch the python aiy script in a Terminal on the Pi (and wait)\nThe Terminal will prompt visiting a URL\nGo to the URL, agree and wait for the Authorisation response code\nPaste said code in the waiting Terminal\nGo through the Authorisation acceptance steps\nThe Code As I was reading about how to configure OAuth I came across this site .\nThe whole site is well worth a read as it gives an in depth overview of how OAuth works.\nEmbedded within that site is a link to this repo .\nThe following code is a direct copy of the Google OAuth example from the that repo and is duplicated here for posterity purposes.\nGoogle OAuth \u0026lt;?php // Fill these out with the values you got from Google $googleClientID = \u0026#39;\u0026#39;; $googleClientSecret = \u0026#39;\u0026#39;; // This is the URL we\u0026#39;ll send the user to first to get their authorization $authorizationEndpoint = \u0026#39;https://accounts.google.com/o/oauth2/v2/auth\u0026#39;; // This is Google\u0026#39;s OpenID Connect token endpoint $tokenEndpoint = \u0026#39;https://www.googleapis.com/oauth2/v4/token\u0026#39;; // The URL for this script, used as the redirect URL // If PHP isn\u0026#39;t setting these right you can put the full URL here manually $protocol = isset($_SERVER[\u0026#39;HTTPS\u0026#39;]) \u0026amp;\u0026amp; $_SERVER[\u0026#39;HTTPS\u0026#39;] === \u0026#39;on\u0026#39; ? \u0026#39;https\u0026#39; : \u0026#39;http\u0026#39;; $redirectURL = $protocol . \u0026#39;://\u0026#39; . $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . $_SERVER[\u0026#39;PHP_SELF\u0026#39;]; // Start a session so we have a place to store things between redirects session_start(); // Start the login process by sending the user // to Google\u0026#39;s authorization page if(isset($_GET[\u0026#39;action\u0026#39;]) \u0026amp;\u0026amp; $_GET[\u0026#39;action\u0026#39;] == \u0026#39;login\u0026#39;) { unset($_SESSION[\u0026#39;user_id\u0026#39;]); // Generate a random hash and store in the session $_SESSION[\u0026#39;state\u0026#39;] = bin2hex(random_bytes(16)); $params = array( \u0026#39;response_type\u0026#39; =\u0026gt; \u0026#39;code\u0026#39;, \u0026#39;client_id\u0026#39; =\u0026gt; $googleClientID, \u0026#39;redirect_uri\u0026#39; =\u0026gt; $redirectURL, \u0026#39;scope\u0026#39; =\u0026gt; \u0026#39;openid email\u0026#39;, \u0026#39;state\u0026#39; =\u0026gt; $_SESSION[\u0026#39;state\u0026#39;] ); // Redirect the user to Google\u0026#39;s authorization page header(\u0026#39;Location: \u0026#39; . $authorizationEndpoint . \u0026#39;?\u0026#39; . http_build_query($params)); die(); } if(isset($_GET[\u0026#39;action\u0026#39;]) \u0026amp;\u0026amp; $_GET[\u0026#39;action\u0026#39;] == \u0026#39;logout\u0026#39;) { unset($_SESSION[\u0026#39;user_id\u0026#39;]); header(\u0026#39;Location: \u0026#39;.$redirectURL); die(); } // When Google redirects the user back here, there will be a \u0026#34;code\u0026#34; and \u0026#34;state\u0026#34; // parameter in the query string if(isset($_GET[\u0026#39;code\u0026#39;])) { // Verify the state matches our stored state if(!isset($_GET[\u0026#39;state\u0026#39;]) || $_SESSION[\u0026#39;state\u0026#39;] != $_GET[\u0026#39;state\u0026#39;]) { header(\u0026#39;Location: \u0026#39; . $redirectURL . \u0026#39;?error=invalid_state\u0026#39;); die(); } // Exchange the auth code for a token $ch = curl_init($tokenEndpoint); curl_setopt($ch, CURLOPT_RETURNTRANSFER, true); curl_setopt($ch, CURLOPT_POSTFIELDS, http_build_query([ \u0026#39;grant_type\u0026#39; =\u0026gt; \u0026#39;authorization_code\u0026#39;, \u0026#39;client_id\u0026#39; =\u0026gt; $googleClientID, \u0026#39;client_secret\u0026#39; =\u0026gt; $googleClientSecret, \u0026#39;redirect_uri\u0026#39; =\u0026gt; $redirectURL, \u0026#39;code\u0026#39; =\u0026gt; $_GET[\u0026#39;code\u0026#39;] ])); $response = curl_exec($ch); $data = json_decode($response, true); // Note: You\u0026#39;d probably want to use a real JWT library // but this will do in a pinch. This is only safe to do // because the ID token came from the https connection // from Google rather than an untrusted browser redirect // Split the JWT string into three parts $jwt = explode(\u0026#39;.\u0026#39;, $data[\u0026#39;id_token\u0026#39;]); // Extract the middle part, base64 decode it, then json_decode it $userinfo = json_decode(base64_decode($jwt[1]), true); $_SESSION[\u0026#39;user_id\u0026#39;] = $userinfo[\u0026#39;sub\u0026#39;]; $_SESSION[\u0026#39;email\u0026#39;] = $userinfo[\u0026#39;email\u0026#39;]; // While we\u0026#39;re at it, let\u0026#39;s store the access token and id token // so we can use them later $_SESSION[\u0026#39;access_token\u0026#39;] = $data[\u0026#39;access_token\u0026#39;]; $_SESSION[\u0026#39;id_token\u0026#39;] = $data[\u0026#39;id_token\u0026#39;]; $_SESSION[\u0026#39;userinfo\u0026#39;] = $userinfo; header(\u0026#39;Location: \u0026#39; . $redirectURL); die(); } // If there is a user ID in the session // the user is already logged in if(!isset($_GET[\u0026#39;action\u0026#39;])) { if(!empty($_SESSION[\u0026#39;user_id\u0026#39;])) { echo \u0026#39;\u0026lt;h3\u0026gt;Logged In\u0026lt;/h3\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;p\u0026gt;User ID: \u0026#39;.$_SESSION[\u0026#39;user_id\u0026#39;].\u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;p\u0026gt;Email: \u0026#39;.$_SESSION[\u0026#39;email\u0026#39;].\u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;?action=logout\u0026#34;\u0026gt;Log Out\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;h3\u0026gt;ID Token\u0026lt;/h3\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;pre\u0026gt;\u0026#39;; print_r($_SESSION[\u0026#39;userinfo\u0026#39;]); echo \u0026#39;\u0026lt;/pre\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;h3\u0026gt;User Info\u0026lt;/h3\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;pre\u0026gt;\u0026#39;; $ch = curl_init(\u0026#39;https://www.googleapis.com/oauth2/v3/userinfo\u0026#39;); curl_setopt($ch, CURLOPT_HTTPHEADER, [ \u0026#39;Authorization: Bearer \u0026#39;.$_SESSION[\u0026#39;access_token\u0026#39;] ]); curl_exec($ch); echo \u0026#39;\u0026lt;/pre\u0026gt;\u0026#39;; } else { echo \u0026#39;\u0026lt;h3\u0026gt;Not logged in\u0026lt;/h3\u0026gt;\u0026#39;; echo \u0026#39;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;?action=login\u0026#34;\u0026gt;Log In\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#39;; } die(); } Yes, it\u0026rsquo;s more complicated.\nYes, there are more steps.\nBut my Google box is back to shouting at me again.\nThe Future Google are known for changing stuff.\nI\u0026rsquo;m sure this authorisation process will change too.\nFinally You might also like to read an illustrated guide to OAuth , though by now you are probably fed up with the whole thing.\n","permalink":"https://boffinsblog.github.io/posts/google-aiy-oauth-for-the-voice-hat/","summary":"Finally got my Google AIY box talking again.","title":"Google AIY OAuth for the Voice Hat"},{"content":"Back when Netflix was in the business of sending DVD\u0026rsquo;s in the post, I bought my first Android phone.\nI\u0026rsquo;d had phones before; the ones with proprietary apps and bloatware, but not an Android one.\nWhen I eventually upgraded, my first Android phone became my first custom ROM phone, (running CyanogenMod ).\nI\u0026rsquo;ve been \u0026lsquo;Team Android\u0026rsquo; ever since.\nUsage First, I had to categorise my applications usage.\nNot that taxi app that I last used in 2022 or the parking app that I had to download that time I was on holiday in Bournemouth.\nDetermine which apps I actually use.\nEssential These are that apps that I use throughout the day.\nPhone (duh)\nSignal\nRSS Reader\nMusic Player\nTo-Do List\nEmail\nDesirable Banking\nSSH\nMaps\nWeather\nNFC Payments\nInstallation Options It seems I have three options:\nVanilla Android A bare metal install of the ROM.\nNo GApps.\nApps from F-Droid and the Aurora Store .\nVanilla Android + GApps - Play Store A bare metal install of the ROM.\nGApps so that location etc. function properly.\nApps from F-Droid and the Aurora Store .\nVanilla Android + GApps + Play Store A bare metal install of the ROM.\nGApps so that location etc. function properly.\nA working, signed into Play Store so that GMail etc. work and previous purchases are available.\nInstallation Choices OnePlus have abandoned updating this handset, and I wanted Android 15.\nBesides I like tinkering.\nFor my hardware, a \u0026lsquo;OnePlus 7T Pro\u0026rsquo; only two Custom ROM\u0026rsquo;s seemed available:\nLineageOS And:\ncrDroid Both have a working version of GApps with LineageOS using MindTheGapps and crDroid running NikGapps .\nInstallation Steps Note: to install a custom ROM a few things need to be in place:\nThe bootloader needs to be unlocked\nadb and fastboot need to be installed on the connected computer\nI\u0026rsquo;m not going to cover any of the prerequisites here.\nThey are covered in great detail elsewhere on the Internet.\nBoth ROM installation steps are similar.\nEnable USB Debugging on the phone\nBoot to Bootloader\nadb -d reboot bootloader\nCheck phone is connected fastboot devices\nInstall the Recovery image fastboot flash recovery recovery_filename.img\nReboot to Recovery Press Volume +/- to get to \u0026lsquo;Recovery Mode\u0026rsquo;\nPress Power\nIn Recovery: Factory Reset\nFormat data / factory reset\nFormat data\nSideload the ROM Apply update\nApply from ADB\nadb sideload rom_filename.zip\nReload Recovery if adding GApps\nSideload GApps (or other packages) if needed\nApply update\nApply from ADB\nadb sideload gapps_filename.zip\nReboot (crossing fingers is optional) LineageOS v crDroid Both install.\nBoth boot.\nBoth work fine.\nBoth GApps function properly.\nHowever, my banking app from the Aurora store works under the crDroid GApps version, ( NikGapps ).\nIt doesn\u0026rsquo;t work under the LineageOS GApps version, ( MindTheGapps ).\nThis was expected and there is even a comment about it with the LineageOS installation guide.\nIt is expected as add-ons aren’t signed with LineageOS’s official key!\nSo, What Did I Choose? I tried de-Googling my phone a while back.\nAt that time I contacted a couple of the developers whose app I had paid for via the Play Store.\nSome of them were gracious enough to send me an unlock code so that I could run a full version of their app outside of Google.\nFor me, choosing a non-Play Store installation was an easy choice.\nSo, even though I would prefer to be using LineageOS (for nostalgia reasons really, LineageOS is the successor to CyanogenMod), I\u0026rsquo;m running crDroid + NikGapps.\nNikGapps because I spent hours reading and multiple ways to get GPS working to no avail.\nmicroG looked promising, maybe LineageOS for microG will eventually support my handset.\nEverything Changes A different (non-Googles) way of doing things.\nConnectBot instead of JuiceSSH.\nI should probably just bite the bullet and learn how to Termux .\nOsmAnd~ instead of Google Maps.\nWith its integrated Wikipedia and traffic, it\u0026rsquo;s almost as good as Maps; I guess it\u0026rsquo;ll take a bit of getting used to.\nQuillpad instead of Google Keep.\nYep, it\u0026rsquo;s a bit of a chore recreating Keep notes but it\u0026rsquo;s a one time thing.\n(I will miss sharing notes with other Google accounts though.)\nThunderbird instead of GMail.\nI\u0026rsquo;ve never really liked the Thunderbird (or its predecessor, K9) UI, however using TB does mean that I can use OpenPGP seamlessly .\nEverything Stays the Same I can still use Feeder which to be fair is my most used application.\nUnobtainium Signal only allow their messaging app to be downloaded through authorised channels (like the Play Store).\nThere is however an apk that can be side-loaded.\nI can\u0026rsquo;t seem to find a Google Wallet replacement.\nI suppose I can always slide my bank card into the phone case.\nI\u0026rsquo;ve used the (paid for) PowerAmp music player for years.\nLooks like I\u0026rsquo;ll be buying it again outside of the Play Store.\nNext? I\u0026rsquo;ve already written about simplifying my digital life .\nI guess that the logical conclusion to this de-Googling is to remove myself from their ecosystem entirely.\nAt this point in time I only really use their mailboxes anyway.\nI have a couple of Domains where, when you send an email to name@domain, it is forwarded to a Google e-mail address.\nIt would be a minor pain to archive those mailboxes and change how name@domain is collected.\nDo-able though.\n","permalink":"https://boffinsblog.github.io/posts/dumping-google/","summary":"The logical next step in simplifying my digital life.","title":"Dumping Google"},{"content":"Note: The cover image is a still from the film \u0026lsquo;Office Space\u0026rsquo; a 1999 American satirical black comedy film written and directed by Mike Judge.\nThe Past About 25 years ago I was sat in a pub with a couple of friends and we were looking at my latest purchase; a digital camera.\nOne of them opined that digitisation would bring about the death of film photography.\nI saw things differently; that we would see boutique printing, that film developing would become an art, a craft to be honed.\nNow, we have online services selling USB uploaded digital prints, with taglines like \u0026lsquo;your phone is where photos go to die\u0026rsquo; and I wonder which one of us was right.\nMore History Back when I was at school we were taught that the Luddites were bone headed vandals who just loved smashing up stuff.\nThis wasn\u0026rsquo;t exactly true.\nThey were pro union, anti child slavery reformers wanting an increased minimum wage and better working conditions.\nTurns out they weren\u0026rsquo;t exactly \u0026lsquo;anti-technology\u0026rsquo; more \u0026lsquo;anti-treating people like shit\u0026rsquo;.\nThey were also looked down on by the Government supporting middle class and put down by the military of the day.\nThe Present Essentially, right now, the Internet is shit Google searches are shit.\nBrowsers are becoming data harvesting machines.\n(Browsers have always been the delivery vehicle for malware and viruses but at least the driver wasn\u0026rsquo;t involved.)\nAI nonsense is fucking everywhere.\nY-combinator \u0026lsquo;successes\u0026rsquo; stories and continued founder adulation, with (but not excluding):\nUber, and its \u0026lsquo;surge pricing\u0026rsquo;\nAirbnb helping devastate economies in pretty, rural villages\nDoorDash and Gusto fuelling the gig economy\nTwitch and the proliferation of streamers (that schoolchildren want to grow up to become)\nI\u0026rsquo;m not even going to get into the subjects of; influencers of various persuasions, the \u0026lsquo;manosphere\u0026rsquo; or incel culture.\nThe Future A new Browser war Someone will create a browser or way of searching the Internet that only returns results from the pre-AI slop era..\nA bit like how Geiger counters have to made using low-background steel; pre-atomic blast metal usually found in ships sunk pre WWII as it contains no radioactive impurities that would hinder radiation detection.\nAd revenues will fall, will Google? One of the biggest budgets in any Company or Brand that create or sell something lies with the the Marketing Departments.\nMarketing Departments send money to Marketing Companies like Google (and Meta) for their Pay per Click (PPC) campaigns.\nMoneybagsMarketingExecs will very quickly get fed up with AI slop usurping their paid for rankings.\nGoogle will lose ad revenue.\nWill Google die?\nProbably not.\nThey\u0026rsquo;ll carry on selling your data to Governments around the world via shady data brokers.\nWe will see the rise of AI advertising That other AI slop generator, Meta are already trying to cut MoneybagsMarketingExec out of their money supply chain by pivoting to AI ads (because they know best what appeases their algorithm).\nMeta are producing these AI ads and selling them (because they know best what appeases their algorithm).\nThese ads will be sold to MoneybagsMarketingExecs Boss who will wonder what MoneybagsMarketingExec is being paid for.\nAI ads, tailored to appease an AI algorithm, marketed on feeds populated by bots.\nIt\u0026rsquo;s an ouroboros of shit.\nAI disruptors will become the norm Similar to how SSH tarpits trap and consume the remote resources of an attacker, AI honeypots exist to trap a scraper in a labyrinth of useless pages.\nCloudflare is already doing this so it\u0026rsquo;s only a matter of time before a smaller project is viable for the self hoster.\nMore people will see value in non-internet related communication It\u0026rsquo;ll be like a glacier, slow but inevitable and on a personal level.\nA human approach to how we use devices.\nSending a text to a single person rather than a group.\nUsing the pocket supercomputer to talk to someone rather than communicating digitally.\nSending postcards when on holiday rather than updating social media.\nI dunno, maybe even writing a letter or two.\nSounds Radical ? Good, it should.\n","permalink":"https://boffinsblog.github.io/posts/techno-luddites/","summary":"Becoming less dependant on tech or at least using it humanly and a few predictions.","title":"Techno Luddites"},{"content":"Why TL;DR I want to read more books.\nThe Past 2024 was the year that I ended a bunch of relationships I was looking at my list of internet logins and thought \u0026ldquo;why the fuck do I have an account with a shop I\u0026rsquo;ve not used for years?\u0026rdquo;\nSo I set about terminating unused or unwanted accounts.\nWith various degrees of success.\nSome account terminations were utterly straightforward, some required a couple of email prompts, others GDPR mails, whist others had simply vanished from the internet.\n(As of writing this I\u0026rsquo;ve gone down from c.200 accounts to c.20.)\nThe Present 2025 is the year I rid myself of unused services (Mainly the ones I\u0026rsquo;ve inflicted upon myself.)\nI have gone through various installations and services over the years*.\nThe server that lives under the telly was running a whole bunch of stuff.\nIn no particular order:\nDifferent flavours of webserver (nginx and lighttpd)\nWebserver traffic analysis tools ( GoAccess and AWStats )\nGraphical packages for traffic analysis ( Grafana , Prometheus and NodeExporter )\nVarious Fediverse social media installations ( Calckey , Firefish , Iceshrimp , Iceshrimp.NET and GoToSocial )\nA Git server (later moved to Forgejo )\nAn RSS server ( FreshRSS )\nA streaming music server ( Navidrome )\nA media server ( Jellyfin )\nPi-hole DNS server ( Unbound )\nVPN ( PiVPN and WireGuard )\nMail server (Postfix)\nIMAP and POP3 server (Dovecot)\nAn FTP server\nBots (*Not a comprehensive list; simply the ones I can remember as I\u0026rsquo;m writing this.)\nSome of which required databases like:\nPostgreSQL\nMariaDB\nSQLite\nAs most of these services were internet facing I needed hardening tools like:\nUFW\nPGtune\nFail2ban\nWith my Domain about to expire and an unexpected fuckup that necessitated reinstalling the server, I felt it was time to simplify a few things:\nOnly install a webserver (nginx) for a couple of static legacy sites, meaning only Ports 80 and 443 need to be open on the router\nAs traffic is coming into the server only on these two ports, the firewall only requires these two ports to be open\nSSH is only accessible from the LAN; no open Port 22 on the router means no need for Fail2ban\nThe media server (Jellyfin) is only accessible from the LAN, so no router or firewall ports need to be open\nAny other networked device (TV etc) has the DNS set to use Cloudflare (1.1.1.1) rather than my DNS server\nNo need for Pi-hole as my browser ad-blockers seem to be doing their job\nThe Future Friction Has streamlining my usage caused friction?\nYes.\nIs that all bad?\nNo.\nI\u0026rsquo;ll use FreshRSS as an example.\nI had it running happily on the server.\nIt provided a flawless central place to bring together feed subscriptions and sync them with my other devices.\nNo complaints whatsoever.\nHowever the administration side went something like this:\nFind a suitable server application (FreshRSS)\nDownload, install and configure\nRegularly check for an update (if the application was not installed via the repositories)\nBackup the database (usually nightly via a cron job)\nOccasionally check that those backups restore correctly\nHave a process in place to get the backups off the server and somewhere safe\nFind an Android app (Feeder)\nDownload, install and configure\nUpdates happen automatically (as I chose an application from the Store)\nSyncs happen automagically\nThat seems a lot but in reality the majority of work was done via bash scripts.\nNow, my current desktop workflow is:\nFind a desktop app (Liferea)\nDownload, install and configure\nUpdates happen automatically (as I chose an application from the repositories)\nOccasionally export a OMPL backup locally\nAs syncing is no longer happening, the current mobile workflow is:\nFind an Android app (FeedMe)\nDownload, install and configure\nUpdates happen automatically (as I chose an application from the Store)\nOccasionally import the OMPL backup using a wire connected to the laptop\nNow, is it easier to have a service running on a server that syncs automatically?\nYes, of course it is.\nBut, overall (for me), it\u0026rsquo;s quicker to plug a wire into my phone and transfer a file, which I then import into my phones RSS reader.\nSelf hosting can be free from a cash perspective but it comes with a time cost Repeat a similar process for all of the above mentioned services and the time cost start to add up.\nFor me, most, if not all were learning vehicles; a curiosity itch to scratch.\nI was curious about the Fediverse so I installed and wrote scripts for various flavours and database schemas.\nI wanted to learn about WordPress so installed and tinkered with that.\nLikewise Hugo.\nThere were $Reasons for all of the services on the list; I\u0026rsquo;ve just run out of $Reasons.\nNext? As there is no Domain, there is no way to list this IP with Google; which means no search engine listings.\nAs I have zero social media there is no way to promote these pages.\nSo, if a blog is not findable and receives no traffic does it even exist?\n¯\\_(ツ)_/¯\nThe point is, I don\u0026rsquo;t care.\nThis was always for myself, my learning and my notes to myself about my learning.\nSo even this will probably be nuked in the near future.\nFor now?\nI\u0026rsquo;m off to read a book.\n","permalink":"https://boffinsblog.github.io/posts/reducing-my-digital-footprint/","summary":"Treading more lightly (digitally).","title":"Reducing My Digital Footprint"},{"content":"Concepts Grafana is a dashboard application; it displays data in a visual way, i.e. graphs and interactive charts.\nPrometheus Scraper sits underneath Grafana and \u0026lsquo;scrapes\u0026rsquo; the data from various inputs; making these \u0026lsquo;scrapes\u0026rsquo; available to Grafana.\nYou can think of Prometheus as a sort of butler; collecting data and presenting it.\nNode-Exporter is one such input Prometheus collects and makes available to Grafana.\nWhat is Grafana? Grafana is a multi-platform open source analytics and interactive visualization web application.\nhttps://grafana.com/ I\u0026rsquo;ve already covered how to set up a Grafana dashboard for collecting my iceshrimp.NET metrics .\nThis post is about expanding on the data available to Grafana.\nWhat is Prometheus? Prometheus is a system for collecting and storing metrics as time series data, with a query language and alerting features.\nhttps://prometheus.io/docs/introduction/overview/ Prometheus collects metrics from targets by scraping metrics HTTP endpoints.\nhttps://prometheus.io/docs/prometheus/latest/getting_started/ I\u0026rsquo;ve already written about how to get a usable Grafana Dashboard .\nPrometheus extends the Dashboard by bringing in extra data sources from e.g. nginx.\nWhat is Node Exporter? The Prometheus Node Exporter exposes a wide variety of hardware- and kernel-related metrics.\nhttps://prometheus.io/docs/guides/node-exporter/ Install Prometheus Resources https://prometheus.io/docs/prometheus/latest/installation/ https://devopscube.com/install-configure-prometheus-linux/ Create a prometheus account I prefer to have separate accounts for distinct processes.\nIn a Terminal:\nsudo useradd --no-create-home --shell /bin/false prometheus\nCreate directories Prometheus needs a place to live.\nTo create and take ownership execute the following in a Terminal:\nsudo mkdir /etc/prometheus\nsudo chown prometheus:prometheus /etc/prometheus\nsudo mkdir /var/lib/prometheus\nsudo chown prometheus:prometheus /var/lib/prometheus\nDownload and Install cd ~\nwget https://github.com/prometheus/prometheus/releases/download/v2.53.1/prometheus-2.53.1.linux-armv7.tar.gz\nExtract and rename tar xfz prometheus-2.53.1.linux-armv7.tar.gz\nmv prometheus-2.53.1.linux-armv7/ prometheus/\nrm prometheus-2.53.1.linux-armv7.tar.gz\nCopy files to the correct location In a Terminal:\nsudo cp prometheus/prometheus /usr/local/bin/\nsudo chown prometheus:prometheus /usr/local/bin/prometheus\nThen:\nsudo cp prometheus/promtool /usr/local/bin/\nsudo chown prometheus:prometheus /usr/local/bin/promtool\nThen:\nsudo cp -r prometheus/consoles /etc/prometheus\nsudo chown -R prometheus:prometheus /etc/prometheus/consoles\nThen:\nsudo cp -r prometheus/console_libraries /etc/prometheus\nsudo chown -R prometheus:prometheus /etc/prometheus/console_libraries\nThen:\nsudo cp prometheus/prometheus.yml /etc/prometheus/prometheus.yml\nsudo chown prometheus:prometheus /etc/prometheus/prometheus.yml\nTidy up cd ~\nsudo rm -R prometheus\nConfigure ufw I am only interested in the prometheus metrics and dashboards being visible from inside my network.\nTo see the current list of applications allowed through the firewall, in a Terminal:\nsudo ufw verbose\nThen:\nsudo ufw allow from server.ip/24 to any port 9090 comment \u0026quot;Prometheus traffic from LAN only\u0026quot;\nCreate a prometheus service file under systemd In a Terminal:\nsudo nano /etc/systemd/system/prometheus.service\n[Unit] Description=Prometheus Wants=network-online.target After=network-online.target [Service] User=prometheus Group=prometheus Type=simple ExecStart=/usr/local/bin/prometheus \\ --config.file /etc/prometheus/prometheus.yml \\ --storage.tsdb.path /var/lib/prometheus/ \\ --web.console.templates=/etc/prometheus/consoles \\ --web.console.libraries=/etc/prometheus/console_libraries \\ --web.enable-admin-api [Install] WantedBy=multi-user.target Reload the systemd daemon:\nsudo systemctl daemon-reload\nStart:\nsudo systemctl start prometheus\nEnable the daemon to restart after a reboot:\nsudo systemctl enable prometheus\nCheck its status:\nsudo systemctl status prometheus\nView To view the Prometheus homepage and start to interact go to:\nserver.ip:9090\nThe server needs to have Node Exporter installed to collect all the system metrics and make it available for Prometheus to scrape it.\nAdmin Notes To remove (annoying) old jobs in the Grafana drop-down:\nhttps://stackoverflow.com/questions/54704117/how-can-i-delete-old-jobs-from-prometheus https://prometheus.io/docs/prometheus/latest/querying/api/#delete-series Prometheus API needs enabling in the prometheus.service:\n' --web.enable-admin-api'\n(Already added to the live version and the example above)\nTo return existing jobs:\ncurl http://localhost:9090/api/v1/label/job/values\nExample result:\n{\u0026quot;status\u0026quot;:\u0026quot;success\u0026quot;,\u0026quot;data\u0026quot;:[\u0026quot;node\u0026quot;,\u0026quot;node_explorer_ServerName\u0026quot;,\u0026quot;node_exporter_ServerName\u0026quot;,\u0026quot;prometheus\u0026quot;]}\nTo remove redundant jobs:\ncurl -X POST -g 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={job=\u0026quot;node\u0026quot;}'\ncurl -X POST -g 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={job=\u0026quot;node_explorer_ServerName\u0026quot;}'\nThen finish deleting entirely:\ncurl -X POST 'http://localhost:9090/api/v1/admin/tsdb/clean_tombstones'\nTo restart:\nsudo systemctl restart prometheus\nCheck its status:\nsudo systemctl status prometheus\nTo check:\ncurl http://localhost:9090/api/v1/label/job/values\nAdd Prometheus Node Exporter Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors.\nhttps://github.com/prometheus/node_exporter Resources https://prometheus.io/docs/guides/node-exporter/ https://devopscube.com/monitor-linux-servers-prometheus-node-exporter/ https://blog.devops.dev/a-step-by-step-guide-to-installing-prometheus-and-grafana-201c66f88b73 https://pimylifeup.com/raspberry-pi-prometheus/ Create a node_exporter account I\u0026rsquo;ve already said that I prefer to have separate accounts for distinct processes.\nIn a Terminal:\nsudo useradd -rs /bin/false node_exporter\nDownload cd ~\nsudo wget -O node_exporter.tar.gz https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-armv7.tar.gz\nExtract, copy and chown tar xvf node_exporter.tar.gz\nsudo mv node_exporter-1.8.2.linux-armv7/node_exporter /usr/local/bin/\nsudo chown node_exporter:node_exporter /usr/local/bin/node_exporter\nTidy up sudo rm -R node_exporter-1.8.2.linux-armv7\nsudo rm node_exporter.tar.gz\nConfigure ufw Allow access to \u0026rsquo;node_exporter\u0026rsquo; from LAN only.\nTo see the current list of applications allowed through the firewall, in a Terminal:\nsudo ufw verbose\nThen:\nsudo ufw allow from server.ip/24 to any port 9100 comment \u0026quot;node_exporter traffic from LAN only\u0026quot;\nCreate a node_exporter service file under systemd In a Terminal:\nsudo nano /etc/systemd/system/node_exporter.service\n[Unit] Description=Node Exporter After=network.target [Service] User=node_exporter Group=node_exporter Type=simple ExecStart=/usr/local/bin/node_exporter [Install] WantedBy=multi-user.target Reload the systemd daemon:\nsudo systemctl daemon-reload\nStart:\nsudo systemctl start node_exporter\nEnable the daemon to restart after a reboot:\nsudo systemctl enable node_exporter\nCheck its status:\nsudo systemctl status node_exporter\nTo view the metrics homepage go to:\nserver.ip:9100/metrics\nInclude the node_exporter stats in the Prometheus data Spacing is ! important !\nIn a Terminal:\nsudo nano /etc/prometheus/prometheus.yml\n# ServerName node_exporter job - job_name: \u0026#34;node_exporter_ServerName\u0026#34; static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] Restart Prometheus To restart:\nsudo systemctl restart prometheus\nCheck its status:\nsudo systemctl status prometheus\nTo see the new endpoint \u0026rsquo;node_exporter_ServerName\u0026rsquo; in the targets homepage go to:\nserver.ip:9090/targets\nCreate the node_exporter dashboard Resources:\nhttps://devopscube.com/integrate-visualize-prometheus-grafana/ https://grafana.com/grafana/dashboards/?dataSource=prometheus https://grafana.com/grafana/dashboards/1860-node-exporter-full/ Add Prometheus as a datasource: Steps:\nConnection\nPrometheus server URL\nlocalhost:9090\nImport a dashboard A pre-made dashboard already exists.\nIt can be imported and tinkered with.\nSteps:\nDashboards\nNew\nImport\nhttps://grafana.com/grafana/dashboards/1860-node-exporter-full Load\nSelect datasource\nImport\nSave\nAdd nginx stub_status The ngx_http_stub_status_module module provides access to basic status information.\nThis module is not built by default, it should be enabled with the --with-http_stub_status_module configuration parameter.\nhttps://nginx.org/en/docs/http/ngx_http_stub_status_module.html Resources: https://github.com/nginxinc/nginx-prometheus-exporter?tab=readme-ov-file#prerequisites Add the stub_status module:\nIn a Terminal:\nsudo nano /etc/nginx/conf.d/stub_status.conf\nserver { listen 8080; # Optionally: allow access only from localhost listen localhost:8080; location /stub_status { stub_status; } } Check and Reload nginx To check the config syntax and restart:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl restart nginx\nConfigure ufw Allow access to stub_status from LAN only.\nTo see the current list of applications allowed through the firewall, in a Terminal:\nsudo ufw verbose\nThen:\nsudo ufw allow from server.ip/24 to any port 8080 comment \u0026quot;stub_status traffic from LAN only\u0026quot;\nTo view the stub_status homepage go to:\nserver.ip:8080/stub_status\nAdd nginx-prometheus-exporter NGINX Prometheus exporter makes it possible to monitor NGINX or NGINX Plus using Prometheus.\nhttps://github.com/nginx/nginx-prometheus-exporter Create an nginx_exporter account I\u0026rsquo;ve already said that I prefer to have separate accounts for distinct processes.\nIn a Terminal:\nsudo useradd -rs /bin/false nginx_exporter\nDownload cd ~\nwget https://github.com/nginxinc/nginx-prometheus-exporter/releases/download/v1.3.0/nginx-prometheus-exporter_1.3.0_linux_armv7.tar.gz\nExtract, copy and chown tar xvf nginx-prometheus-exporter_1.3.0_linux_armv7.tar.gz\nsudo mv nginx-prometheus-exporter /usr/local/bin/nginx_prometheus_exporter\nsudo chown nginx_exporter:nginx_exporter /usr/local/bin/nginx_prometheus_exporter\nTidy up rm nginx-prometheus-exporter_1.3.0_linux_armv7.tar.gz\nrm -R manpages\nrm -R completions\nrm LICENSE\nrm README.md\nConfigure ufw Allow access to \u0026rsquo;nginx_exporter\u0026rsquo; from LAN only\nTo see the current list of applications allowed through the firewall, in a Terminal:\nsudo ufw verbose\nThen:\nsudo ufw allow from server.ip/24 to any port 9113 comment \u0026quot;nginx_exporter traffic from LAN only\u0026quot;\nCreate an nginx_exporter service file under systemd In a Terminal:\nsudo nano /etc/systemd/system/nginx_exporter.service\n[Unit] Description=Nginx Exporter Wants=network-online.target After=network-online.target StartLimitIntervalSec=0 [Service] User=nginx_exporter Group=nginx_exporter Type=simple Restart=on-failure RestartSec=5s ExecStart=/usr/local/bin/nginx_prometheus_exporter \\ -nginx.scrape-uri=http://localhost:8080/stub_status [Install] WantedBy=multi-user.target Reload the systemd daemon:\nsudo systemctl daemon-reload\nStart:\nsudo systemctl start nginx_exporter\nEnable the daemon to restart after a reboot:\nsudo systemctl enable nginx_exporter\nCheck its status:\nsudo systemctl status nginx_exporter\nUpdate the Prometheus yaml Spacing is ! important !\nIn a Terminal:\nsudo nano /etc/prometheus/prometheus.yml\n# ServerName nginx_exporter job - job_name: \u0026#34;nginx_exporter_ServerName\u0026#34; static_configs: - targets: [\u0026#39;localhost:9113\u0026#39;] Restart Prometheus To restart:\nsudo systemctl restart prometheus\nCheck its status:\nsudo systemctl status prometheus\nTo view the targets homepage go to:\nserver.ip:9090/targets\nTo see then new endpoint nginx_exporter_ServerName\nCreate an nginx_exporter dashboard Official dashboard for NGINX Prometheus exporter for https://github.com/nginxinc/nginx-prometheus-exporter https://grafana.com/grafana/dashboards/12708-nginx/ Resources https://github.com/nginxinc/nginx-prometheus-exporter/blob/main/grafana/dashboard.json https://github.com/nginxinc/nginx-prometheus-exporter/blob/main/grafana/README.md#installing-the-dashboard Use the New Dashboard button and click Import.\nUpload dashboard.json or copy and paste the contents of the file in the textbox and click Load.\nSet the Prometheus datasource and click Import.\nThe dashboard will appear.\nNote how you can filter the instance label just below the dashboard title (top left corner).\nThis allows you to filter metrics per instance.\nBy default, all instances are selected.\n","permalink":"https://boffinsblog.github.io/posts/prometheus-scraper/","summary":"Moar data, Grafana needs feeding.","title":"Prometheus Scraper"},{"content":"Why? It\u0026rsquo;s more a question of \u0026lsquo;Why Not?\u0026rsquo;.\nI was perfectly happy with Iceshrimp.NET but wanted to try something new.\nGtS seemed lighter and has hooks into Grafana and Prometheus so I\u0026rsquo;d be able to mess around with Metrics.\nResources https://docs.gotosocial.org/en/latest/getting_started/ https://docs.gotosocial.org/en/latest/getting_started/installation/ https://docs.gotosocial.org/en/latest/getting_started/installation/metal/ Preparatory Stuff Create a PostgreSQL GtS database In a Terminal, do the following as normal user:\nsudo -u postgres psql\nThen, (line by line):\nCREATE DATABASE gotosocial_database WITH LOCALE 'C.UTF-8' TEMPLATE template0;\nCREATE USER gotosocial_user WITH PASSWORD 'super_long_password';\nGRANT ALL PRIVILEGES ON DATABASE gotosocial_database TO gotosocial_user;\nALTER DATABASE gotosocial_database OWNER TO gotosocial_user;\nDatabase Notes: To list all the databases:\n\\l\nTo quit:\n\\q\nTo DROP a database (in case something gets bungled):\nDROP DATABASE \u0026quot;gotosocial_database\u0026quot;;\nTo DROP a User (in case something gets bungled):\nDROP USER \u0026quot;gotosocial_user\u0026quot;;\nnginx Configuration In a Terminal, do the following as normal user to create the .conf file:\nsudo nano /etc/nginx/sites-available/GtS\nThen add (changing as appropriate):\nserver { listen 80; # No need for an ipv6 address # listen [::]:80; server_name gts.example.com; location / { # set to 127.0.0.1 instead of localhost to work around https://stackoverflow.com/a/52550758 proxy_pass http://127.0.0.1:8888; proxy_set_header Host $host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; } client_max_body_size 40M; } Check:\nsudo nginx -t\nSymlink:\nsudo ln -s /etc/nginx/sites-available/GtS /etc/nginx/sites-enabled/\nCheck and reload the webserver:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nUFW I use UFW to limit which services can connect to the server.\nTo authorise traffic on Port 8888, first see a verbose list of currently configured connections:\nsudo ufw status verbose\nThen, allow GtS on Port 8888:\nsudo ufw allow 8888 comment \u0026quot;GtS (GoToSocial) Traffic\u0026quot;\nTo delete first see a list of currently configured connections:\nsudo ufw status numbered\nThen, delete the appropriate number:\nsudo ufw delete #\nCertbot Certbot is a free, open source software tool for automatically using Let’s Encrypt certificates on manually-administrated websites to enable HTTPS.\nCertbot is made by the Electronic Frontier Foundation (EFF), a 501(c)3 nonprofit based in San Francisco, CA, that defends digital privacy, free speech, and innovation.\nhttps://certbot.eff.org/pages/about If needed Certbot is a Terminal command away:\nsudo apt install certbot python3-certbot-nginx\nThen, to generate a certificate:\nsudo certbot\nAfter cert has been deployed, re-edit the nginx .conf file:\nsudo nano /etc/nginx/sites-available/GtS\nTo:\nserver { # HTTPS configuration listen 443 ssl; # Server server_name gts.example.com; # SSL ssl_certificate /etc/letsencrypt/live/gts.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/gts.example.com/privkey.pem; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # logging access_log /var/log/nginx/gts.example.com_access.log combined buffer=512k flush=1m; error_log /var/log/nginx/gts.example.com_error.log warn; # security include nginxconfig.io/security.conf; include /etc/letsencrypt/options-ssl-nginx.conf; client_max_body_size 40M; # Proxy to Node location / { proxy_pass http://127.0.0.1:8888; proxy_set_header Host $host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Proto $scheme; } } # HTTP redirect server { if ($host = gts.example.com) { return 301 https://$host$request_uri; } listen 80; server_name gts.example.com; return 404; } Check and reload the webserver:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nInstall Now that there is a database to store the info in, and a running webserver, it\u0026rsquo;s finally time to install GtS.\nCreate a directory:\nmkdir gotosocial\nMove to the newly created directory:\ncd gotosocial\nMake a \u0026lsquo;storage\u0026rsquo; directory:\nmkdir storage\nGrab the latest GtS bundle:\nwget https://github.com/superseriousbusiness/gotosocial/releases/download/v0.17.1/gotosocial_0.17.1_linux_arm64.tar.gz\nUnzip everything:\ntar -xzf gotosocial_0.17.1_linux_arm64.tar.gz\nTidy up:\nrm gotosocial_0.17.1_linux_arm64.tar.gz\nConfigure Copy the example config file before making any changes:\ncp example/config.yaml config.yaml\nOpen it:\nnano config.yaml\nI made the following adjustments (the descriptors are self-explanatory):\n#host: \u0026#34;localhost\u0026#34; host: \u0026#34;gts.example.com\u0026#34; #account-domain: \u0026#34;\u0026#34; account-domain: \u0026#34;example.com\u0026#34; #bind-address: \u0026#34;0.0.0.0\u0026#34; bind-address: \u0026#34;localhost\u0026#34; #Port 8080 is already in use by something else, so change it to something else #port: 8080 port: 8888 #db-address: \u0026#34;\u0026#34; db-address: \u0026#34;localhost\u0026#34; #db-user: \u0026#34;\u0026#34; db-user: \u0026#34;gotosocial_user\u0026#34; #db-password: \u0026#34;\u0026#34; db-password: \u0026#34;super_long_password\u0026#34; #db-database: \u0026#34;gotosocial\u0026#34; db-database: \u0026#34;gotosocial_database\u0026#34; #media-remote-cache-days: 7 media-remote-cache-days: 1 #media-cleanup-every: \u0026#34;24h\u0026#34; media-cleanup-every: \u0026#34;8h\u0026#34; Start, Create Account(s) The moment of truth!\nStart the GtS server:\n./gotosocial --config-path ./config.yaml server start\nCreate the first user(s).\nhttps://docs.gotosocial.org/en/latest/getting_started/user_creation/ Personally, I have two accounts:\nAn Admin account A general non privileged (normal) account YMMV.\nAdd an account:\ncd ~/gotosocial\n./gotosocial --config-path ~/gotosocial/config.yaml \\ admin account create \\ --username admin_account \\ --email admin_account@example.com \\ --password \u0026#39;admin_account_password\u0026#39; Promote to Admin:\n./gotosocial admin account promote --username admin_account --config-path config.yaml\nAdd another account:\n./gotosocial --config-path ~/gotosocial/config.yaml \\ admin account create \\ --username general_account \\ --email general_account@example.com \\ --password \u0026#39;general_account_password\u0026#39; Do not promote to Admin.\nEnable the systemd service Create the systemd service that will start when the server reboots.\nhttps://codeberg.org/superseriousbusiness/gotosocial/src/branch/main/example/gotosocial.service sudo nano /etc/systemd/system/gotosocial.service\nRun:\nsudo systemctl enable --now gotosocial.service\nsudo systemctl start gotosocial.service\nCheck the service status:\nsudo systemctl status gotosocial.service\nAuthentication with the API (get a TOKEN) I have a couple of bots that need to post autonomously using the API.\nThis functionality needs an an Auth token:\nhttps://docs.gotosocial.org/en/latest/api/authentication/ Landing Page (config.yaml) As this is a self-hosted single user instance, rather than the generic GtS homepage I would like my Account Profile to be displayed.\nnano gotosocial/config.yaml\n#landing-page-user: \u0026#34;\u0026#34; landing-page-user: \u0026#34;general_account\u0026#34; sudo systemctl restart gotosocial\nMetrics https://docs.gotosocial.org/en/latest/advanced/metrics/ I\u0026rsquo;m a big fan of Metrics and Dashboards , so of course I\u0026rsquo;m going to enable this feature:\nEdit the config.yaml file.\nI made the following adjustments (the descriptors are self-explanatory):\nnano gotosocial/config.yaml\n#metrics-enabled: false metrics-enabled: true #metrics-auth-enabled: false metrics-auth-enabled: true #metrics-auth-username: \u0026#34;\u0026#34; metrics-auth-username: \u0026#34;user_login_name\u0026#34; #metrics-auth-password: \u0026#34;\u0026#34; metrics-auth-password: \u0026#34;user_login_password\u0026#34; Restart GtS:\nsudo systemctl restart gotosocial\nGo to:\nhttps://server.IP/metrics\nNow 404 the metrics page from external requests (if desired):\nsudo nano /etc/nginx/sites-available/GtS\nAdd:\n# 404 the metrics page from external requests location /metrics { return 404; } Check and reload the webserver:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nAdd to Prometheus sudo nano /etc/prometheus/prometheus.yml\nAdd:\n# gotosocial/metrics job - job_name: \u0026#34;gotosocial\u0026#34; metrics_path: /metrics scheme: https basic_auth: username: \u0026#34;prometheus_login_name\u0026#34; password: \u0026#34;prometheus_login_password\u0026#34; static_configs: - targets: - gts.example.com Restart Prometheus:\nsudo systemctl restart prometheus\nsudo systemctl status prometheus\nGo to:\nhttp://server.IP:9090/targets\nTo see then new endpoint \u0026lsquo;gotosocial\u0026rsquo;\nAdd Dashboard to Grafana http://server.IP:3030/login\nUFW I already mentioned that I use UFW to limit access to the server.\nAs Prometheus and Grafana were already set up and running, there was no need for me to adjust or add to the existing UFW rules.\nAllow Prometheus:\nsudo ufw allow from 192.168.0.0/24 to any port 3030 comment \u0026quot;Grafana traffic from LAN only\u0026quot;\nAllow Grafana:\nsudo ufw allow from 192.168.0.0/24 to any port 9090 comment \u0026quot;Prometheus traffic from LAN only\u0026quot;\nBoth of the above allow access to Grafana and Prometheus from the LAN only.\nI have no interest in viewing my dashboard when away from home or allowing the data to be publicly accessible.\nMetrics Notes: Remember to allow access to the gotosocial_database to the grafana user\nsudo -u postgres psql\n\\connect gotosocial_database;\nGRANT CONNECT ON DATABASE \u0026quot;gotosocial_database\u0026quot; TO grafana;\nGRANT USAGE ON SCHEMA public TO grafana;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO grafana;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO grafana;\nThe Scripts A selection of SQL scripts for the GoToSocial PostgreSQL database.\nThese three scripts are the routines that I run daily, hourly and monthly.\nWhy?\nAll of the Fediverse software that I have used seem to retain too much data for too long.\nPersonally I see no need to keep the e.g. account details for every account that has passed through my timeline.\nLikewise, I do not need to store, archive and backup a huge amount of emoji from various Fediverse instances.\nSo I remove them, along with what I consider to be other extraneous info.\n(The list of data to be removed will probably grow as I dig deeper into the database.)\ngotosocial_tidy_hourly.sql WARNING:\nThis script will:\nTidy (DELETE FROM) the gotosocial db mentions table:\nDelete all mentions over n days old General (DELETE FROM) the gotosocial db statuses table:\nPer account deletes will be done later\nDelete all statuses over n days old\nDon\u0026rsquo;t remove any statuses from any accounts in the \u0026lsquo;users\u0026rsquo; table\nPer account (DELETE FROM) the gotosocial db statuses table:\nDelete all statuses over n days old\nProbably makes sense to keep this in line with the \u0026lsquo;General\u0026rsquo; setting\nDelete from: account_1_username account\nRetain pinned posts\nPer account (DELETE FROM) the gotosocial db statuses table:\nDelete all statuses over n days old\nProbably makes sense to keep account_2_username posts for longer\nDelete from: account_2_username account\nRetain pinned posts\nTidy (DELETE FROM) the gotosocial db status_faves table:\nDelete all status_faves over n days old Tidy (ANALYZE) any table that has had a DELETE performed upon it:\npublic.\u0026ldquo;mentions\u0026rdquo;\npublic.\u0026ldquo;statuses\u0026rdquo;\npublic.\u0026ldquo;status_faves\ngotosocial_tidy_daily.sql WARNING:\nThis script will:\nTidy (DELETE FROM) the gotosocial db accounts table:\nDon\u0026rsquo;t remove any accounts with the following id\u0026rsquo;s\nRemove any accounts that are not followers\nRemove any accounts that are not followees\nDon\u0026rsquo;t remove any accounts that are in the blocking table\nDon\u0026rsquo;t remove any accounts that are in the muting table\nTidy (DELETE FROM) the gotosocial db accounts_stats table:\nDon\u0026rsquo;t remove any stats with the following id\u0026rsquo;s\nRemove any stats that are not followers\nRemove any stats that are not followees\nTidy (DELETE FROM) the gotosocial db tokens table:\nKeeping the (bot) sessions Tidy (TRUNCATE) the gotosocial db tables:\npublic.clients\npublic.emojis\npublic.tombstones\nTidy (VACUUM.ANALYZE) the gotosocial db\ngotosocial_tidy_monthly.sql This script will:\nPerform a VACUUM FULL ANALYZE on the database and only needs to be run occasionally Bugs So far I have found very few bugs and certainly zero application killing ones.\nThere doesn\u0026rsquo;t seem to be a way to clear previously authorised sessions i.e. an old browser session, clearing the table stops the application from functioning.\nThe also_known_as field, if present, prevents the account from being found yet the account can still create content.\nNeither of the above would be encountered during normal everyday usage; it\u0026rsquo;s more me, tinkering with the guts of the database, something a normal person would ever do.\n","permalink":"https://boffinsblog.github.io/posts/moving-to-gotosocial/","summary":"Because I like fixing things until they break, I decided to move my Socials to a different Fediverse software.","title":"Moving to GoToSocial"},{"content":"Why? I\u0026rsquo;ve seen a few people lately eager to start self-hosting something and are unsure how the jigsaw pieces fit together.\nThis is not intended to be a \u0026lsquo;how do I configure blah blah?\u0026rsquo; piece; think of it more like the picture on the front of the jigsaw box.\nHopefully this will go some way to deciphering some of the technical terms and make starting the self-hosting puzzle a bit easier.\nTerminology I\u0026rsquo;ll try and keep this bit brief but unfortunately invariably there are acronyms and a bit of technical language ahead.\nWhat are \u0026lsquo;Packets\u0026rsquo;? Essentially someone using a web browser \u0026lsquo;requests\u0026rsquo; a web page.\nThis web page is then \u0026lsquo;served\u0026rsquo; by a web server back to the web browser that requested it.\nThe web page is broken into chunks, or \u0026lsquo;packets\u0026rsquo; and sent back to the web browser that requested it.\nIt is these packets that flow through the pipes that connect the internet together.\nPackets know a couple of things, most importantly:\nwhere they came from (the web browser that requested the content)\nwhere they are going to (the web server where the content is hosted)\nAside: Some packets know about the the packets that are ahead and behind them but that\u0026rsquo;s not really relevant to cover here.\nThe following will hopefully describe the hoops that the packets have to jump through.\nWhat is \u0026lsquo;DNS\u0026rsquo;? So, you want to look at https://google.com/ ?\nThe first thing to do is turn the human readable bit \u0026lsquo;google.com\u0026rsquo; into an IP address .\nAn IP address is a unique identifying number assigned to every device connected to the internet (including your router).\nIn this case the IP address of https://google.com/ is \u0026lsquo;142.250.187.238\u0026rsquo;.\n(You can check this by putting \u0026lsquo;142.250.187.238\u0026rsquo; in a browser\u0026rsquo;s address bar.)\nThis is called Domain Name Resolution or DNS and normally the ISP of whoever is requesting the web page does this.\nCertbot Back in the olden days, having an \u0026lsquo;S\u0026rsquo; at the end of the HTTP bit of your web address was fancy and reserved for sites doing financial stuff.\nIt was also expensive.\nYou had to buy a Certificate from a Certificate Authority, and then renew it annually.\nCertbot is run by the Electronic Frontier Foundation ( EFF ) and gives Certs away for free.\n(It also handles the renewal process, so once you have grabbed a Cert for your site, that\u0026rsquo;s it, that\u0026rsquo;s everything you need to do).\nThe Request All of the above is lovely to know but what about when someone using a browser wants to look at your website.\nThe first thing you have to do is buy a Domain and set the IP address of that Domain as your router\u0026rsquo;s IP address with the Domain registrar.\nThis is called setting the \u0026lsquo;A record\u0026rsquo;.\n\u0026lsquo;A records\u0026rsquo;, known as Address records are used to store IP address information for a Domain name.\nNow, their browser request will resolve your Domain name to your router\u0026rsquo;s IP address.\nTheir browser will send a packet requesting the content from your website.\nThis packet\u0026rsquo;s first stop will be your router.\nThe Router So, the packet has now reached your router.\nWeb traffic happens on Port 80 (HTTP) and Port 443 (HTTPS).\nThose Ports need to be opened on the router, and the packet traffic directed to the IP address of your web server.\nThis will be the internal network IP address of the machine where your web server lives, probably 192.168.0.something.\nThe Firewall So, now the packet has traversed the router and been directed towards the web server.\nFirst it has to get through the firewall (you are running a firewall aren\u0026rsquo;t you?).\nIf it\u0026rsquo;s a Debian or Ubuntu server it\u0026rsquo;s probably using Uncomplicated Firewall or UFW.\nAgain, Ports 80 and 443 need to be opened on the firewall to allow traffic to get to and from the web server.\n(No-one really uses HTTP on Port 80 anymore but requests may come in via that Port, which your web server will turn into a HTTPS request.)\nIf you want to connect to your server using the Secure Shell (SSH) Protocol, Port 22 will need to be opened on the firewall too.\nIf SSH traffic has been allowed through both the router and the firewall, you will need to look into installing Fail2Ban ; this will limit the exposure of Port 22 to the wider Internet.\nMore complex applications (e.g. Fediverse software) may require different Ports to be opened.\nThis is beyond the scope of this piece.\nThe Web Server So, the packet has now traversed the router, been allowed through the firewall and has finally gotten to the web server.\nWhere does it go now?\nWhat if there are multiple websites on the web server?\nWell, the web server directs traffic packets to the appropriate folder where the content lives.\nSay you have two sites \u0026lsquo;FantasticWebsite.com\u0026rsquo; and \u0026lsquo;EvenMoreFantasticWebsite.com\u0026rsquo;.\nThere will be two web server folders with content:\n/var/www/FantasticWebsite.com And\n/var/www/EvenMoreFantasticWebsite.com The web server sends the requesting packets to the corresponding folder.\nRemember how I said right at the beginning that packets know two things; \u0026lsquo;where they came from\u0026rsquo; and \u0026lsquo;where they are going to\u0026rsquo;?\nWell now that they have reached your web server the \u0026lsquo;where they are going to\u0026rsquo; bit has been completed.\nYour web server now does the \u0026lsquo;where they came from\u0026rsquo; bit and sends the content back to the requesting web browser.\nDon\u0026rsquo;t Run Before You Can Walk If I were thinking of starting to self-host stuff, I would start small, something like:\nBuy a Domain, say, \u0026lsquo;MyNewWebsite.com\u0026rsquo;.\nPoint the Domain\u0026rsquo;s Address record to my router\u0026rsquo;s IP address.\nFigure out how to log into the router to open Ports 80 and 443.\nInstall a web server, say, nginx.\nCreate a directory at /var/www/MyNewWebsite.com\nCreate a simple \u0026lsquo;index.html` file to go in the above folder, something like:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My First Webpage\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;My First Webpage\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Run certbot to grab a HTTPS certificate.\nUse a web browser to look at \u0026lsquo;MyNewWebsite.com\u0026rsquo; to make sure that it all works.\nOK, Now You Can Run! Now that you understand how a packet travels from a distant web browser, across the Internet, through your router and firewall, is handled by your web server and sends a response, you\u0026rsquo;ll probably want to install some more interesting services.\nThis is a great place to spend some time browsing general self-hosting applications:\nhttps://awesome-selfhosted.net/ For an ActivityPub (Fediverse) oriented list see here:\nhttps://github.com/sguzman/delightful-fediverse-apps Shameless Self Promotion For a music streaming server, Navidrome is an easy one to recommend.\nFor Fediverse Socials, GoToSocial has excellent documentation and is fairly uncomplicated to setup (there is a bit of tinkering with config files, nothing scarier than that).\nBoth of the above are written in the Go language and unlike a traditional Linux installation, (which requires the installation of other files called Dependencies) they are self contained units.\nYou download the relevant Go binary, put it in a folder and run it.\nThis approach has both positives and negatives.\nPositive: If the binary runs and works, well, that\u0026rsquo;s it, forget about it, that\u0026rsquo;s all you have to do.\nNegative: You\u0026rsquo;ll never know when an update is available and if you care about updates will have to keep an eye on their GitHub page or Socials.\nObviously there are a whole host of other services you can self-host; spend some time browsing and reading about what other people are doing.\nTech Stacks Finally, some advice you can choose to disregard.\nAll-in-One, point and click packages have been around for ages.\nBack in the \u0026lsquo;90\u0026rsquo;s LAMP (Linux, Apache, MySQL and PHP) were all the rage.\nI avoided them, preferring to stack the pieces myself.\nI feel you gain a better understanding of how everything sits together and interacts if you have installed it yourself.\n(You are also far more likely to find online help if you are running a vanilla installation, rather than one that has been fucked about with in unknown ways.)\nAnyway, YMMV, it\u0026rsquo;s your journey; you do you!\nMore Reading https://awesome-selfhosted.net/ https://github.com/sguzman/delightful-fediverse-apps https://www.digitalocean.com/community/conceptual-articles/introduction-to-web-servers https://certbot.eff.org/ https://www.digitalocean.com/community/tutorials/how-fail2ban-works-to-protect-services-on-a-linux-server ","permalink":"https://boffinsblog.github.io/posts/the-travelling-packet/","summary":"A very high level description of how information flows from a web server to a browser.","title":"The Travelling Packet"},{"content":"Or: \u0026lsquo;How I Learned to Stop Worrying and Love the Code\u0026rsquo; There are whole buch of \u0026lsquo;How To\u0026rsquo; blogs on the Interwebs about moving from WordPress to a Static Site Generator (SSG).\nThis is not another one of those, it\u0026rsquo;s more my Notes and Thoughts.\nFirst, Why NOT? I\u0026rsquo;m not really interested in the recent WordPress shenanigans.\nAs a self-hoster, one petty millionaire being cross at another millionaire doesn\u0026rsquo;t affect me in the slightest.\nThis is not the reason that I made the move.\nSo, Why? There are a number of reasons why.\nThis site only really existed as a vehicle for me the learn about WorldPress, and I feel as if I\u0026rsquo;ve come to the end of that journey.\nI\u0026rsquo;d also noticed something else, my writing had become SEO oriented; I was becoming transfixed on getting all the Yeost SEO green dots for each post.\nI was losing my writing style to please the Google Gods and I didn\u0026rsquo;t like that.\nI wanted to learn something new.\nI\u0026rsquo;m very much of the midset \u0026ldquo;everything is working now; time to change it\u0026rdquo;.\nThere is a longer term plan to offload my static sites onto my old SheevaPlug which currently resides in the \u0026lsquo;Cupboard of Forgotten Tech\u0026rsquo; , so this is the first step on that journey.\nStatic sites are blazingly fast.\nExporting WordPress Content I tried both:\nhttps://github.com/SchumacherFM/wordpress-to-hugo-exporter https://github.com/benbalter/wordpress-to-jekyll-exporter The first requires downloading, unzipping and adding to the /wp-content/plugins/ folder.\nThe second can either be installed as a plugin; or downloaded, unzipped and placed in the /wp-content/plugins/ folder.\nMore tools can be found here .\nFor my usage the Jekyll exporter worked better.\nCreating Markdown Content By far the biggest time sink was cleaning up the export.\nYes, both of the above produced usable Markdown.\nYet:\nWordPress html artifacts remained\nImages were in a weird folder\nImages had lost their alt text\nGenerally it was all a bit of a mess\nAll of the above and a bunch of other things I can\u0026rsquo;t remember needed addressing before I was happy with the Markdown.\nIt was essentially a long find and replace few hours.\nThe process did however afforded me the opportunity to do a bit of light editing (read: get rid of some of the SEO chasing I\u0026rsquo;d done in the past).\nStuff I Don\u0026rsquo;t Like There is one thing that I don\u0026rsquo;t really like about static sites, and that\u0026rsquo;s where the content ends up in relation to the final URL.\nIn the development enviroment all of the content goes under /content/posts/.\nThis, I like; it\u0026rsquo;s tidy and uncluttered.\nHowever, when deploying to the web server this hierarchy is replicated, meaning that my content ends up in https://tld/posts/my-post.html.\nThis, I don\u0026rsquo;t like.\nI completely understand why this happens.\nI completely understand how having a database prevents this happening.\nI also understand that I can take out the /posts/ portion of the URL by editing the [permalinks] in the hugo.yaml config file.\nUnfortunately that leads to everything being in the server\u0026rsquo;s root folder; which is untidy and cluttered.\nI should probably look at creating an nginx rewrite rule or something.\nAn Inelegant nginx Rewrite Rule So, I cobbled together the below, while I look for a one line regex rewrite rule:\n# 0001 location /googleaiy-raspberrypi-scroll-phat/ { rewrite ^/googleaiy-raspberrypi-scroll-phat/ /posts/2018/04/googleaiy-raspberrypi-scroll-phat/ break; } # 0002 location /refurbishing-a-picade/ { rewrite ^/refurbishing-a-picade/ /posts/2018/10/refurbishing-a-picade/ break; } My Current Workflow The Internet told me that a good chunk of time would be sunk into developing the new way of working.\nYep, this turned out to be true.\nNot least of all:\nGetting content to the web server with the correct permissions\nBacking up the current content\nSo, naturally I wrote a bash script that does the following:\nStop the Hugo dev-server\nchown the remote web server directory (take ownership)\nRemove any existing content on the remote web server\nRemove any existing local Hugo \u0026lsquo;/public\u0026rsquo; content\nBuild the new local Hugo website\nCopy the Hugo \u0026lsquo;/public\u0026rsquo; content to the remote web server\nchown the remote web server directory (give ownership to www-data)\nBackup (rsync) the working directory\nClear out the Hugo generated \u0026lsquo;/public\u0026rsquo; folder before syncing (no need to store this in Forgejo)\nPush \u0026lsquo;/path/to/git/hugo/\u0026rsquo; to Forgejo\nStart the Hugo dev-server\nThe Deployment Script deploy.sh This is linked to an alias in my .bashrc file so that I can simply type \u0026lsquo;Deploy_Hugo\u0026rsquo; in a Termininal, and it goes off and does its thing.\nIn a Terminal:\nnano .bashrc\nAdd:\n# Add deploy alias(s) for Hugo alias Deploy_Hugo=\u0026#34;bash /path/to/script/deploy.sh\u0026#34; Then reload .bashrc.\nsource ~/.bashrc\nFeel free to chop this up and season to taste:\n#!/bin/bash # Functions nuke_hugo () { clear; echo \u0026#34;\u0026#34;; # Nuke the existing Hugo installation echo -e \u0026#34;\\e[1;36mNuke the existing Hugo installation...\\e[0m\u0026#34;; # Sleep so the screen can be read before moving on sleep 2; # Stop the Hugo dev-server echo \u0026#34;\u0026#34;; echo -e \u0026#34;\\e[1;33mStop the Hugo dev-server...\\e[0m\u0026#34;; killall hugo; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # chown the remote web server directory (take ownership) echo -e \u0026#34;\\e[1;33mchown the remote web server directory (take ownership)...\\e[0m\u0026#34;; ssh -t foo@server \u0026#39;sudo chown $USER:$USER -R /var/www/example.org\u0026#39;; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Remove any existing content on the remote web server echo -e \u0026#34;\\e[1;33mRemove any existing content on the remote web server...\\e[0m\u0026#34;; ssh -t foo@server \u0026#39;rm -R /var/www/example.org/*\u0026#39;; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Remove any existing local Hugo \u0026#39;/public\u0026#39; content echo -e \u0026#34;\\e[1;33mRemove any existing local Hugo \u0026#39;/public\u0026#39; content...\\e[0m\u0026#34;; rm -R /path/to/local/hugo/public/*; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Build the new local Hugo website echo -e \u0026#34;\\e[1;33mBuild the new local Hugo website...\\e[0m\u0026#34;; (cd /path/to/local/hugo/ \u0026amp;\u0026amp; hugo); echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Copy the Hugo \u0026#39;/public\u0026#39; content to the remote web server echo -e \u0026#34;\\e[1;33mCopy the Hugo \u0026#39;/public\u0026#39; content to the remote web server...\\e[0m\u0026#34;; scp -r /path/to/local/hugo/public/* foo@server:/var/www/example.org/; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # chown the remote web server directory (give ownership to www-data) echo -e \u0026#34;\\e[1;33mchown the remote web server directory (give ownership to www-data)...\\e[0m\u0026#34;; ssh -t foo@server \u0026#39;sudo chown www-data:www-data -R /var/www/example.org\u0026#39;; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Backup (rsync) the working directory echo \u0026#34;\u0026#34;; echo -e \u0026#34;\\e[1;33mBackup (rsync) the working directory...\\e[0m\u0026#34;; rsync -a -P --delete -v --exclude \u0026#39;.git\u0026#39; /path/to/local/hugo /path/to/git/hugo; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Clear out the Hugo generated \u0026#39;/public\u0026#39; folder before syncing (no need to store this in Forgejo) echo -e \u0026#34;\\e[1;33mClear out the Hugo generated \u0026#39;/public\u0026#39; folder before syncing (no need to store this in Forgejo)...\\e[0m\u0026#34;; rm -R /path/to/git/hugo/public/*; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Push \u0026#39;/path/to/git/hugo/\u0026#39; to Forgejo echo -e \u0026#34;\\e[1;33mPush \u0026#39;/path/to/git/hugo/\u0026#39; to Forgejo...\\e[0m\u0026#34;; cd /path/to/git/hugo/; git add .; git commit -m \u0026#39;Modified\u0026#39; .; git push -u origin main; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; # Start the Hugo dev-server echo -e \u0026#34;\\e[1;33mStart the hugo dev-server...\\e[0m\u0026#34;; hugo server -s /path/to/local/hugo; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; } # Start by clearing the Terminal window clear echo \u0026#34;\u0026#34;; echo -e \u0026#34;\\e[1;34mUse \\e[1;36mCtrl-c\\e[1;34m to \\e[1;31mExit\\e[1;34m and close the window...\\e[0m\u0026#34;; echo \u0026#34;\u0026#34;; echo -e \u0026#34;\\e[1;31mNuke\\e[1;34m will:\u0026#34;; echo -e \u0026#34; Stop the Hugo dev-server\u0026#34;; echo -e \u0026#34; chown the remote web server directory (take ownership)\u0026#34;; echo -e \u0026#34; Remove any existing content on the remote web server\u0026#34;; echo -e \u0026#34; Remove any existing local Hugo \u0026#39;/public\u0026#39; content\u0026#34;; echo -e \u0026#34; Build the new local Hugo website\u0026#34;; echo -e \u0026#34; Copy the Hugo \u0026#39;/public\u0026#39; content to the remote web server\u0026#34;; echo -e \u0026#34; chown the remote web server directory (give ownership to www-data)\u0026#34;; echo -e \u0026#34; Backup (rsync) the working directory\u0026#34;; echo -e \u0026#34; Clear out the Hugo generated \u0026#39;/public\u0026#39; folder before syncing (no need to store this in Forgejo)\u0026#34;; echo -e \u0026#34; Push \u0026#39;/path/to/git/hugo/\u0026#39; to Forgejo\u0026#34;; echo -e \u0026#34; Start the Hugo dev-server\u0026#34;; echo \u0026#34;\u0026#34;; # Loop while true; do options=(\u0026#34;Nuke\u0026#34;) echo -e \u0026#34;\\e[1;36mChoose an option:\\e[0m\u0026#34; select opt in \u0026#34;${options[@]}\u0026#34;; do case $REPLY in 1) nuke_hugo; break ;; *) echo \u0026#34;Whoops, choose again, option \u0026#39;$REPLY\u0026#39; is invalid...\u0026#34; \u0026gt;\u0026amp;2 esac done done Hope that helps someone.\n","permalink":"https://boffinsblog.github.io/posts/moving-to-hugo-from-wordpress/","summary":"Why I spent an inordinate amount of time moving to the Hugo \u0026lsquo;Static Site Generator\u0026rsquo;.","title":"Moving to Hugo from WordPress"},{"content":"Just one more self-hosted application, then I’ll stop, I promise Self-Hosting FreshRSS is kind of pointless as a single user but I’m going to do it anyway.\nWhy though? Whilst not quite as straightforward as self-hosting a music streaming server , running your own RSS server is eminently do-able.\nHowever you will end up with a very pretty and usable interface.\nThis, and the ability to synchronise the feeds across multiple devices using an API are the main reasons I chose to do this.\nIt’s a learning exercise as a single user though (unless you care deeply about having thousands of articles archived on your server for posterity).\nFreshRSS Main Stream Screenshot\nCreate a sub-domain Not strictly a requirement as I could access it locally but I want to be able to sync the feeds with multiple devices (phone, tablet etc.) so a dedicated URL is preferred (by me).\nCreate a MySQL database and user I already have MariaDB installed and running as it is the DB behind this WordPress site.\nPostgreSQL is recommended, although as the installation is just for me and I already had MariaDB installed I went with that.\nIf needed, install by running the following in a Terminal:\nsudo apt install mariadb-server -y\nHowever if this is the first time MariaDB has been installed it is a good idea to run through an initial setup in a Terminal:\nsudo mysql_secure_installation\nTo check that it’s running and to start on a reboot, run the following in a Terminal:\nsudo systemctl start mariadb\nsudo systemctl enable mariadb\nsudo systemctl status mariadb\nTo add the Gitea database run the following in a Terminal:\nsudo mysql -u root -p\nThen run:\nCREATE DATABASE freshrss;\nCREATE USER 'freshrss'@'localhost' IDENTIFIED BY 'super_long_password';\nGRANT ALL PRIVILEGES ON freshrss.* TO 'freshrss'@localhost;\nFLUSH PRIVILEGES;\nQUIT;\nSelf-Hosting FreshRSS – Installing https://github.com/FreshRSS/FreshRSS?tab=readme-ov-file#installation https://freshrss.github.io/FreshRSS/en/admins/03_Installation.html https://freshrss.github.io/FreshRSS/en/admins/06_LinuxInstall.html To download and install, first, navigate to:\ncd /usr/share/\nThen, clone the git repository:\nsudo git clone https://github.com/FreshRSS/FreshRSS.git\nSet the installation permissions Move to the FreshRSS directory:\ncd FreshRSS\nThen run the permissions script:\nsudo cli/access-permissions.sh\nSelf-Hosting FreshRSS – nginx https://freshrss.github.io/FreshRSS/en/admins/10_ServerConfig.html nginx part I – pre Certbot Create an nginx conf file:\nsudo nano /etc/nginx/sites-available/rss\nThen paste in something similar to the below:\nserver { listen 80; #listen 443 ssl; # HTTPS configuration #ssl on; #ssl_certificate /etc/nginx/server.crt; #ssl_certificate_key /etc/nginx/server.key; # your server’s URL(s) server_name rss.example.net; # the folder p of your FreshRSS installation root /srv/FreshRSS/p/; index index.php index.html index.htm; # nginx log files access_log /var/log/nginx/rss.example.net_access.log; error_log /var/log/nginx/rss.example.net_error.log; # php files handling # this regex is mandatory because of the API location ~ ^.+?\\.php(/.*)?$ { fastcgi_pass unix:/var/run/php/php8.1-fpm.sock; fastcgi_split_path_info ^(.+\\.php)(/.*)$; # By default, the variable PATH_INFO is not set under PHP-FPM # But FreshRSS API greader.php need it. If you have a “Bad Request” error, double check this var! # NOTE: the separate $path_info variable is required. For more details, see: # https://trac.nginx.org/nginx/ticket/321 set $path_info $fastcgi_path_info; fastcgi_param PATH_INFO $path_info; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; } location / { try_files $uri $uri/ index.php; } } Ensure it has the right PHP version:\nfastcgi_pass unix:/var/run/php/php8.1-fpm.sock;\nCheck, symlink and restart First, check that nginx is configured correctly and return no errors:\nsudo nginx -t\nThen, symlink the newly created file:\nsudo ln -s /etc/nginx/sites-available/rss /etc/nginx/sites-enabled/\nFinally, re-check and re-load nginx:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nCertbot sudo certbot\nThe result (i.e. successful) should look something like this:\nSuccessfully received certificate. Certificate is saved at: /etc/letsencrypt/live/rss.example.net/fullchain.pem Key is saved at: /etc/letsencrypt/live/rss.example.net/privkey.pem nginx part II – post Certbot Certbot should have taken care of adding the HTTPS settings.\nHowever for completeness here is mine.\nserver { # HTTPS configuration listen 443 ssl; # managed by Certbot listen [::]:443 ssl; http2 on; # Server server_name rss.example.net; root /usr/share/FreshRSS/p/; # index pages index index.php index.html index.htm; # SSL ssl_certificate /etc/letsencrypt/live/rss.example.net/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/rss.example.net/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot # logging access_log /var/log/nginx/rss.example.net_access.log; error_log /var/log/nginx/rss.example.net_error.log; # php files handling # this regex is mandatory because of the API location ~ ^.+?\\.php(/.*)?$ { fastcgi_pass unix:/var/run/php/php8.3-fpm.sock; fastcgi_split_path_info ^(.+\\.php)(/.*)$; # By default, the variable PATH_INFO is not set under PHP-FPM # But FreshRSS API greader.php need it. If you have a “Bad Request” error, double check this var! # NOTE: the separate $path_info variable is required. For more details, see: # https://trac.nginx.org/nginx/ticket/321 set $path_info $fastcgi_path_info; fastcgi_param PATH_INFO $path_info; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; } location / { try_files $uri $uri/ index.php; } } # HTTP redirect server { if ($host = rss.example.net) { } # managed by Certbot listen 80; server_name rss.example.net; return 404; # managed by Certbot } Check and reload nginx sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nFreshRSS – initial setup Go to:\nrss.example.net\nGo through the setup\nHost = localhost\nRemember that the first user created will have ‘admin’ rights.\nThen, log in and create new users as needed.\nFreshRSS Subscription Management Screenshot\nSelf-Hosting FreshRSS – API access At this point it might be useful to set up API access, especially if your plan is to use an external client.\nGo to (and tick):\nAdministration | Authentication | Allow API access\nRefresh feeds with cron https://freshrss.github.io/FreshRSS/en/admins/08_FeedUpdates.html FreshRSS is updated by the actualize_script.php script.\nThis can be triggered to run at set times using cron.\nNot normal cron, the www-data user cron.\nSwap to www-data user and open a sh prompt:\nsudo -u www-data sh\nFirst, see if the command works:\n/usr/bin/php /usr/share/FreshRSS/app/actualize_script.php \u0026gt; /tmp/FreshRSS.log 2\u0026gt;\u0026amp;1;\nnano /tmp/FreshRSS.log\nThen, setup www-data cron:\nRun the following in a Terminal:\ncrontab -e\nThen add something like the below (choose your preferred run frequency):\n# Refresh all of the FreshRSS feeds, every 10 minutes */10 * * * * /usr/bin/php /usr/share/FreshRSS/app/actualize_script.php \u0026gt; /tmp/FreshRSS.log 2\u0026gt;\u0026amp;1; # To exit the Terminal press:\nshift - ctrl - d\nSelf-Hosting FreshRSS – Extensions FreshRSS extensions are added to the following directory:\ncd /usr/share/FreshRSS/extensions\nAuto-Refresh-Extension https://github.com/Eisa01/FreshRSS---Auto-Refresh-Extension Whilst having a browser tab log-out after a period of time is good security I would prefer my session to stay alive as long as my browser window is open.\nThat is what the ‘Auto-Refresh-Extension’ achieves.\nFirst, download the zip to a local machine and extract.\nThen, rsync to the /TEMP/ folder on the server (assuming there is a /TEMP/ obviously):\nrsync -av -e ssh /home/foo/Downloads/FreshRSS---Auto-Refresh-Extension-master/xExtension-AutoRefresh/* foo@server:/home/foo/TEMP/AutoRefresh/\nNow, on the remote machine, create the ‘extension’ directory and then copy the files and folders.\nRun the following in a Terminal to make a directory for the extension to live in:\nsudo mkdir /usr/share/FreshRSS/extensions/AutoRefresh\nThen, copy the extracted files to the newly created directory:\nsudo cp -R /home/foo/TEMP/AutoRefresh/* /usr/share/FreshRSS/extensions/AutoRefresh/\nFinally, refresh the web page and ‘enable’ the new extension.\nOther extensions are available. Self-Hosting FreshRSS – Clients I’m using the FreshRSS Android Client from F-Droid (link seems broken):\nf-droid.org/packages/fr.chenry.android.freshrss\n(Hence why API access was activated earlier.)\nOther clients on other platforms are available:\nhttps://github.com/FreshRSS/FreshRSS#apis--native-apps Finally Whilst it might have been a bit of a drawn out process; self-hosting FreshRSS does mean that I am no longer exporting and importing OPML files across devices.\nAlso, the server side works perfectly, although I’m not sure I’m ever going to read those thousands of archives links that are building up.\nThe Client works perfectly too with a whole bunch of settings to tinker with.\nOverall 10/10 would do again.\nMy Feeds A list of the feeds the I currently follow can be found here *.\n* YMMV and not guaranteed to be kept up to date\n","permalink":"https://boffinsblog.github.io/posts/self-hosting-freshrss/","summary":"Self-Hosting FreshRSS is kind of pointless as a single user but I\u0026rsquo;m going to do it anyway. Just one more self-hosted app then I\u0026rsquo;ll stop.","title":"Self-Hosting FreshRSS"},{"content":"It’s my music and I’ll stream if I want to, stream if I want to, stream if I want to! This is hands down one the easiest self-hosting project that I’ve installed.\nWhat is Navidrome Streaming? Navidrome can be used as a standalone server, that allows you to browse and listen to your music collection using a web browser.\nIt can also work as a lightweight Subsonic-API compatible server, that can be used with any Subsonic compatible client .\nhttps://www.navidrome.org/docs/overview/ Navidrome Random Albums Screenshot\nWhy choose Navidrome for Streaming? I already have a Jellyfin media server, so I could have used my existing Jellyfin app to listen to my music when out and about.\nHowever, as per the name of this blog, I like tinkering.\nWithout a doubt, setting up the reverse proxy was the most time consuming part for me.\nGiven that there were so few necessary steps, the whole process took about thirty minutes.\n(It probably helped that I already had all of the music folders on the server already.)\nInstalling the Navidrome Server https://www.navidrome.org/docs/installation/linux Create the directory structure The following will create the directory to store the Navidrome executable with the proper permissions.\nsudo install -d -o $USER -g $USER /opt/navidrome\nThen create a Navidrome working directory with the proper permissions.\nsudo install -d -o $USER -g $USER /var/lib/navidrome\nDownload the archive The following will grab the Navidrome release.\nSpecifically the one for my Raspberry Pi.\nwget https://github.com/navidrome/navidrome/releases/download/v0.52.5/navidrome_0.52.5_linux_arm64.tar.gz -O Navidrome.tar.gz\nExtract the archive The following extracts the archive contents to the previously created Navidrome executable directory.\nsudo tar -xvzf Navidrome.tar.gz -C /opt/navidrome/\nTake ownership Then, take ownership of the executable.\nsudo chown -R $USER:$USER /opt/navidrome\nPoint to a music library First, create the config file:\nnano /var/lib/navidrome/navidrome.toml\nThen add:\n# Music location MusicFolder = \u0026#34;/home/foo/music\u0026#34; Create a Navidrome .service file (optional) First, create the config file:\nsudo nano /etc/systemd/system/navidrome.service\nThen add:\n[Unit] Description=Navidrome Music Server and Streamer compatible with Subsonic/Airsonic After=remote-fs.target network.target AssertPathExists=/var/lib/navidrome [Install] WantedBy=multi-user.target [Service] User=foo Group=foo Type=simple ExecStart=/opt/navidrome/navidrome --configfile \u0026#34;/var/lib/navidrome/navidrome.toml\u0026#34; WorkingDirectory=/var/lib/navidrome TimeoutStopSec=20 KillMode=process Restart=on-failure # See https://www.freedesktop.org/software/systemd/man/systemd.exec.html DevicePolicy=closed NoNewPrivileges=yes PrivateTmp=yes PrivateUsers=yes ProtectControlGroups=yes ProtectKernelModules=yes ProtectKernelTunables=yes RestrictAddressFamilies=AF_UNIX AF_INET AF_INET6 RestrictNamespaces=yes RestrictRealtime=yes SystemCallFilter=~@clock @debug @module @mount @obsolete @reboot @setuid @swap ReadWritePaths=/var/lib/navidrome # You can uncomment the following line if you\u0026#39;re not using the jukebox This # will prevent navidrome from accessing any real (physical) devices PrivateDevices=yes # You can change the following line to `strict` instead of `full` if you don\u0026#39;t # want navidrome to be able to write anything on your filesystem outside of # /var/lib/navidrome. ProtectSystem=strict # You can uncomment the following line if you don\u0026#39;t have any media in /home/*. # This will prevent navidrome from ever reading/writing anything there. ProtectHome=true # You can customize some Navidrome config options by setting environment variables here. Ex: #Environment=ND_BASEURL=\u0026#34;/navidrome\u0026#34; Open a port on the firewall (optional depending on firewall usage) First, add a rule to allow Navidrome traffic:\nsudo ufw allow 4533 comment 'Navidrome Traffic'\nThen, check the status:\nsudo ufw status\nOr, to only allow traffic on the internal network:\nsudo ufw allow from 192.168.0.0/24 to any port 4533 comment \u0026quot;Navidrome traffic from LAN only\u0026quot;\nCreate an account Remember that the first account created will have Admin i.e. escalted privileges.\nSet up a reverse proxy (optional) https://blog.yossarian.net/2022/02/02/Setting-up-Navidrome-with-Nginx-as-a-reverse-proxy https://ked.wtf/posts/2022/navidrome-setup-guide First, create a basic nginx config file for Navidrome:\nsudo nano /etc/nginx/sites-available/NAVIDROME\nThen, add:\nserver { listen 80; listen [::]:80; server_name navidrome.example.net; location / { proxy_pass http://\u0026lt;server_ip\u0026gt;:4533; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Protocol $scheme; proxy_set_header X-Forwarded-Host $http_host; proxy_buffering off; } } Then, link:\nsudo ln -s /etc/nginx/sites-available/NAVIDROME /etc/nginx/sites-enabled/\nNow, check the nginx status:\nsudo nginx -t\nFinally, restart nginx:\nsudo systemctl restart nginx\nNow, grab an SSL Certificate:\nsudo certbot\nCertbot will do it’s thing and update the previously created nginx conf.\nThen, check and restart nginx:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl restart nginx\nSet an encryption key (optional) This is a one-time only configuration\nhttps://www.navidrome.org/docs/usage/security/#encrypted-passwords Generate a key (basically a long password):\nhttps://bitwarden.com/password-generator Then, in:\nnano /var/lib/navidrome/navidrome.toml\nAdd:\n# Re-encrypt all existing current passwords (one time thing) PasswordEncryptionKey = \u0026#34;super_long_password\u0026#34; Then, restart and check the status:\nsudo systemctl restart navidrome\nsudo systemctl status navidrome\nWhich Navidrome Streaming Client? The hardest aspect of using Navidrome for me, was choosing a client that I liked.\nAll of my music is stored in folders (Artist – Album Title) and I wanted a client that could reflect this.\nAdditionally I wanted a simple UI.\nAfter an array of clients were trawled through, (one popular Android player has a dizzying amount of customisation options) I settled on Tempo from the F-Droid store.\nBonus! Navidrome will also stream radio stations.\nNavidrome Radio Tab Screenshot\nObviously you will have to add your own stations to the list but here are a couple to get you started:\nLove a Brother Radio – http://23.106.236.229:24914/stream\nRadio Free Fedi Closed\nHowever, many more stations can be added.\n","permalink":"https://boffinsblog.github.io/posts/navidrome-streaming/","summary":"Navidrome is a Server and Streamer compatible with Subsonic/Airsonic Clients.","title":"Navidrome Streaming"},{"content":"Metrics, metrics everywhere Building an Iceshrimp.NET Grafana dashboard.\nWhat is Grafana? Grafana open source software enables you to query, visualize, alert on, and explore your metrics, logs, and traces wherever they are stored.\nhttps://grafana.com/docs/grafana/latest/introduction Iceshrimp.NET Grafana Dashboard Installation Resources:\nhttps://grafana.com/ https://grafana.com/docs/grafana/latest/setup-grafana/installation/debian/ https://www.digitalocean.com/community/tutorials/how-to-install-and-secure-grafana-on-ubuntu-22-04 Update Everything The following may be a bit of overkill but I run this command quite frequently:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade \u0026amp;\u0026amp; sudo apt remove \u0026amp;\u0026amp; sudo apt clean \u0026amp;\u0026amp; sudo apt autoremove \u0026amp;\u0026amp; sudo apt autoclean\nInstall part I sudo apt-get install -y apt-transport-https software-properties-common wget\nAdd Grafana APT repository and key wget -q -O - https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg \u0026gt; /dev/null\necho \u0026quot;deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main\u0026quot; | sudo tee -a /etc/apt/sources.list.d/grafana.list\nInstall part II sudo apt-get update\nsudo apt-get install grafana\nEnable Grafana Server sudo /bin/systemctl daemon-reload\nsudo /bin/systemctl enable grafana-server\nsudo systemctl start grafana-server\nsudo systemctl status grafana-server\nChange listening port Then, if iceshrimp.NET is using the default port, change the Grafana listening port.\nsudo systemctl stop grafana-server\nsudo nano /etc/grafana/grafana.ini\nChange:\n# The http port to use ;http_port = 3000 To:\n# The http port to use # ;http_port = 3000 http_port = \u0026#39;port of your choice\u0026#39; Restart sudo systemctl restart grafana-server\nUpdate firewall sudo ufw allow 'port of your choice' comment \u0026quot;Grafana traffic\u0026quot;\nsudo ufw status verbose\nA slight installation detour Even though Grafana only READs from the database, I prefer to have a dedicated user per process.\nFirst, CREATE a PostgreSQL User with READ-ONLY access to the ‘iceshrimp‘ database:\nsudo -u postgres psql\nCREATE ROLE grafana_user WITH LOGIN PASSWORD 'super_long_password';\nThen, connect as that User:\n\\connect iceshrimp;\nGRANT CONNECT ON DATABASE \u0026quot;iceshrimp\u0026quot; TO grafana_user;\nGRANT USAGE ON SCHEMA public TO grafana_user;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO grafana_user;\nThen, alter the privileges the new User has:\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO grafana_user;\nCreate Account(s) Then, go to the Grafana homepage and create the first Account.\nRemember that by default this will be an Admin (i.e. will have elevated privileges).\nhttp://\u0026lt;server-ip\u0026gt;:port of your choice\nIceshrimp.NET Grafana Dashboard I am aware that Grafana can hook into all sorts of other stuff; this was a learning exercise for me and as such is limited to using the PostgreSQL database as a data source.\nCurrent Grafana Dashboard Links go to the SQL used to pull the data into the Grafana panel.\nMost of the time this was done with the ‘Builder’ tool but a couple of queries are handwritten.\n00. Instance Info: Just a Header really (i.e. the top of the page content) 01. Overview: Federated Instances Shows the total number of Federated instances (i.e. how many instances this installation is connected to)\nBlocked Instances Shows the total number of blocked instances (i.e. how many instances this installation is blocks)\nAccounts Shows the total number of Accounts; this includes the instance.actor (i.e. accounts have been created on this instance)\nNotes Shows the total number of Notes (i.e. how many Notes this installation has processed)\nBlocked Accounts Shows the total number of blocked Accounts that fall outside of blocked Instances (i.e. how many Accounts that have been individually blocked)\nMuted Accounts Total number of muted Accounts (i.e. how many Accounts that have been individually muted)\n02. Jobs: Job Queue by Status Current status of Notes (jobs) being processed (i.e. the current workload in respect of job processing) 03. Notes: Notes Processed by Instance Shows the total number of Notes processed by Instance (i.e. which Instance posted the most Notes)\nNotes Processed by Account and Instance Shows the total number of Notes processed by Account and Instance (i.e. which single Account posted the most Notes per Instance)\n04. Sessions: Active sessions by Account and App Shows the currently authorised (App) sessions by Account (i.e. which Account is currently active and using which App)\nActive sessions by Account and OAuth Token Shows the currently authorised (OAuth Token) sessions by Account (i.e. which Account is currently active and using which OAuth Token)\n05. Misc: Is Cat ? Yes, really (i.e. whether or not Accounts identify as a cat)\nNote Reactions A list of posted Reactions, ranked by usage (i.e. the most used emoji)\nSetup a Sub Domain for the Dashboard Resources:\nhttps://grafana.com/tutorials/run-grafana-behind-a-proxy First, set up a basic nginx grafana.conf to ensure domain resolution works:\nsudo nano /etc/nginx/sites-available/grafana.conf\nserver { listen 80; listen [::]:80; server_name grafana.example.net; location / { root /usr/share/nginx/html; index index.html index.htm; } } Then, enable the new site by linking:\nsudo ln -s /etc/nginx/sites-available/grafana.conf /etc/nginx/sites-enabled/\nThen, restart nginx:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl restart nginx\nSecondly, setup a basic non-SSL nginx conf file for Grafana:\nsudo nano /etc/nginx/sites-available/grafana.conf\nmap $http_upgrade $connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } upstream grafana { server localhost:3000; } server { server_name grafana.example.net; root /usr/share/nginx/html; index index.html index.htm; location / { proxy_set_header Host $host; proxy_pass http://grafana; } # Proxy Grafana Live WebSocket connections. location /api/live/ { proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_set_header Host $host; proxy_pass http://grafana; } } Check and restart nginx:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl restart nginx\nThen, restart and check Grafana:\nsudo systemctl restart grafana-server\nsudo systemctl status grafana-server\nFinally, generate up a Grafana SSL certificate.\nsudo certbot\nThe result (i.e. successful) should look something like this:\nSuccessfully received certificate. Certificate is saved at: /etc/letsencrypt/live/viz.example.org/fullchain.pem Key is saved at: /etc/letsencrypt/live/viz.example.org/privkey.pem Check and restart nginx:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl restart nginx\nThen, restart and check Grafana:\nsudo systemctl restart grafana-server\nsudo systemctl status grafana-server\nThere are a slew of Grafana posts on Google about updating the grafani.ini to get SSL functioning properly.\nHowever I did none of those things and it seems to be functioning quite well.\nIceshrimp.NET Grafana Dashboard Screenshot\n","permalink":"https://boffinsblog.github.io/posts/iceshrimp-net-grafana-dashboard/","summary":"Building an Iceshrimp.NET Grafana dashboard. Install Grafana, connect to the iceshrimp.NET database, build a metrics dashboard.","title":"Iceshrimp.NET Grafana Dashboard"},{"content":"Real ‘Shrimp Syndication Turning RSS feeds into an Iceshrimp.NET bot driven timeline.\nThis is a much simpler reworking of the Mastodon rss-to-toot scripts.\nWhy ?\nMastodon has a low character count per toot.\n(I am aware that character count is instance specific; don’t contact me to correct me.)\nAlso, I had to create several variants of the script to allow toots to fit this character limit.\nThen, choose the correct variant to use, to prevent toots from exceeding the character limit.\nDoing this meant that some toots were sent with, say, the RSS ‘Description’ excluded.\nMy Iceshrimp.NET instance character limit per Note allows for much longer posts.\nSo, no variants needed.\nFeeding the Bot The overall workflow concept is quite straightforward.\nThe python code takes an RSS feed.\nIt takes the latest RSS post in the feed.\nIf that RSS post is different to the last Note sent, it crafts a new Iceshrimp.NET Note announcing the new RSS content, and updates the Iceshrimp.NET timeline.\nA cron job then runs every 15 minutes to loop through this workflow, picking up and sending new additions to the RSS feed to the Iceshrimp.NET timeline.\nPrerequisites to installing the Iceshrimp.NET RSS Bot Create a new account Firstly, and while it may be stating the obvious, there needs to be an account to send the Notes FROM.\nObviously you can skip this part if you intend to use an existing account.\nTurn Registrations ON:\nnano iceshrimp.net/Iceshrimp.Backend/configuration.ini\n;; Whether to allow instance registrations ;; Options: [Closed, Invite, Open] #Registrations = Closed Registrations = Open Then, in a Terminal send a POST request:\ncurl -X \u0026#39;POST\u0026#39; \\ \u0026#39;http://localhost:3000/api/iceshrimp/auth/register\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Host: shrimp.example.org\u0026#39; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;account_name_goes_here\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;super_long_password_goes_here\u0026#34; }\u0026#39; The output should be something like this:\n{\u0026quot;status\u0026quot;:\u0026quot;authenticated\u0026quot;,\u0026quot;isAdmin\u0026quot;:false,\u0026quot;isModerator\u0026quot;:false,\u0026quot;token\u0026quot;:\u0026quot;this_is_the_TOKEN\u0026quot;,\u0026quot;user\u0026quot;:{\u0026quot;id\u0026quot;:\u0026quot;9fqw78arpr8rv21m\u0026quot;,\u0026quot;username\u0026quot;:\u0026quot;account_name_goes_here\u0026quot;,\u0026quot;host\u0026quot;:null,\u0026quot;displayName\u0026quot;:null,\u0026quot;avatarUrl\u0026quot;:\u0026quot;https://shrimp.example.org/identicon/9fqw78arpr8rv21m\u0026quot;,\u0026quot;bannerUrl\u0026quot;:null,\u0026quot;instanceName\u0026quot;:\u0026quot;example.org\u0026quot;,\u0026quot;instanceIconUrl\u0026quot;:null}}\nMake a note of the Authentication TOKEN Secondly, make a note of the TOKEN; it will be difficult to recreate this Terminal output.\nThis bit is the Authentication TOKEN:\n\u0026quot;token\u0026quot;:\u0026quot;this_is_the_TOKEN\u0026quot;\nThen, turn the Registrations back OFF:\nnano iceshrimp.net/Iceshrimp.Backend/configuration.ini\n;; Whether to allow instance registrations ;; Options: [Closed, Invite, Open] Registrations = Closed #Registrations = Open Test the TOKEN works Finally, test to see that it all works.\nIn a Terminal post something to the instance using the TOKEN provided:\ncurl -X \u0026#39;POST\u0026#39; \\ \u0026#39;https://shrimp.example.org/api/iceshrimp/notes\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#34;Authorization: Bearer this_is_the_TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;text\u0026#34;: \u0026#34;Test Post\u0026#34;, \u0026#34;visibility\u0026#34;: \u0026#34;public\u0026#34; }\u0026#39; A couple of notes.\nFirstly, the ‘text‘ parameter is required; this is the ‘body’ of the Note.\nSecondly, the ‘visibility‘ parameter is required; the Note will fail to post without it.\nFiles for running the Iceshrimp.NET RSS Bot Keep it secret, keep it safe The Python file that contains the access TOKEN and other needed connection info:\nsecrets.py ICESHRIMP_URL = \u0026#39;https://shrimp.example.org\u0026#39; ICESHRIMP_TOKEN = \u0026#39;this_is_the_TOKEN\u0026#39; ICESHRIMP_NOTE = ICESHRIMP_URL + \u0026#39;/api/iceshrimp/notes\u0026#39; FEED_URL = \u0026#39;rss_feed_url_goes_here\u0026#39; GAME_HASHTAGS = \u0026#39;game_hashtags_go_here\u0026#39; NOTE_HASHTAGS = \u0026#39;#hashtag #hashtag #hashtag\u0026#39; FILE_PATH = \u0026#39;/home/foo/full_filepath_goes_here/\u0026#39; Sending a single Iceshrimp.NET Note This will populate an Iceshrimp timeline with a single item of RSS content.\nThe script below will post only the latest item in the RSS feed:\nbot_no_image.py #!/usr/bin/env python # -*- coding: utf-8 -*- # A bot that takes an RSS feed and posts to iceshrimp.NET # Parses the latest entry in the RSS feed and if it doesn\u0026#39;t match the last note, sends the latest entry from secrets import * from pathlib import Path import requests import feedparser RSS_FEED = feedparser.parse(FEED_URL) RSS_ENTRY = RSS_FEED.entries[0] # Function to build the note, send it and update the \u0026#39;last_note.txt\u0026#39; file def send_note(): # Build the headers including the ICESHRIMP_TOKEN headers = { \u0026#39;accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39; : f\u0026#39;Bearer {ICESHRIMP_TOKEN}\u0026#39;, } # Build the text part of the note from the chosen \u0026#39;RSS_ENTRY\u0026#39; note_str = \u0026#39;\u0026#39; note_str += f\u0026#34;{RSS_ENTRY[\u0026#39;title\u0026#39;]}\\n\\n\u0026#34; note_str += f\u0026#34;{RSS_ENTRY[\u0026#39;description\u0026#39;]}\\n\u0026#34; note_str += f\u0026#34;\\n\\n{RSS_ENTRY[\u0026#39;link\u0026#39;]}\\n\\n\u0026#34; note_str += f\u0026#34;\\n\\nThis is an automated post from the {GAME_HASHTAGS} RSS feed dated {RSS_ENTRY[\u0026#39;published\u0026#39;]}\\n\\n\u0026#34; note_str += f\u0026#34;\\n\\n{NOTE_HASHTAGS}\\n\\n\u0026#34; # Build the payload payload = { \u0026#39;text\u0026#39;: note_str, \u0026#39;visibility\u0026#39;: \u0026#39;public\u0026#39;, } # POST the payload to the ICESHRIMP_URL response = requests.post(ICESHRIMP_NOTE, headers=headers, json=payload) # Update the \u0026#39;last_note.txt\u0026#39; file with \u0026#39;latest_entry\u0026#39; last_note = open(Path(FILE_PATH) / \u0026#34;last_note.txt\u0026#34;,\u0026#34;w\u0026#34;) last_note.write(latest_entry) last_note.close() # Update the \u0026#39;last_rss.txt\u0026#39; file with \u0026#39;published\u0026#39; date last_rss = open(Path(FILE_PATH) / \u0026#34;last_rss.txt\u0026#34;,\u0026#34;w\u0026#34;) last_rss.write(published) last_rss.close() # Output a response print(\u0026#34;New note sent from the \u0026#34; + GAME_HASHTAGS + \u0026#34; RSS feed!\u0026#34;) # Find the latest RSS entry latest_entry = RSS_ENTRY.id # Find the latest published date published = RSS_ENTRY.published # Find the \u0026#39;last_note\u0026#39; sent with open(Path(FILE_PATH) / \u0026#34;last_note.txt\u0026#34;) as last_note: last_note = last_note.read() # If \u0026#39;latest_entry\u0026#39; does not equal \u0026#39;last_note\u0026#39; then send a new note based on \u0026#39;latest_entry\u0026#39; if latest_entry != last_note: send_note() elif latest_entry == last_note: # Output a response print(\u0026#34;Nothing new to add from the \u0026#34; + GAME_HASHTAGS + \u0026#34; RSS feed!\u0026#34;) Sending a stream of Notes What if we want to populate an Iceshrimp timeline with a whole stream of RSS content?\nThe following works in a similar way to the ‘Sending a single Iceshrimp Note’ approach, except it starts at the bottom of the RSS feed.\nAnd then loops upwards through the feed items.\nThat way, the latest RSS post will never be the latest Note, until the Python loop has reached the start of the RSS content.\nThe script below will cycle through all of the items in the RSS feed:\nbot_all_no_image.py #!/usr/bin/env python # -*- coding: utf-8 -*- # A bot that takes an RSS feed and posts to iceshrimp.NET # Parses the latest entry in the RSS feed and if it doesn\u0026#39;t match the last note, sends the latest entry from secrets import * from pathlib import Path import requests import feedparser import time RSS_FEED = feedparser.parse(FEED_URL) # Function to build the note, send it and update the \u0026#39;last_note.txt\u0026#39; file def send_note(): # Build the headers including the ICESHRIMP_TOKEN headers = { \u0026#39;accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39; : f\u0026#39;Bearer {ICESHRIMP_TOKEN}\u0026#39;, } # Build the text part of the note from the chosen \u0026#39;RSS_ENTRY\u0026#39; note_str = \u0026#39;\u0026#39; note_str += f\u0026#34;{RSS_ENTRY[\u0026#39;title\u0026#39;]}\\n\\n\u0026#34; note_str += f\u0026#34;{RSS_ENTRY[\u0026#39;description\u0026#39;]}\\n\u0026#34; note_str += f\u0026#34;\\n\\n{RSS_ENTRY[\u0026#39;link\u0026#39;]}\\n\\n\u0026#34; note_str += f\u0026#34;\\n\\nThis is an automated post from the {GAME_HASHTAGS} RSS feed dated {RSS_ENTRY[\u0026#39;published\u0026#39;]}\\n\\n\u0026#34; note_str += f\u0026#34;\\n\\n{NOTE_HASHTAGS}\\n\\n\u0026#34; # Build the payload payload = { \u0026#39;text\u0026#39;: note_str, \u0026#39;visibility\u0026#39;: \u0026#39;public\u0026#39;, } # POST the payload to the ICESHRIMP_URL response = requests.post(ICESHRIMP_NOTE, headers=headers, json=payload) # Update the \u0026#39;last_note.txt\u0026#39; file with \u0026#39;latest_entry\u0026#39; last_note = open(Path(FILE_PATH) / \u0026#34;last_note.txt\u0026#34;,\u0026#34;w\u0026#34;) last_note.write(latest_entry) last_note.close() # Update the \u0026#39;last_rss.txt\u0026#39; file with \u0026#39;published\u0026#39; date last_rss = open(Path(FILE_PATH) / \u0026#34;last_rss.txt\u0026#34;,\u0026#34;w\u0026#34;) last_rss.write(published) last_rss.close() # How many RSS entries are there (-1 as the indexing starts at 0) count = len(feedparser.parse(FEED_URL)[\u0026#39;entries\u0026#39;])-1 # Loop through all of the RSS entries (until it is less than 0, where the indexing started) print(\u0026#34;There are\u0026#34;, count+1, \u0026#34;RSS entries to send\u0026#34;) while count != -1: latest_entry = RSS_FEED.entries[count].id # Find the published date published = RSS_FEED.entries[count].published RSS_ENTRY = RSS_FEED.entries[count] #print(\u0026#34;Sending - \u0026#34; + RSS_FEED.entries[count].title, count, \u0026#34;notes remaining\u0026#34;) print(\u0026#34;Sending - \u0026#34; + RSS_FEED.entries[count].title + \u0026#34; -\u0026#34;, count, \u0026#34;notes remaining\u0026#34;) send_note() count = count -1 time.sleep(1) # Output a response print(\u0026#34;All done!\u0026#34;) RSS items and Note logging A couple of files are needed to keep track of:\nWhat the last item in an RSS feed is\nWhat the last Note sent was\nIt is by comparing these contents of these two files that the script decides whether create and send a new Note.\nThe text file that contains the ID of the last RSS entry:\nlast_rss.txt This file can start empy, it will be written to The text file that contains the ID of the last RSS entry that was posted as a Note:\nlast_note.txt This file can start empy, it will be written to Deploying the Iceshrimp.NET RSS Bot I prefer to have all of the files for a given feed in the same directory.\nHowever there is nothing to stop you having multiple bot folders, each with their own RSS feed and TOKEN.\nI have more than one Python script set up and they are all called from a bash script every 15 minutes using cron.\ncron job This is what calling one of those jobs looks like.\n# Run the Iceshrimp bot that turn RSS feeds into notes every N minutes */5 * * * * python /home/foo/bot_with_image.py \u0026amp; However, these individual commands could potentially be contained in a single bash script and have cron execute that instead.\nCode examples are at .\n","permalink":"https://boffinsblog.github.io/posts/iceshrimp-net-rss-bot/","summary":"Turn an RSS feed into an Iceshrimp note. Send either the latest RSS entry or a stream of Iceshrimp posts.","title":"Iceshrimp.NET RSS Bot"},{"content":"I ‘Fin-ally did it ! Jellyfin has a supported app, however it hasn’t made it into the Samsung store. This is about how to install the Jellyfin app on a Tizen TV.\nThere are a multitude of posts in the Interwebs about ‘How to install Jellyfin’; this is not another one of those.\nDownload the Tizen Studio First, download the appropriate version from:\nhttps://developer.tizen.org/development/tizen-studio/download Then, install, I used the ‘Tizen Studio 5.6 with IDE installer’ for Ubuntu.\nInstall Tizen Studio Resources:\nhttps://docs.tizen.org/application/tizen-studio/setup/install-sdk First, create an installation location:\nmkdir /home/foo/Tizen/\nThen, make the download executable:\ncd /home/foo/Downloads/\nsudo chmod a+x web-ide_Tizen_Studio_5.6_ubuntu-64.bin\nFinally, install:\n./web-ide_Tizen_Studio_5.6_ubuntu-64.bin\nSince the installation process prompts for the SDK and data location.\nI set my SDK location to:\n/home/foo/Tizen\nThen I set my Data location to:\n/home/foo/Tizen/tizen-studio-data\nA warning:\n“This process can take some time…” and has multiple su password prompts.\nWait while the Tizen Studio is installed.\nThen, launch the ‘Package Manager’.\nAnother warning:\n“This process can take some time…” and has multiple su password prompts.\nTizen Package Manager Main SDK Options\nInstall the required extensions Resources:\nhttps://developer.samsung.com/smarttv/develop/getting-started/setting-up-sdk/installing-tv-sdk.html#Installing-Required-Extensions In the ‘Extension SDK’ tab:\nDownload the following:\nTV Extensions 8.0\nTizen Studio Package Manager – TV Extensions – Extension SDK\nSamsung Certificate Extension\nTizen Studio Package Manager – Certificate Extension – Extension SDK\nAnother warning:\n“This process can take some time…” and has multiple su password prompts.\nYou may want to copy / paste the password, due to there being that many prompts.\nIn the meantime, go and make a brew, it takes a long time for some reason.\nThen, exit the ‘Package Manager’ and launch the ‘Tizen Studio’.\nAlso, ‘Tizen Studio’ needs a ‘workspace’:\nmkdir /home/foo/Tizen/workspace\nThen, browse to the ‘workspace’ created and set as ‘Default’.\nSetup Tizen certificate(s) in Certificate Manager Tizen Certificate Manager Dialog Box\nAs I had gone through this process quite recently and repeatedly, I imported my existing Certificate(s).\nThe process of creating a new pair of Certificate(s) needs two additional pieces of information:\nThe TV UDID, which on my device was found at:\n‘Settings | Support | About this TV’\nA Samsung Account which I already had\nResources:\nhttps://developer.samsung.com/smarttv/develop/getting-started/setting-up-sdk/creating-certificates.html First, create a Certificate Profile:\nA certificate profile consists of an author certificate and 1 or more distributor certificates.\nInitial Tizen Certificate Manager Dialog Box\nTo create a certificate profile:\nIn the Tizen Studio menu, select “Tools; Certificate Manager”.\nCreate Certificate Profile – Create a TV Certificate Profile\nThen choose the ‘TV’ option, obviously.\nCreate Certificate Profile – Create New Certificate Profile\n(Even if importing existing certificated, a new profile seems to need creating, the ‘import’ is the the step following this one.)\nI already had an ‘author certificate’.\nSo I chose, ‘Select an existing author certificate’.\nCreate Certificate Profile – Select Existing Author Certificate\nThen browsed to its location.\nCreate Certificate Profile – Browse for Author Certificate\nI already had a ‘distributor certificate’.\nSo I chose, ‘Select an existing distributor certificate’.\nCreate Certificate Profile – Existing Distributor Certificate\nSuccess!\nTizen Certificate Profile Creation Success\nMore success!\nTizen Certificate Success\nDownload the Jellyfin App for Tizen Resources:\nhttps://github.com/jellyfin/jellyfin-tizen https://github.com/jeppevinkel/jellyfin-tizen-builds/releases It is entirely possible to download and make an executable which can then be uploaded to the TV.\nHowever, I choose to use the prebuilt ones.\nEnable ‘Developer Mode’ on TV Before an App can be uploaded to the TV, it needs to be in ‘Developer Mode.\nOn the TV, select the “Apps” panel.\nThen, in the “Apps” panel, enter “12345”\nFor some reason, I had to use the Smart Things App on my phone to successfully complete this step.\nThen the developer mode configuration popup will appear.\nSwitch “Developer mode” to “On”.\nThen enter the IP address of the computer that you want to connect to the TV, and click “OK”.\nFinally, reboot the TV by holding the power button for a few seconds.\nInstall the Jellyfin App on a Tizen TV using the SDK Resources:\nhttps://developer.samsung.com/smarttv/develop/getting-started/setting-up-sdk/installing-tv-sdk.html Using the SDK ‘Device Manager’ never really worked for me.\nTizen Studio Device Manger\nI would always get the following error when trying to upload the Jellyfin package:\nTizen Studio App Install Error\nSo, plan B.\nUse sdb instead.\nInstall the Jellyfin App on a Tizen TV using SDB The Tizen Studio comes with sdb.\nsdb is the Samsung version of adb.\nEnabling sdb Add sdb to $PATH Resources:\nhttps://linuxize.com/post/how-to-add-directory-to-path-in-linux/ Use sdb one time export PATH=/home/foo/Tizen/tools:$PATH\nexport PATH=/home/foo/Tizen/tools/ide/bin:$PATH\nOr, make sdb persistent Edit:\nnano ~/.bashrc\nThen, add at the bottom:\n# Tizen tools export PATH=/home/foo/Tizen/tools:$PATH export PATH=/home/foo/Tizen/tools/ide/bin:$PATH Then, reload:\nsource ~/.bashrc\nNow you can test (assuming that you know the IP of the TV):\nFirst, connect to the TV:\nsdb connect \u0026lt;TV.IP\u0026gt;:26101\nThen, see the devices connected and available:\nsdb devices\nWhen finished:\nsdb disconnect\nNow that sdb is working you can upload the Jellyfin package.\nFirst, connect to the TV:\nsdb connect \u0026lt;TV.IP\u0026gt;:26101\nThen, see the devices connected and available:\nsdb devices\nMake a note of the ‘tizen_device_id‘, under the ‘List of devices attached’.\nIn a Terminal, browse to where the Jellyfin package was downloaded to:\ncd /path/to/Jellyfin_for_Tizen_Download\nThen, install:\ntizen install -n Jellyfin_xx.x.x.wgt -t tizen_device_id\nWhere ‘Jellyfin_xx.x.x.wgt‘ is the version number that was downloaded.\nWhen finished:\nsdb disconnect\nFinally, a bit of housekeeping.\nIt is good form to keep the App version number and the Server version number in sync.\nIn order to find the Server version, in a Terminal:\njellyfin --version\nRemember when i said it was a long process?\nIn conclusion, it’s a bit of a faff to do all of the above when the TV browser can do exactly the same thing.\n","permalink":"https://boffinsblog.github.io/posts/jellyfin-app-on-a-tizen-tv/","summary":"Jellyfin has a supported app, however it hasn’t made it into the Samsung store. This is about how to install the Jellyfin app on a Tizen TV.","title":"Jellyfin App on a Tizen TV"},{"content":"But will it run Crysis? Take some OpenWrt with a dash of AdGuardHome, add a splash of Unbound and install it all on a SheevaPlug; will it be a recipe for disaster?\nI have a SheevaPlug Dev Kit from 2009 (?).\nFor many years it ran a host of static and WordPress websites, booting from a multimedia card (a 32 GB MMC running Ubuntu).\nBut my SheevaPlug was retired a few years ago in favour of a RasPi4 with an solid state drive (a 1 TB SSD running Ubuntu Server).\nSo, I decided to bring the ‘Plug back to life and use it as a DNS server, VPN and for Pi-hole.\nWhat on Earth is a SheevaPlug? The SheevaPlug is a “ plug computer ” designed to allow standard computing features in as small a space as possible. It was a small embedded Linux ARM computer without a display which can be considered an early predecessor to the subsequent Raspberry Pi .\nhttps://en.wikipedia.org/wiki/SheevaPlug OpenWrt? Never heard of it The OpenWrt Project is a Linux operating system targeting embedded devices.\nhttps://openwrt.org/ Prerequisites Note:\nSome of the content below is taken directly from the source documents.\nThese commands were correct at the time this page was published.\nIf you are going to follow my notes it would be prudent to check the source documents for updates and additions.\nTFTP server Trivial File Transfer Protocol (TFTP) is a simple lockstep File Transfer Protocol which allows a client to get a file from or put a file onto a remote host .\nhttps://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol It is entirely possible to do all of the updating from USB but having done both, I find that pulling files on to the SheevaPlug from my local machine (which is acting as the TFTP server) to be the far easier route.\nA TFTP server can be installed by running the following in a Terminal:\nsudo apt install tftpd-hpa\nThen, check that it is running:\nsudo systemctl status tftpd-hpa\nThen, take ownership of the TFTP server directory:\nsudo chown -R $USER /srv/tftp\nAccordingly, any files now placed in this directory will be available to pull onto the Plug.\nA working u-boot The following process assumes that there is a working u-boot in place, and that the SheevaPlug can actually boot.\nThere is another post about how to unbrick a SheevaPlug if that needs to be done prior to following this post.\nHowever, if a new u-boot is needed, follow these steps.\nUpdating u-boot if needed First, plug in the ‘Plug and power up.\nIn Terminal 1 run the following:\nscreen /dev/ttyUSB0 115200\nThen, interrupt the boot process to get to the ‘Plug command prompt and set the ‘Plug IP:\nsetenv ipaddr 192.168.0.55\nNow, set the local machine’s IP (or the TFTP server IP):\nsetenv serverip 192.168.0.205\nThen, set the ‘Plug MAC Address:\nsetenv ethaddr 00:50:43:01:63:EA\nFinally, boot from the u-boot.kwb that was placed in the TFTP directory:\ntftpboot u-boot.kwb\nThe output should be similar to the below.\n=\u0026gt; tftpboot u-boot.kwb Using egiga0 device TFTP from server 192.168.0.205; our IP address is 192.168.0.55 Filename \u0026#39;u-boot.kwb\u0026#39;. Load address: 0x800000 Loading: ################################### 625 KiB/s done Bytes transferred = 502516 (7aaf4 hex) =\u0026gt; Wipe and then Write to NAND Firstly, wipe the NAND:\nnand erase 0x0 0x100000\n=\u0026gt; nand erase 0x0 0x100000 NAND erase: device 0 offset 0x0, size 0x100000 Erasing at 0xe0000 -- 100% complete. OK =\u0026gt; Secondly, write the new u-boot to NAND:\nnand write 0x800000 0x0 0x100000\n=\u0026gt; nand write 0x800000 0x0 0x100000 NAND write: device 0 offset 0x0, size 0x100000 1048576 bytes written: OK =\u0026gt; Finally, reboot the ‘Plug:\nreset\nThen, interrupt the boot process to get to the ‘Plug command prompt.\nThe new u-boot will now need its environment variables re-setting.\nRun the following in a Terminal to set the ‘Plug IP:\nsetenv ipaddr 192.168.0.55\nThen, set the local machine’s IP (or the tftp server IP):\nsetenv serverip 192.168.0.205\nThen, set the ‘Plug MAC Address:\nsetenv ethaddr 00:50:43:01:63:EA\nFinally, save the current set of environment variables:\nsaveenv\n=\u0026gt; saveenv Saving Environment to NAND... Erasing NAND... Erasing at 0x80000 -- 100% complete. Writing to NAND... OK OK =\u0026gt; Install OpenWrt Resources https://downloads.openwrt.org/releases/23.05.0-rc4/targets/kirkwood/generic/ https://downloads.openwrt.org/releases/23.05.0-rc4/targets/kirkwood/generic/openwrt-23.05.0-rc4-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin https://downloads.openwrt.org/releases/23.05.0-rc4/targets/kirkwood/generic/openwrt-23.05.0-rc4-kirkwood-generic-globalscale_sheevaplug-squashfs-sysupgrade.bin Boot the ‘Plug Interrupt the boot process to get to the ‘Plug command prompt.\nThen boot the ‘Plug from the OpenWrt .bin file:\ntftpboot openwrt-23.05.0-rc4-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin\n=\u0026gt; tftpboot openwrt-23.05.0-rc4-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin Using egiga0 device TFTP from server 192.168.0.205; our IP address is 192.168.0.55 Filename \u0026#39;openwrt-23.05.0-rc4-kirkwood-generic-globalscale_sheevaplug-squashfs-factory.bin\u0026#39;. Load address: 0x800000 Loading: ################################################################# ################################################################# ################################################################# ################################################################# ################################################################# ################################################################# ### 694.3 KiB/s done Bytes transferred = 5767168 (580000 hex) =\u0026gt; Now, erase the part of NAND where OpenWrt is to be installed:\nnand erase.part ubi\n=\u0026gt; nand erase.part ubi NAND erase.part: device 0 offset 0x100000, size 0x1ff00000 Skipping bad block at 0x169c0000 Erasing at 0x1ffe0000 -- 100% complete. OK =\u0026gt; Then, write to those sectors:\nnand write 0x800000 ubi 0x600000\n=\u0026gt; nand write 0x800000 ubi 0x600000 NAND write: device 0 offset 0x100000, size 0x600000 6291456 bytes written: OK =\u0026gt; Finally, reboot the ‘Plug:\nreset\nAfter booting there should be the OpenWrt prompt.\nOpenWrt BusyBox Prompt\nPost OpenWrt Installation Find the SheevaPlug IP on the network, I use ‘ fing ‘.\nLogin via SSH ssh root@\nChange the password passwd\nUpdate the firmware cd /tmp\nwget https://downloads.openwrt.org/releases/23.05.0/targets/kirkwood/generic/openwrt-23.05.0-kirkwood-generic-globalscale_sheevaplug-squashfs-sysupgrade.bin\nsysupgrade openwrt-23.05.0-kirkwood-generic-globalscale_sheevaplug-squashfs-sysupgrade.bin\nUpdate existing packages OpenWrt uses the opkg package management system.\nOpkg is a full package manager for the root file system, including kernel modules and drivers\nhttps://openwrt.org/docs/guide-user/additional-software/opkg Run the following in a Terminal to update:\nopkg update\nThen, update all of the installed packages:\nopkg list-upgradable | cut -f 1 -d ' ' | xargs opkg upgrade\n(Might need 2 passes)\nInstall a few extra packages These are pretty much the packages that I use all the time.\nnano for editing text files\nmc or Midnight Commander for a visual file manager\nhtop for viewing running processes\nRun the following in a Terminal to update:\nopkg update\nThen, install the packages:\nopkg install nano\nopkg install mc\nopkg install htop\nSet the hostname Run the following in a Terminal to edit the config file:\nnano /etc/config/system\nThen, change:\noption hostname 'OpenWrt'\nTo:\noption hostname 'sheevaplug'\nThen, restart:\nreboot\nSet a fixed IP Run the following in a Terminal to edit the config file:\nnano /etc/config/network\nThen, change where needed / appropriate for your network:\nconfig interface \u0026#39;lan\u0026#39; option device \u0026#39;br-lan\u0026#39; option proto \u0026#39;static\u0026#39; option ipaddr \u0026#39;192.168.0.55\u0026#39; option netmask \u0026#39;255.255.255.0\u0026#39; option gateway \u0026#39;192.168.0.1\u0026#39; option broadcast \u0026#39;192.168.0.255\u0026#39; list dns \u0026#39;8.8.8.8\u0026#39; /etc/init.d/network restart\nAdd a non-root user ‘foo‘ who can ‘su‘\nhttps://openwrt.org/docs/guide-user/additional-software/create-new-users Install the packages needed to create new users Run the following in a Terminal to update:\nopkg update\nThen, install the packages:\nopkg install shadow-useradd shadow-su\nCreate a new ‘foo’ user useradd -m -s /bin/ash foo\nSet a new user ‘foo’ password passwd foo\nAdd the SSH Keys and secure login https://openwrt.org/docs/guide-user/security/dropbear.public-key.auth Firstly, make a .ssh dir for ‘foo‘:\nmkdir /home/foo/.ssh\nSecondly, add the key ‘id_rsa_sheevaplug.pub‘:\nnano /home/foo/.ssh/authorized_keys\nFinally, restart the SSH service service log restart; service dropbear restart\nTest that you can connect to the ‘Plug from another machine ssh -o PasswordAuthentication=no sheevaplug\nAnd:\nssh -o PasswordAuthentication=no -o PubkeyAcceptedKeyTypes=ssh-rsa sheevaplug\nHarden security Disable password authentication uci set dropbear.@dropbear[0].PasswordAuth=\u0026quot;0\u0026quot;\nDisable logging in with root privileges uci set dropbear.@dropbear[0].RootPasswordAuth=\u0026quot;0\u0026quot;\nCommit the changes uci commit dropbear\nThen, restart service dropbear restart\nTest SSH Security Logging in as a normal user:\nssh foo@192.168.0.55\nShould fail with a message like this:\nfoo@192.168.0.55: Permission denied (publickey).\nLogging in as a root user:\nssh root@192.168.0.55\nShould fail with a message like this:\nroot@192.168.0.55: Permission denied (publickey).\nHowever, logging in using the SSH key should work:\nssh sheevaplug\nUser ‘foo‘ can then ‘su‘.\nInstall AdGuardHome AdGuard Home is a network-based solution for blocking ads and trackers.\nhttps://adguard.com/en/adguard-home/overview.html https://openwrt.org/docs/guide-user/services/dns/adguard-home AdGuardHome Dashboard Screenshot\nRun the following in a Terminal to update:\nopkg update\nThen install AdGuardHome:\nopkg install adguardhome\nMake AdGuardHome the Primary DNS After installing the opkg package, run the following commands through SSH to prepare for making AdGuard Home the primary DNS resolver.\nDNS and DHCP are a bit of a hole in my knowledge so the majority of the following commands came from:\nhttps://openwrt.org/docs/guide-user/services/dns/adguard-home Get the first IPv4 and IPv6 Address of router and store them in following variables for use during the script:\nNET_ADDR=$(/sbin/ip -o -4 addr list br-lan | awk 'NR==1{ split($4, ip_addr, \u0026quot;/\u0026quot;); print ip_addr[1] }')\nNET_ADDR6=$(/sbin/ip -o -6 addr list br-lan scope global | awk 'NR==1{ split($4, ip_addr, \u0026quot;/\u0026quot;); print ip_addr[1] }')\nSee the results of the above changes:\necho \u0026quot;Router IPv4 : \u0026quot;\u0026quot;${NET_ADDR}\u0026quot;\necho \u0026quot;Router IPv6 : \u0026quot;\u0026quot;${NET_ADDR6}\u0026quot;\nConfigure the AdGuardHome DHCP server Again, the following commands come from:\nhttps://openwrt.org/docs/guide-user/services/dns/adguard-home Enable dnsmasq to do PTR requests:\nuci set dhcp.@dnsmasq[0].noresolv=\u0026quot;0\u0026quot;\nReduce dnsmasq cache size as it will only provide PTR/rDNS info:\nuci set dhcp.@dnsmasq[0].cachesize=\u0026quot;1000\u0026quot;\nDisable rebind protection. Filtered DNS service responses from blocked domains are 0.0.0.0 which causes dnsmasq to fill the system log with possible DNS-rebind attack detected messages:\nuci set dhcp.@dnsmasq[0].rebind_protection='0'\nMove dnsmasq to port 54:\nuci set dhcp.@dnsmasq[0].port=\u0026quot;54\u0026quot;\nSet Ipv4 DNS advertised by option 6 DHCP:\nuci -q delete dhcp.@dnsmasq[0].server\nSet Ipv6 DNS advertised by DHCP:\nuci add_list dhcp.@dnsmasq[0].server=\u0026quot;${NET_ADDR}\u0026quot;\nThen delete existing configs, ready to install new options:\nuci -q delete dhcp.lan.dhcp_option\nuci -q delete dhcp.lan.dns\nMore DHCP configuration Again, the following commands come from:\nhttps://openwrt.org/docs/guide-user/services/dns/adguard-home DHCP option 6: which DNS (Domain Name Server) to include in the IP configuration for name resolution:\nuci add_list dhcp.lan.dhcp_option='6,'\u0026quot;${NET_ADDR}\u0026quot;\nDHCP option 3: default router or last resort gateway for this interface:\nuci add_list dhcp.lan.dhcp_option='3,'\u0026quot;${NET_ADDR}\u0026quot;\nSet IPv6 Announced DNS:\nfor OUTPUT in $(ip -o -6 addr list br-lan scope global | awk \u0026#39;{ split($4, ip_addr, \u0026#34;/\u0026#34;); print ip_addr[1] }\u0026#39;) do echo \u0026#34;Adding $OUTPUT to IPV6 DNS\u0026#34; uci add_list dhcp.lan.dns=$OUTPUT done Commit the changes and restart:\nuci commit dhcp\n/etc/init.d/dnsmasq restart\nFinally, setup AdGuardHome AdGuard Home has it’s own web interface for configuration and management and is not managed through LuCI.\nhttps://openwrt.org/docs/guide-user/services/dns/adguard-home#web_interface AdGuardHome is accessed through the web interface.\nOn first time setup the default web interface port is TCP 3000.\nGo to http://192.168.0.55:3000/\n(This is the ‘Plug IP in my case, not the Router IP as my Router sends ALL traffic to the ‘Plug.)\nFirst, setup the Admin Web Interface to listen on all interfaces, port 8080.\nThen set the DNS server to listen on all interfaces, port 53.\nFinally, create an AdGuardHome user and choose a strong password.\nLogin to AdGuardHome Go to http://192.168.0.55:8080/\n(Again, this is the ‘Plug IP in my case.)\nFirst, setup the DHCP server.\nThen turn off DHCP on the router.\nFinally, reboot the router.\nMy SheevaPlug is now acting as my DHCP server!\nAdGuardHome DHCP Settings Screenshot\nUse Unbound and odhcpd Unbound is a validating, recursive, and caching DNS resolver.\nhttps://openwrt.org/docs/guide-user/services/dns/unbound Why use Unbound?\nDependence on the upstream resolver can be cause for concern. It is often provided by the ISP, and some users have switched to public DNS providers. Either way can result in problems due to performance, hijacking, trustworthiness, or several other reasons. Running a recursive resolver is a solution.\nhttps://openwrt.org/docs/guide-user/services/dns/unbound Remove dnsmasq and use odhcpd for both DHCP and DHCPv6 Again, the following commands come from:\nhttps://openwrt.org/docs/guide-user/base-system/dhcp_configuration#dhcp_and_dns_examples opkg update\nopkg remove dnsmasq odhcpd-ipv6only\nopkg install odhcpd\nuci -q delete dhcp.@dnsmasq[0]\nuci set dhcp.lan.dhcpv4=\u0026quot;server\u0026quot;\nuci set dhcp.odhcpd.maindhcp=\u0026quot;1\u0026quot;\nuci commit dhcp\nservice odhcpd restart\nUse Unbound for DNS Again, the following commands come from:\nhttps://openwrt.org/docs/guide-user/base-system/dhcp_configuration#dhcp_and_dns_examples opkg update\nopkg install unbound-control unbound-daemon\nuci set unbound.@unbound[0].add_local_fqdn=\u0026quot;3\u0026quot;\nuci set unbound.@unbound[0].add_wan_fqdn=\u0026quot;1\u0026quot;\nuci set unbound.@unbound[0].dhcp_link=\u0026quot;odhcpd\u0026quot;\nuci set unbound.@unbound[0].dhcp4_slaac6=\u0026quot;1\u0026quot;\nuci set unbound.@unbound[0].unbound_control=\u0026quot;1\u0026quot;\nuci commit unbound\nservice unbound restart\nuci set dhcp.odhcpd.leasefile=\u0026quot;/var/lib/odhcpd/dhcp.leases\u0026quot;\nuci set dhcp.odhcpd.leasetrigger=\u0026quot;/usr/lib/unbound/odhcpd.sh\u0026quot;\nuci commit dhcp\nservice odhcpd restart\nEnable DNS encryption Encrypt your DNS traffic improving security and privacy.\nPrevent DNS leaks and DNS hijacking.\nBypass regional restrictions using public DNS providers.\nEscape DNS-based content filters and internet censorship.\nhttps://openwrt.org/docs/guide-user/services/dns/dot_unbound uci set unbound.fwd_google.enabled=\u0026quot;1\u0026quot;\nuci set unbound.fwd_google.fallback=\u0026quot;0\u0026quot;\nuci commit unbound\nservice unbound restart\nUsing LuCI While OpenWrt can be managed completely using SSH and the terminal, the LuCI WebUI makes many administration tasks easier\nhttps://openwrt.org/docs/guide-user/luci/luci.essentials Unbound LuCI interface To manage the settings using the LuCI web interface, install the necessary packages:\nRun the following in a Terminal to update:\nopkg update\nThen, install the packages:\nopkg install luci-app-unbound\nservice rpcd restart\nservice odhcpd restart\nCongratulations! Yes, it’s a long winded process!\nHopefully you made it this far and now have OpenWrt with AdGuardHome and Unbound running on a SheevaPlug.\n","permalink":"https://boffinsblog.github.io/posts/openwrt-with-adguardhome-and-unbound-on-a-sheevaplug/","summary":"Take some OpenWrt with a dash of AdGuardHome, add a splash of Unbound and install it all on a SheevaPlug; will it be a recipe for disaster?","title":"OpenWrt with AdGuardHome and Unbound on a SheevaPlug"},{"content":"Better than it was before Better, Stronger, Faster\nMore Fediverse adventures, this time with installing Iceshrimp.NET\nWhy? Iceshrimp JS (as I’ll call the current version) is a hard fork of FireFish which is in itself was a rebranding of Calckey which was a fork of Misskey.\nIceshrimp.NET is a complete rewrite of the code-base.\nAll of it.\nBoth front and back end.\nAlthough this doesn’t answer why I want to migrate.\nThat answer to that is simple, I like tinkering.\nBefore I begin Firstly, I have to say a big ‘thank you’ to the Iceshrimp Dev team.\nAlso, the pace of development is ridiculously rapid considering the small amount of people involved, and is testament to their skills.\nAdd to that, they are open and approachable and always willing to help an idiot like me when I bungle something.\nKudos!\nBefore YOU begin If you are thinking of installing Iceshrimp.NET, please read the documentation .\nPreparations https://kb.iceshrimp.dev/s/docs/doc/production-readiness-Bs0HK2ZXO8 Obligatory Warning I\nNote This is alpha (soon to be beta!) software. While the Mastodon client API is relatively complete, the frontend is not. It is therefore not advisable to upgrade existing Iceshrimp instances at present. Setting up staging instances to help us with testing is very appreciated, however, and will help us ensure eventual upgrades go as smoothly as possible.\nhttps://iceshrimp.dev/iceshrimp/Iceshrimp.NET Obligatory Warning II\nThe following steps were completed on a Raspberry Pi 4 4GB running Ubuntu Server 22.04.4 LTS\nObviously, the usual caveats apply:\nDon’t simply copy and paste lines of code that you find on the Internet\nAlso, I am not responsible if any of this breaks your machine\nAs I was coming from a previously installed version of Iceshrimp a few of the pieces of this jigsaw were already in place.\nUpdate everything The following may be a bit of overkill but I run this command quite frequently:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade \u0026amp;\u0026amp; sudo apt remove \u0026amp;\u0026amp; sudo apt clean \u0026amp;\u0026amp; sudo apt autoremove \u0026amp;\u0026amp; sudo apt autoclean\nOr if you prefer a shorter version, run the following in a Terminal:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade\nInstall PostgreSQL PostgreSQL was already in place and I covered how to install in my previously published Installing Iceshrimp post.\nFull installation instructions are also on the PostgreSQL website .\ntl:dr\napt install postgresql\nStep by Step Create a ‘foo’ user I prefer to run a dedicated processes with a dedicated user.\nFirst, to add a new user, run the following in a Terminal:\nsudo adduser foo\nThen password protect it:\nsudo passwd foo\nInstall the .net SDK https://dotnet.microsoft.com/en-us/download/dotnet/8.0 Whilst as the foo user and in the $HOME directory, run the following in a Terminal:\nsu foo\ncd $HOME\nFirstly, download the installation script:\nwget https://dot.net/v1/dotnet-install.sh -O dotnet-install.sh\nThen change the file permissions to enable execution:\nchmod +x ./dotnet-install.sh\nFinally install:\n./dotnet-install.sh --version latest\nAdd the .net variables https://learn.microsoft.com/en-us/dotnet/core/install/linux-scripted-manual#set-environment-variables-system-wide The variables are volatile i.e. they do not persist between Terminal sessions.\nFor using in the current Terminal session only:\nexport DOTNET_ROOT=$HOME/.dotnet\nexport PATH=$PATH:$DOTNET_ROOT:$DOTNET_ROOT/tools\nOr, to make the variables persistent and usable between Terminal sessions, execute the following in a Terminal:\nnano $HOME/.bashrc\nThen, add at the bottom:\n# Add the dotnet variables export DOTNET_ROOT=$HOME/.dotnet export PATH=$PATH:$DOTNET_ROOT:$DOTNET_ROOT/tools After doing either of the above, check:\ndotnet --list-sdks\nThe output should be something like:\n8.0.204 [/home/foo/.dotnet/sdk]\nInstall the WebAssembly tools This step is not entirely necessary but is needed if the build is to include the wasm-tools.\nWebAssembly is a type of code that can be run in modern web browsers — it is a low-level assembly-like language with a compact binary format that runs with near-native performance and provides languages such as C/C++, C# and Rust with a compilation target so that they can run on the web.\nhttps://developer.mozilla.org/en-US/docs/WebAssembly I installed with:\ndotnet workload install wasm-tools\nClone Iceshrimp.NET from git https://kb.iceshrimp.dev/s/docs/doc/installation-guide-cjqv6iTgnS Whilst as the foo user and in the $HOME directory, run the following in a Terminal:\ngit clone https://iceshrimp.dev/iceshrimp/iceshrimp.net\ncd iceshrimp.net/Iceshrimp.Backend\nBuild the project in release configuration dotnet build -c Release\nNOTE: To build with NativeAOT enabled, the wasm-tools workload needs to have been installed:\ndotnet build -c Release -p:EnableAOT=true\nCreate an Iceshrimp.Storage directory Iceshrimp.NET needs somewhere to store media and the installation will fail if a path is not put into the configuration.ini file.\nAny storage directory can be created as long as it is writable by the foo user.\nI chose the following:\nmkdir /$HOME/iceshrimp.net/Iceshrimp.Storage/\nObject Storage is also an option however, I don’t use any third party provider for that:\nhttps://kb.iceshrimp.dev/s/docs/doc/object-storage-k2nxHYOocd Configure Very little needs to be configured (the database name and domain can be set here even if they don’t exist yet).\nnano $HOME/iceshrimp.net/Iceshrimp.Backend/configuration.ini\nChange:\nWebDomain = shrimp.example.org AccountDomain = example.org Database = iceshrimp Username = iceshrimp Password = iceshrimp This is the path to the Iceshrimp.Storage that was created earlier.\nPath = /path/to/media/location Set up certbot To only get a certificate before editing the nginx conf file:\nsudo certbot certonly\nThen put in the domain name e.g. shrimp.example.org\nThis will create a certificate and the local files to be used in the iceshrimp.nginx.conf file.\nMake a note of their path, it will be needed in the next step.\nSet up nginx https://kb.iceshrimp.dev/s/docs/doc/post-install-guide-caco15miOE I recycled an iceshrimp.nginx.conf from a previous installation.\nIt goes in /etc/nginx/sites-available/\nThe iceshrimp.nginx.conf changes are quite straightforward.\nChange:\nserver_name example.com;\nTo:\nserver_name shrimp.example.org;\n! In two (2) places !\nChange the path of the:\nssl_certificate\nssl_certificate_key\nTo the path that certbot provided.\nCreate the symlink:\nsudo ln -s /etc/nginx/sites-available/iceshrimp.nginx.conf /etc/nginx/sites-enabled/\nCheck the configuration and reload the nginx server:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nSplit Domain setup https://kb.iceshrimp.dev/doc/split-domain-setup-jHgCAvRar2 https://webfinger.net The following snippet GOES IN THE ROOT DOMAIN.\n(It took me far to long to realise this and as a result webfinger never worked properly for me.)\n# Webfinger location /.well-known/webfinger { rewrite ^.*$ https://shrimp.example.org/.well-known/webfinger permanent; } location /.well-known/host-meta { rewrite ^.*$ https://shrimp.example.org/.well-known/host-meta permanent; } location /.well-known/nodeinfo { rewrite ^.*$ https://shrimp.example.org/.well-known/nodeinfo permanent; } Check the configuration and reload the nginx server:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl restart nginx\nCreate a new PostgreSQL user (if needed) Irrespective of whether to migrate an existing database or create a fresh one, a PostgreSQL User will need to be exist (if it doesn’t already).\nLog into PostgreSQL:\nsudo -u postgres psql\nCREATE a new User:\nCREATE USER \u0026quot;iceshrimp\u0026quot; WITH ENCRYPTED PASSWORD 'super-long-password';\nTo DROP a User:\nDROP USER \u0026quot;iceshrimp\u0026quot;;\nDatabase choices I have both migrated an existing iceshrimp database and started with a clean slate.\nMigration is not recommended yet unless you know what you are doing; have read the disclaimers .\nhttps://kb.iceshrimp.dev/s/docs/doc/migration-guide-XRdCqvbu31 I’ll cover how to do both.\nTo create a new iceshrimp database Log into PostgreSQL:\nsudo -u postgres psql\nTo LIST the current databases:\n\\l\nThen, to CREATE an empty database:\nCREATE DATABASE \u0026quot;iceshrimp\u0026quot; WITH ENCODING = 'UTF8';\nNow take ownership:\nALTER DATABASE \u0026quot;iceshrimp\u0026quot; OWNER TO \u0026quot;iceshrimp\u0026quot;;\nTo DROP a database:\nDROP DATABASE \u0026quot;iceshrimp_net\u0026quot;;\nTo migrate an existing iceshrimp database Stop any running iceshrimp.service that is using the database to be duplicated:\nsudo systemctl stop iceshrimp.service\nsudo systemctl status iceshrimp.service\nLog into PostgreSQL:\nsudo -u postgres psql\nTo LIST the current databases:\n\\l\nThis next bit is extremely important!\nChange to the iceshrimp user:\nSET role iceshrimp;\nCheck you are the correct User:\nSELECT current_user, session_user;\nApply the migrations from:\nhttps://kb.iceshrimp.dev/s/docs/doc/migration-guide-XRdCqvbu31 I’m not going to replicate the script here as it may change.\nLast step! Finally, whether creating a new database or migrating an existing one, the last step is to VACUUM and ANALYZE the migrated database.\nAs the postgres user, run a VACUUM FULL ANALYZE;\nSET ROLE postgres;\nCheck you are the correct User:\nSELECT current_user, session_user;\nVACUUM FULL ANALYZE;\nTo quit:\n\\q\nRunning Iceshrimp.NET using systemd I prefer to use systemd to launch a service rather than starting one manually.\nThis approach has the added benefit of enabling a restart after a reboot if desired.\nsudo nano /etc/systemd/system/iceshrimp.net.service\n[Unit] Description=Iceshrimp.net daemon [Service] Type=simple User=foo WorkingDirectory=/home/foo/iceshrimp.net/Iceshrimp.Backend ExecStart=/home/foo/.dotnet/dotnet run --migrate-and-start [Install] WantedBy=multi-user.target A copy is also here .\nReload the systemd service:\nsudo systemctl daemon-reload\nAnd to make the systemd service restart after a reboot, enable it:\nsudo systemctl enable iceshrimp.service\nStart the systemd service sudo systemctl start iceshrimp.service\nsudo systemctl status iceshrimp.service\nCongratulations! Iceshrimp.NET should now be up and running.\nIf all has gone to plan you should see the above when going to the homepage.\nIceshrimp.NET Login Screenshot\nWhen using a new database, it will be empty and a new User will need creating.\nCreate a new Iceshrimp.NET user Firstly, turn Registrations ON:\nnano iceshrimp.net/Iceshrimp.Backend/configuration.ini\n;; Whether to allow instance registrations ;; Options: [Closed, Invite, Open] Registrations = Open Then send a PUT request e.g.:\ncurl -X PUT -H \u0026quot;Content-Type: application/json\u0026quot; http://localhost:3000/api/iceshrimp/auth -d' { \u0026quot;username\u0026quot; : \u0026quot;MyNewUsername\u0026quot;, \u0026quot;password\u0026quot; : \u0026quot;super-long-password\u0026quot; }'\nFinally, turn Registrations OFF:\nnano iceshrimp.net/Iceshrimp.Backend/configuration.ini\n;; Whether to allow instance registrations ;; Options: [Closed, Invite, Open] Registrations = Closed Updating Iceshrimp.NET Stop the iceshrimp.net.service\nsudo systemctl stop iceshrimp.net.service\nChange to the foo user and go to the iceshrimp.net directory:\nsu foo\ncd $HOME/iceshrimp.net\nThen:\ngit pull\nPGTune https://kb.iceshrimp.dev/s/docs/doc/database-maintenance-pibqfnr2he PGTune provides recommendations for PostgreSQL configuration based on your hardware specs and intended database workload.\nThere are a couple of Python based scripts on the Internet but I prefer to edit config files manually.\nPut in a couple of details about your system in the web form and generate the amendments to be made to /etc/postgresql/16/main/postgresql.conf\nhttps://pgtune.leopard.in.ua I highly recommended doing this.\nServer Load Just a couple of graphs showing the difference in server load between IceshrimpJS and Iceshrimp.NET\nIt’s generally lighter and runs faster.\nFirstly, the server load whilst running IceshrimpJS:\nIceshrimpJS Server Load Screenshot\nSecondly, the server load whilst running Iceshrimp.NET:\nIceshrimp.NET Server Load Screenshot\nUpdating the Blocklist There was a previous post about how to update the Blocklist when running Iceshrimp.\nThese scripts have also been updated / added to.\nClients Given that Iceshrimp.NET currently does not have a front-end, the only way to access an account is by using a third party application.\nWeb Elk Phanpy Mobile Tusky Finally (I did mention this in a prior post but it bears repeating.)\nFor anyone interested in getting into the SQL side, pgAdmin is a useful tool.\nI installed it using:\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\nsudo sh -c 'echo \u0026quot;deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/jammy pgadmin4 main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgadmin4.list \u0026amp;\u0026amp; apt update'\nsudo apt update\nsudo apt install pgadmin4\npgAdmin Screenshot\nHowever you Fediverse, Enjoy!\n","permalink":"https://boffinsblog.github.io/posts/installing-iceshrimp-net/","summary":"How I moved from IceshrimpJS to Iceshrimp.NET, a guide to migrating the database and installing.","title":"Installing Iceshrimp.NET"},{"content":"Scrape my site, get a free surprise Fuck techbros\nWhy would you want to fuck techbros? So you’ve published your latest pithy take on something to the Fediverse.\nAnd a supremely arrogant and entitled white guy comes along (it’s always a white guy) and scrapes your aphorisms for their latest widget.\nIt’s also probably some AI bollocks.\nThe something doesn’t matter, suffice to say it won’t work without your output being its input.\nHow to fuck techbros with tech Yes, stuff that you post on the Internet is generally viewable (and therefore scrapeable) by all.\nYes, any engineer entitled white guy with beautiful soup and pandas can scrape a Fedi timeline but that is not the only way that your info can be grabbed.\nAlso, closing some of the ways that data is harvested boils down to a personal choice:\nYou can be nice.\nOr you can choose violence.\nAct accordingly!\nBeing nice Any feed can become unwanted RSS feeds Every Fedi feed is accessible through an RSS link.\nSimply append .rss to a domain@username combination.\nSurprisingly, this is a feature, not a bug.\nSo, those people who set the “Require follow requests” option in preferences, thinking that their accounts are unviewable by non followers, sorry, you were misled.\nThe fix: Send the RSS requests back to the instance homepage # Send any .rss request to the homepage location ~* \\.rss$ { return 301 $scheme://social.example.net; } The /api/v1/instance leaks broadcasts personal info This one really annoys me.\nIf you append /api/v1/instance to the end of a Mastodon URL, you get the instance info.\nFine.\nBut part of the info returned is an Admin account name, presented as the ‘contact_account’.\n(Which is usually the first account created when setting up the software.)\nThe Mastodon helpers from /packages/backend/src/server/api/mastodon/helpers/misc.ts code seems to me to be saying; “find the first Admin account that is not deleted or suspended and present that User as a contact”.\nThis is Internet Security 101.\nHave separate Users and Admins.\nDon’t broadcast any aspect of the Admin account login details.\nexport class MiscHelpers { public static async getInstance(ctx: MastoContext): Promise\u0026lt;MastodonEntity.Instance\u0026gt; { const userCount = Users.count({ where: { host: IsNull() } }); const noteCount = Notes.count({ where: { userHost: IsNull() } }); const instanceCount = Instances.count({ cache: 3600000 }); const contact = await Users.findOne({ where: { host: IsNull(), isAdmin: true, isDeleted: false, isSuspended: false, }, order: { id: \u0026#34;ASC\u0026#34; }, }) The fix: Reroute the traffic In the same fashion that we reroute the RSS request, we send the traffic back to the instance homepage.\n# Send the instance API URL to the homepage location = /api/v1/instance { return 301 $scheme://social.example.com; } The /api/v1/peers leaks broadcasts instance info The fix: Reroute the traffic once more In the same fashion that we re-route the RSS request, we send the traffic back to the instance homepage.\n# Send the peers API URL to the homepage location = /api/v1/peers { return 301 $scheme://social.example.com; } Choosing violence Make it someone else’s problem So far the ‘fixes’ have been non adversarial.\nYour server routes requests from URLs to another, different URL, also on your server.\nHowever, nothing is preventing the re-routing of traffic to a third party, say, an intelligence agency with letters in its name.\nHere’s the /api/v1/peers example again but this time re-routing the traffic to an external web address.\n# Send the peers API URL to an external Address location = /api/v1/peers { return 301 ThreeLetterAgency.Address; } Make it their problem But what if you wanted to disrupt the incoming request?\nMaybe respond somehow?\nnull bombs Rather than rerouting the traffic to a URL, why not send a null bomb?\nI found the following somewhere on the Internets at some point in the distant past.\nI have NOT tried it out.\nFirst, make a bomb Create a 42.gz file.\nThis is a file that:\ncreates an archive that requires an excessive amount of time, disk space, or memory to unpack. https://en.wikipedia.org/wiki/Zip_bomb Execute the following in a Terminal:\ndd if=/dev/zero bs=1M count=102400 | gzip -c - \u0026gt; 42.gz\nThen, deploy the bomb 42.gz in the example below is in the root folder of the web server (which we can change).\nThis nginx.conf code example would send the 42.gz file when a request is made to /wp-login.php location.\n# Any location you want to deny location = /wp-login.php { # Make the bot think it\u0026#39;s just transport compression add_header Content-Encoding gzip; try_files /42.gz =404; # tell nginx to not blow it\u0026#39;s own foot off if the client doesn\u0026#39;t support gzip gunzip off; # lie to the client that the nullbomb is HTML so it doesn\u0026#39;t immediately reject it types { text/html gz; } } tarpits Tarpits are a tried and tested way to devour incoming SSH requests.\nEndlessh is an SSH tarpit that very slowly sends an endless, random SSH banner.\nIt keeps SSH clients locked up for hours or even days at a time.\nThe purpose is to put your real SSH server on another port and then let the script kiddies get stuck in this tarpit instead of bothering a real server.\nhttps://github.com/skeeto/endlessh https://www.digitalocean.com/community/tutorials/how-to-set-up-an-endlessh-tarpit-on-ubuntu-22-04 Wrap it all up and present it with a bow What if you could:\nIdentify the scrapers\nReroute the traffic\nMake it someone else’s problem\nDeploy a null bomb\nAll at once ?\nWell, Hetzner have a ‘speed test’ file you can use for such a project.\nhttps://fsn1-speed.hetzner.com (Other files are also available.)\nMaybe add something like the below to your nginx.conf file?\n# Send bots a present if ($http_user_agent ~* \u0026#34;AdsBot-Google|Amazonbot\u0026#34;) { return 307 https://fsn1-speed.hetzner.com/10GB.bin; } Resources I’ll also add to this list as time goes by and more useful resources appear.\nHow to block ChatGPT https://sizeof.cat/post/block-chatgpt-scraping Keep a robots.txt file up to date automatically https://darkvisitors.com ","permalink":"https://boffinsblog.github.io/posts/fuck-techbros/","summary":"Redirect their web scraping, either to a friendly URL or a not-so-friendly one. Now with a how-to on null bombs and tarpits.","title":"Fuck Techbros"},{"content":"To log or not to log, that is the question I was fed up of asking “is the network down”, so I resorted to logging the Broadband outages.\nA while ago I had a terrible Broadband provider.\nFrequent outages.\nRouters that crashed for no reason.\nTerrible customer service that couldn’t comprehend that I didn’t own a Windows computer.\nA reluctant Engineer who wondered why he had been tasked with visiting my house.\nBut the bottom line was; I never really knew whether the network was up or down.\nSo I decided to start logging when the network failed, at least that way I’d have some actual ammunition data when I got through to the terrible customer service.\nLogging the Broadband Outages Scripts Essentially, a cron job runs every five minutes.\nThe cron job does three things:\nFirstly, it runs a Python script that pings a URL and appends a ‘Network is up / Network is down‘ response to an ever growing .csv file.\nSecondly the cron job looks through that .csv file for the latest time and date that the network was down.\nLastly, it takes this piece of information and writes it to a new file.\nA web page can then display the contents of this new file.\nnetwork_status_python.py The Python script pings a known good URL and writes a response to the network_status_data.csv file.\n#!/usr/bin/env python import os import csv from datetime import datetime # Local path and filenames csv_path = \u0026#34;/home/foo/path/to/network_status/\u0026#34; csv_filename = \u0026#34;network_status_data.csv\u0026#34; log_path = \u0026#34;/home/foo/Logs/\u0026#34; log_filename = \u0026#34;network_status_updated\u0026#34; # Remove the log file if it exists if os.path.exists(log_path+log_filename): os.remove(log_path+log_filename) else: print(\u0026#34;The file does not exist\u0026#34;) # ping Google and note the response hostname = \u0026#34;google.com\u0026#34; response = os.system(\u0026#34;ping -c 1 \u0026#34; + hostname) # Generate what to write to the .csv file dt = datetime.now() full_date = dt.strftime(\u0026#39;%Y-%m-%d\u0026#39;) full_time = dt.strftime(\u0026#39;%H:%M:%S\u0026#39;) # Write to the .csv file if response == 0: csvWrite = full_date, full_time, \u0026#39;Network is up\u0026#39; csvFile = open(csv_path+csv_filename, \u0026#39;a\u0026#39;) with csvFile: writer = csv.writer(csvFile) writer.writerow(csvWrite) else: csvWrite = full_date, full_time, \u0026#39;Network is down\u0026#39; csvFile = open(csv_path+csv_filename, \u0026#39;a\u0026#39;) with csvFile: writer = csv.writer(csvFile) writer.writerow(csvWrite) # Create a log file to show the latest status was captured f = open(log_path+log_filename, \u0026#34;x\u0026#34;) Create the script with:\nnano ~/home/foo/path/to/network_status_python.py\nMake the script executable with:\nchmod +x ~/home/foo/path/to/network_status_python.py\nnetwork_status_data.csv This file contains the logged responses, growing in size over time.\nfull_date\tfull_time\tnetwork_status 2022-08-07\t03:45:01\tNetwork is up 2022-08-07\t03:50:01\tNetwork is up 2022-08-07\t03:55:01\tNetwork is up 2022-08-07\t04:00:12\tNetwork is down 2022-08-07\t04:05:11\tNetwork is down 2022-08-07\t04:10:11\tNetwork is down 2022-08-07\t04:15:01\tNetwork is up 2022-08-07\t04:20:01\tNetwork is up 2022-08-07\t04:25:02\tNetwork is up Before Python can write to the .csv file it needs to be created:\nnano ~/home/foo/path/to/network_status_data.csv\ncrontab.txt The cron example that:\nexecutes the Python script\ngreps when the network was last down\nwrites that info to the network_down.csv file\n# # This runs the python script to log the network status, then creates the last network outage file */5 * * * * python3 /home/foo/path/to/network_status_python.py \u0026amp;\u0026amp; grep \u0026#39;Network is down\u0026#39; /home/foo/path/to/network_status_data.csv | tail -1 \u0026gt; /var/www/example.com/network_down.csv # network_down.csv The timestamp of when the network was last down.\nI have placed this file in /var/www/example.com, a location that is accessible to a web page.\n2022-08-07\t04:10:11\tNetwork is down Before cron can write to the .csv file it needs to be created:\nsudo nano /var/www/example.com/network_down.csv\nOwn the file with:\nsudo chown $USER:$USER /var/www/example.com/network_down.csv\nphp_snippet.php An example of how to display the network_down.csv info on a web page.\nI used PHP.\nYMMV\n\u0026lt;?php $fcsv = fopen( \u0026#39;network_down.csv\u0026#39;, \u0026#39;r\u0026#39; ); $row = fgetcsv( $fcsv ); ?\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th colspan=\u0026#34;3\u0026#34;\u0026gt;Last Outage\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Date\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Time\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Status\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;?= $row[0] ?\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;?= $row[1] ?\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;?= $row[2] ?\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; ","permalink":"https://boffinsblog.github.io/posts/logging-the-broadband-outages/","summary":"Regularly pinging a known good URL and logging the response to a .csv file to record network outages.","title":"Logging the Broadband Outages"},{"content":"“Its my instance and I’ll Block if I want to, Block if I want to…” Right now, updating the Iceshrimp Blocklist is a manual task; these scripts automate that process.\nNote:\nI am well aware of the controversy and debate around Blocklist(s) but the fact remains that they exist and people use them. This is a post about how to automate the currently manual process.\nLike many Instance owners and Administrators, I employ Blocklists.\nThese Blocklists need regular updating, mainly because a Domain may be on a Blocklist one week and removed from that Blocklist later on.\nWhen using Iceshrimp, this process is a manual one:\nFind a suitable blocklist\nDownload the list (usually a .csv file)\nExtract the column of Domains\nCopy that column to the clipboard\nLog onto Iceshrimp as an Administrator\nGo to ‘Control Panel | Federation Management’\nDelete any old Domain names in the text box\nPaste in the clipboard contents\nClearly this is less than ideal.\nUpdating the Iceshrimp Blocklist Python Scripts This first script replaces that manual task by; collecting a single online list, doing the data extraction, and then inserting the Domain info into the PostgreSQL database using Python.\nupdate_blocklist_single.py This requires the following:\nThe remote URL link to a .csv Blocklist file\nA local path setting\nThe local filename for the .csv file (this can be the same as the remote name)\nFinally, a local filename for the log file\n#!/usr/bin/env python # -*- coding: utf-8 -*- # The url library import urllib.request # The pandas library (pip install pandas) import pandas as pd # The PostgreSQL library (pip install psycopg2) import psycopg2 # Local path and filenames blocklist_path = \u0026#34;/home/foo/path/to/update_iceshrimp_blocklist/\u0026#34; blocklist_filename = \u0026#34;blocklist_single.csv\u0026#34; blocklist_log_path = \u0026#34;/home/foo/Logs/\u0026#34; blocklist_log_filename = \u0026#34;iceshrimp_blocklist_single_updated\u0026#34; # Link to the Blocklist to be used url = (\u0026#34;https://codeberg.org/oliphant/blocklists/raw/branch/main/blocklists/_unified_tier0_blocklist.csv\u0026#34;) # Go grab the Blocklist urllib.request.urlretrieve(url, blocklist_path+blocklist_filename) # Read the CSV file into a pandas DataFrame df = pd.read_csv(blocklist_path+blocklist_filename) # Select the first column (column 0) and convert it to a text Series blocklist_series = df.iloc[:, 0].astype(str) # Join the text Series into a single string with a comma blocklist_string = \u0026#39;,\u0026#39;.join(blocklist_series) # Establish a Postgres DB connection conn = psycopg2.connect(host=\u0026#34;127.0.0.1\u0026#34;, database=\u0026#34;iceshrimp_db\u0026#34;, user=\u0026#34;iceshrimp_user\u0026#34;, password=\u0026#34;iceshrimp_db_user_password\u0026#34;, port= \u0026#39;5432\u0026#39;) # Create a cursor object using the cursor() method cursor = conn.cursor() # Prepare the SQL query to UPDATE the \u0026#39;blockedHosts\u0026#39; field in the \u0026#39;meta\u0026#39; table in the database table_modification = \u0026#34;\u0026#34;\u0026#34;UPDATE meta SET \u0026#34;blockedHosts\u0026#34; = \u0026#39;{\u0026#34;\u0026#34;\u0026#34; + blocklist_string + \u0026#34;\u0026#34;\u0026#34;}\u0026#39; WHERE id = \u0026#39;x\u0026#39;\u0026#34;\u0026#34;\u0026#34; # Execute and error-trap try: # Execute the SQL command cursor.execute(table_modification) print(\u0026#34;Executed and UPDATED\u0026#34;) # Commit the changes to the database conn.commit() print(\u0026#34;UPDATE Committed\u0026#34;) # Create a log file to show that the UPDATE has successfully completed open(blocklist_log_path+blocklist_log_filename, \u0026#39;w\u0026#39;) print(\u0026#34;UPDATE log file created\u0026#34;) except: # Roll back in case of error conn.rollback() print(\u0026#34;UPDATE Failed and rolled back\u0026#34;) # Close the connection conn.close() print(\u0026#34;Connection CLOSED\u0026#34;) This second script replaces that manual task by; collecting multiple online lists, doing the data extraction, collating them, removing any duplicates and then inserting the Domain info into the PostgreSQL database using Python.\nupdate_blocklist_multiple.py Note:\nThis process will work just as well with one entry in the list as it will with multiple entries.\nThis requires the following:\nA ‘ blocklist_multiple.txt ‘ file that contains URLs to the remote .csv files\nAn optional .csv file containing URLS to be blocked; a ‘ personal_blocklist.csv ‘\nA local path setting\nA local filename for the log file\n#!/usr/bin/env python # -*- coding: utf-8 -*- # The url library import urllib.request # The pandas library (pip install pandas) import pandas as pd # The PostgreSQL library (pip install psycopg2) import psycopg2 # Local path and filenames blocklist_path = \u0026#34;/home/foo/path/to/update_iceshrimp_blocklist/\u0026#34; blocklist_filename = \u0026#34;blocklist_multiple.txt\u0026#34; blocklist_log_path = \u0026#34;/home/foo/Logs/\u0026#34; blocklist_log_filename = \u0026#34;iceshrimp_blocklist_multiple_updated\u0026#34; # Python pathname pattern matching import glob # Download all of the *.csv files in \u0026#39;blocklist.txt\u0026#39; into the current directory with open(blocklist_path+blocklist_filename) as f: for line in f: url = line blocklist_filename = url.split(\u0026#39;/\u0026#39;, -1)[-1] urllib.request.urlretrieve(url, blocklist_path+blocklist_filename.rstrip(\u0026#39;\\n\u0026#39;)) # Put all of the *.csv files in the current directory into a single DataFrame files = glob.glob(blocklist_path+\u0026#34;*.csv\u0026#34;) content = [] # Loop through all the *.csv files for filename in files: df = pd.read_csv(filename, index_col=None) print(len(df.index), \u0026#34;rows in\u0026#34;, filename) content.append(df) # Convert \u0026#39;content\u0026#39; to a DataFrame df = pd.concat(content) # Print the # of DataFrame rows print(len(df.index), \u0026#34;rows in the DataFrame\u0026#34;) # Select the first column (column 0) and convert it to a text Series blocklist_series = df.iloc[:, 0].astype(str) # Make the Series unique blocklist_series = pd.unique(blocklist_series) # Print the # of rows in the Series print(pd.Series(blocklist_series).count(), \u0026#34;unique rows in the Series\u0026#34;) # Join the text Series into a single string with a comma blocklist_string = \u0026#39;,\u0026#39;.join(blocklist_series) # Establish a Postgres DB connection conn = psycopg2.connect(host=\u0026#34;127.0.0.1\u0026#34;, database=\u0026#34;iceshrimp_db\u0026#34;, user=\u0026#34;iceshrimp_user\u0026#34;, password=\u0026#34;iceshrimp_db_user_password\u0026#34;, port= \u0026#39;5432\u0026#39;) # Create a cursor object using the cursor() method cursor = conn.cursor() # Prepare the SQL query to UPDATE the \u0026#39;blockedHosts\u0026#39; field in the \u0026#39;meta\u0026#39; table in the database table_modification = \u0026#34;\u0026#34;\u0026#34;UPDATE meta SET \u0026#34;blockedHosts\u0026#34; = \u0026#39;{\u0026#34;\u0026#34;\u0026#34; + blocklist_string + \u0026#34;\u0026#34;\u0026#34;}\u0026#39; WHERE id = \u0026#39;x\u0026#39;\u0026#34;\u0026#34;\u0026#34; # Execute and error-trap try: # Execute the SQL command cursor.execute(table_modification) print(\u0026#34;Executed and UPDATED\u0026#34;) # Commit the changes to the database conn.commit() print(\u0026#34;UPDATE Committed\u0026#34;) # Create a log file to show that the UPDATE has successfully completed open(blocklist_log_path+blocklist_log_filename, \u0026#39;w\u0026#39;) print(\u0026#34;UPDATE log file created\u0026#34;) except: # Roll back in case of error conn.rollback() print(\u0026#34;UPDATE Failed and rolled back\u0026#34;) # Close the connection conn.close() print(\u0026#34;Connection CLOSED\u0026#34;) blocklist_multiple.txt This is the list of URLs that link to the .csv Blocklist(s):\nhttps://codeberg.org/oliphant/blocklists/raw/branch/main/blocklists/_unified_tier0_blocklist.csv https://seirdy.one/pb/pleroma.envs.net.csv personal_blocklist.csv Additionally, a ‘personal_blocklist’ can be created (this file can be named what you like as long as it is a .csv file).\ndomain naughty_fedi.com Updating the Iceshrimp Blocklist crontab In addition to running the script manually, either script can be executed nightly, at midnight, from a normal crontab (crontab -e from a Terminal).\n# Update the iceshrimp blocklist daily at midnight #0 0 * * * python3 /home/foo/path/to/script/update_blocklist_single.py 0 0 * * * python3 /home/foo/path/to/script/update_blocklist_multiple.py # Finally (I did mention this in a prior post but it bears repeating.)\nFor anyone interested in getting into the SQL side, pgAdmin is a useful tool.\nI installed it using:\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\nsudo sh -c 'echo \u0026quot;deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/jammy pgadmin4 main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgadmin4.list \u0026amp;\u0026amp; apt update'\nsudo apt update\nsudo apt install pgadmin4\npgAdmin Screenshot\nHowever you Fediverse, Enjoy!\nUPDATE Iceshrimp is moving to a .NET framework.\nAdditional scripts for updating a blocklist when running Iceshrimp.NET can be found at .\n","permalink":"https://boffinsblog.github.io/posts/updating-the-iceshrimp-blocklist/","summary":"Right now updating the blocklists is a manual process, this is a way of updating the Iceshrimp blocklist automatically using Python and cron.","title":"Updating the Iceshrimp Blocklist"},{"content":"Don’t brick it; this process is quite straightforward Step by step guide to unbricking a SheevaPlug. You may think your ‘Plug is dead, but if you can plug it into a computer you can resurrect it.\nWhat on Earth is a SheevaPlug? The SheevaPlug is a “ plug computer ” designed to allow standard computing features in as small a space as possible.\nIt was a small embedded Linux ARM computer without a display which can be considered an early predecessor to the subsequent Raspberry Pi .\nhttps://en.wikipedia.org/wiki/SheevaPlug The SheevaPlug Dev Kit comes with a JTAG connection.\nThis connects the ‘Plug to a computer’s USB port and communicates via the serial console.\nThis is exactly the same way that an Arduino can send info to the outside world.\nUnbricking a SheevaPlug – Installing a TFTP server Rather that messing about with files on a USB, you can use a TFTP server (trust me, it’s a lot easier).\nTo install a TFTP server run the following in a Terminal:\nsudo apt install tftpd-hpa\nThen, check that it is running:\nsudo systemctl status tftpd-hpa\nThen, take ownership of the TFTP server directory:\nsudo chown -R $USER /srv/tftp\nAny files now placed in this directory will be available to pull onto the ‘Plug.\nUnbricking a SheevaPlug – Recovery to a good u-boot after erasing NAND The NAND contains three partitions:\nMTD0 – u-boot, think of it as the grub of embedded systems MTD1 – uImage, the kernel\nMTD2 – rootfs, the filesystem\nIf you were tinkering and completely erased the NAND then there will be no u-boot, which will not process the kernel, which will not boot into the filesystem.\nIn other words; bricked, so the first step is to get a working u-boot.\nFor this you’ll need openocd, the Open On-Chip Debugger.\nTo install openocd run the following in a Terminal:\nsudo apt-get install openocd\nInstall some needed tools:\nsudo apt-get install telnet screen\nFinally a valid u-boot.elf needs to be in the host home directory.\nI have an ancient version archived away but there may be a newer version on the Internet somewhere.\nOpen three (3) Terminals.\nI use Terminator which allows for window tiling and a whole host of things I have yet to discover.\nIn Terminal 1 run the following:\nscreen /dev/ttyUSB0 115200\nThen, in Terminal 2 run the following:\nsudo openocd -f /usr/share/openocd/scripts/board/sheevaplug.cfg -s /usr/share/openocd/scripts\nNow, in Terminal 3 run the following:\ntelnet localhost 4444\nThen:\nreset;sheevaplug_init;load_image u-boot.elf;resume 0x00600000\nBack in Terminal 1 something is happening!\nu-boot Das U-Boot (subtitled “the Universal Boot Loader” and often shortened to U-Boot; see History for more about the name) is an open-source boot loader used in embedded devices to perform various low-level hardware initialization tasks and boot the device’s operating system kernel.\nhttps://en.wikipedia.org/wiki/Das_U-Boot This process assumes that the new u-boot files are on the TFTP server.\nThe Bookworm version provided by Debian is here .\nPower on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nThen, type the following to set the TFTP environment variables:\nsetenv serverip \u0026lt;Host_IP\u0026gt; setenv ipaddr \u0026lt;SheevaPlug_IP\u0026gt; setenv ethaddr 00:50:43:01:63:EA setenv macaddr 00:50:43:01:63:EA saveenv saveenv stores the variables so they will not need setting again.\nThen:\ntftpboot 0x0800000 u-boot.kwb nand erase 0x0 0x80000 nand write 0x0800000 0x0 0x80000 reset This erases a the portion of NAND reserved for u-boot and writes the new version.\nThe ‘Plug will now reboot.\nInterrupt the boot process to get to the ‘Plug command prompt.\nInstalling Debian to USB from the TFTP server Now that the ‘Plug boots with an updated u-boot we can install Debian Bookworm.\nThe Bookworm installation files are on the Debian website .\nSet the ‘Plug to use the files on the TFTP server.\ntftpboot 0x00800000 uImage tftpboot 0x01100000 uInitrd setenv bootargs console=ttyS0,115200n8 base-installer/initramfs-tools/driver-policy=most bootm 0x00800000 0x01100000 The Debian installer should now install Bookworm to the USB.\nBoot to Debian from USB Instruct the ‘Plug to boot from USB.\nPower on the ‘Plug.\nIn a Terminal run the following:\nscreen /dev/ttyUSB0 115200\nInterrupt the boot process to get to the ‘Plug command prompt.\nsetenv bootargs_console console=ttyS0,115200 setenv bootcmd_usb \u0026#39;usb start; ext2load usb 0:1 0x00800000 /uImage; ext2load usb 0:1 0x01100000 /uInitrd\u0026#39; setenv bootcmd \u0026#39;setenv bootargs ${bootargs_console}; run bootcmd_usb; bootm 0x00800000 0x01100000\u0026#39; saveenv run bootcmd As before, saveenv stores the variables so they will not need setting again.\nIf in the future the ‘Plug needs to boot from NAND or MMC this instruction will need revisiting.\nTo reboot reset\nAnd that is how you go about unbricking a SheevaPlug.\nFinally Read more about my ‘ Cupboard of Forgotten Tech ‘ misadventures.\n","permalink":"https://boffinsblog.github.io/posts/how-to-unbrick-a-sheevaplug/","summary":"You may think your \u0026lsquo;Plug is dead, but if you can plug it into a computer you can resurrect it.","title":"How to Unbrick a SheevaPlug"},{"content":"Self hosting is not everyone’s cup of tea With GitHub repositories becoming the krill for Microsoft’s behemoth AI, and Codeberg still broken, it was time to get around to installing Gitea.\nGetting stuff ready Resources As usual, I scoured the Internets for several hours and read everything I could find.\nHowever this was kind of pointless as most of the Gitea installation instructions can be found in the official documentation .\nUpdate everything Firstly, update everything and get ready to install.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade\nThen, add a couple of necessary packages (these may already be installed):\nsudo apt install wget\nsudo apt install git\nnginx part I – pre Certbot I tend to run my nginx setup in three parts:\nFirst, a base setup to get things up and running and test that everything works\nThen, grab a HTTPS Certificate using Certbot\nThen, add the Certificate and restart nginx\nStart setting up the initial nginx conf file by running the following in a Terminal:\nsudo nano /etc/nginx/sites-available/GITEA\nWith the example copied from here :\nserver { listen 80; server_name git.example.com; location / { client_max_body_size 512M; proxy_pass http://localhost:3000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Then, ‘enable’ the site by creating a symlink:\nsudo ln -s /etc/nginx/sites-available/GITEA/etc/nginx/sites-enabled/\nCheck and reload nginx Then make sure it all works:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nCertbot I already have a HTTPS site running on this server (you are reading it) so there was no need for me to install extra Certbot software.\nI would like the Gitea connection to be over HTTPS too, so for me, adding an extra certificate was a matter of running:\nsudo certbot certonly --nginx -d git.example.com\n( There is a guide on how to install Certbot however. )\nnginx part II – post Certbot Now that the site is up and running and the Certificate issued, I can replace the earlier one with the following:\n(The additional lines come from the excellent Digital Ocean nginx config generator .)\n# Upstream server upstream gitea { server 192.168.0.90:3003; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name git.example.com; # set ; root /usr/local/bin/gitea; # SSL ssl_certificate /etc/letsencrypt/live/git.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/git.example.com/privkey.pem; ssl_trusted_certificate /etc/letsencrypt/live/git.example.com/chain.pem; # security include nginxconfig.io/security.conf; # restrict methods if ($request_method !~ ^(GET|POST|HEAD)$) { return \u0026#39;405\u0026#39;; } location / { try_files maintain.html $uri $uri/index.html @node; } location @node { client_max_body_size 0; proxy_pass http://localhost:3003; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Proto $scheme; proxy_max_temp_file_size 0; proxy_redirect off; proxy_read_timeout 120; } } # HTTP redirect server { listen 80; listen [::]:80; server_name git.example.com; include nginxconfig.io/letsencrypt.conf; location / { return 301 git.example.com$request_uri; } } Check and reload nginx once more Then make sure it all still works:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nSetup ufw Next, Gitea need to be able to be seen from the outside world.\nI use ‘ufw’ or ‘ Uncomplicated Firewall ‘ so I need to allow traffic to the Gitea port.\nFirst, see all of the existing connections:\nsudo ufw verbose\nThen, allow tcp traffic on the Gitea port:\nsudo ufw allow 3003/tcp comment \u0026quot;Gitea tcp traffic\u0026quot;\nThen, allow udp traffic on the Gitea port:\nsudo ufw allow 3003/udp comment \u0026quot;Gitea udp traffic\u0026quot;\nCreate a user to run Gitea I prefer to run dedicated processes with a dedicated user.\nTo add a new user run the following in a Terminal:\nadduser \\ --system \\ --shell /bin/bash \\ --gecos \u0026#39;Git Version Control\u0026#39; \\ --group \\ --disabled-password \\ --home /home/git \\ git Or:\nsudo adduser --system --shell /bin/bash --gecos 'Git Version Control' --group --disabled-password --home /home/git git\nCreate a Gitea database Like Certbot I already have MariaDB installed and running as it is the DB behind this WordPress site.\nIf needed, install by running the following in a Terminal:\nsudo apt install mariadb-server -y\nHowever if this is the first time MariaDB has been installed it is a good idea to run through an initial setup in a Terminal:\nsudo mysql_secure_installation\nTo check that it’s running and to start on a reboot, run the following in a Terminal:\nsudo systemctl start mariadb\nsudo systemctl enable mariadb\nsudo systemctl status mariadb\nTo add the Gitea database run the following in a Terminal:\nsudo mysql -u root -p\nCREATE DATABASE gitea;\nGRANT ALL PRIVILEGES ON gitea.* TO 'gitea'@'localhost' IDENTIFIED BY \u0026quot;super_long_password_goes_here\u0026quot;;\nFLUSH PRIVILEGES;\nQUIT;\nCreate the required directory structure Before downloading, Gitea needs a place to live.\nRun the following in a Terminal to create the directories:\nsudo mkdir -p /var/lib/gitea/{custom,data,log}\nThen, own them:\nsudo chown -R git:git /var/lib/gitea/\nThen, set the appropriate permissions:\nsudo chmod -R 750 /var/lib/gitea/\nNow create the /etc/ directory:\nsudo mkdir /etc/gitea\nThen, own it:\nsudo chown root:git /etc/gitea\nThen, set the appropriate permissions:\nsudo chmod 770 /etc/gitea\nNOTE: /etc/gitea is temporarily set with write permissions for user git so that the web installer can write the configuration file.\nAfter the installation is finished, it is recommended to set permissions to read-only using:\nchmod 750 /etc/gitea\nchmod 640 /etc/gitea/app.ini\nDownload and install Run the following in a Terminal:\nwget -O gitea https://dl.gitea.com/gitea/1.21/gitea-1.21-linux-arm-6\nchmod +x gitea\nCopy the Gitea binary to a global location Run the following in a Terminal:\nsudo cp gitea /usr/local/bin/gitea\nCreate a systemd unit file I want Gitea to start up after a reboot so I create a systemd unit file and enable it.\nThe following uses wget to grab the latest version from the Gitea repo and saves a copy to the /etc/systemd/system/ folder:\nsudo wget https://raw.githubusercontent.com/go-gitea/gitea/main/contrib/systemd/gitea.service -P /etc/systemd/system/\nEdit the downloaded file to start on a different port:\nsudo nano /etc/systemd/system/gitea.service\nBy changing:\nExecStart=/usr/local/bin/gitea web --config /etc/gitea/app.ini\nTo:\nExecStart=/usr/local/bin/gitea web -p 3003 --config /etc/gitea/app.ini\nCheck and reload Gitea First, enable, this will ensure that the service restarts after a reboot:\nsudo systemctl enable gitea.service\nThen, start the service:\nsudo systemctl start gitea.service\nThen, check the status of the service to see if it is running:\nsudo systemctl status gitea.service\nFinish installation In a browser finish setting up the installation.\ngit.example.com\nSite Title – Can Be Whatever You Like\nAdministrator Account Settings:\nThe first account created will automatically become an Administrator.\nPress the big button, ‘Install Gitea’.\nAdditional users can now be enrolled or added.\nNOTE: There might be a timeout on first start.\nRunning ‘sudo systemctl start gitea.service‘ again seems to resolve this.\nPost Install As mentioned previously it is recommended to set permissions to read-only using:\nsudo chmod 750 /etc/gitea\nsudo chmod 640 /etc/gitea/app.ini\nConnecting with SSH NOTE: These steps are done on the local machine that will connect to Gitea, not the server where Gitea is installed.\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent Key Management Create a new key ssh-keygen -t ed25519 -C \u0026quot;user@example.com\u0026quot;\nGenerating a key will give it a default name.\nI don’t like the default names even though they make sense, so I rename the keys to something I can remember.\nmv id_ed25519 gitea\nmv id_ed25519.pub gitea.pub\nEdit the config sudo nano ~/.ssh/config\nHost gitea User git HostName git.example.com IdentityFile ~/.ssh/gitea IdentitiesOnly=yes Restart SSH sudo systemctl restart ssh\nAdding the SSH key to the ssh-agent Start the ssh-agent in the background:\neval \u0026quot;$(ssh-agent -s)\u0026quot;\nAdd the SSH private key to the ssh-agent ssh-add ~/.ssh/gitea\nAdd the SSH public key to Gitea Copy the Public Key:\ncat ~/.ssh/gitea.pub\nCopy the Terminal output\nLog into Gitea\nSettings | SSH GPG Keys | Add Key\nKey Name – Gitea SSH Key\nPaste and ‘Add Key’\nConnect with SSH In a Terminal:\nssh gitea\nWill return the expected error message:\n‘Hi there, User! You’ve successfully authenticated with the key named Gitea SSH Key, but Gitea does not provide shell access.’\nSending to Gitea using SSH Create a repository in Gitea e.g. ‘Arduino’.\nTick ‘Initialize Repository’ and create.\nInitialise locally e.g.\ncd /foo/Git/Arduino/\ngit init\nEdit the config to use the SSH Key:\nsudo nano /foo/Git/Arduino/.git/config\n[remote \u0026#34;origin\u0026#34;] url = gitea:User/Arduino.git Send (force -f) an initial commit:\ngit add .\ngit commit -m \u0026quot;Initial Commit\u0026quot;\ngit push -f origin main\nSubsequent pushes using ‘git push -u origin main‘ will be fine.\nTinkering Turn off registrations As I noted earlier I am the only user here and only ever will be so there is no sense in having registrations open for other people to sign up to.\nSo I turn off registrations:\nsudo nano /etc/gitea/app.ini\nDISABLE_REGISTRATION = true Disable OpenID sign-ins Again, I’m the only user so I disable OpenID sign-ins:\n[openid] ENABLE_OPENID_SIGNIN = false ENABLE_OPENID_SIGNUP = false Set up Mailer (Mailjet) In case I bungle something sign-in related, I can use Mailjet to resend a password to a verified email address:\n[mailer] ENABLED = true SMTP_ADDR = in-v3.mailjet.com SMTP_PORT = 587 USER = mailjet_user_id PASSWD = mailjet_user_key FROM = gitea@example.com Set the ‘Landing Page’ As noted earlier, I’m the only user here and probably always will be.\nIt makes no sense to have an incoming browser land on the Gitea homepage, so rather than fiddle with nginx redirects Gitea allows the application to set a ‘landing page’.\nIn the [server] section, change:\nLANDING_PAGE = https://server.IP/user Check and reload Gitea once more Then make sure it all still works.\nsudo systemctl restart gitea.service\nThen, check the ‘status’.\nsudo systemctl status gitea.service\nSome scripts Gitea Repository Screenshot\nGitea will look familiar to anyone who has used a remote Git repository like GitHub or Codeberg in the past.\nHowever right now my installation is empty; it needs a few repositories migrating to it.\nThe following two (2) scripts live locally at /foo/Git/ and assume that:\nA local repository exists at /foo/Git/Arduino/\nA local repository exists at /foo/Git/Calckey/\nAlso that:\nA remote repository exists at git.example.com/User/Arduino\nA remote repository exists at git.example.com/User/Calckey\n(I keep the local repository names the same as the remote repository names to prevent brain ache.)\ngit_init_everything.sh The first script loops through all of the directory names in the array and re-initialises them.\nIt then deletes all of the local history, commits etc.\nIt then ‘pushes’ the local repository to the pre-existing remote repository.\n#!/bin/bash clear # Re-initialise and Push everything to Gitea... echo -e \u0026#39;\\e[1;33mRe-initialise and Push everything to Gitea...\\e[0m\u0026#39;; echo \u0026#39;\u0026#39;; function initGit { local -n arr=$1 for i in \u0026#34;${arr[@]}\u0026#34; do # Initialise the local git repo... echo -e \u0026#39;\\e[1;36mInitialise the local\u0026#39; $i \u0026#39;git repo...\\e[0m\u0026#39;; # Remove the old repo... echo -e \u0026#39;\\e[32mRemove the old\u0026#39; $i \u0026#39;repo...\\e[0m\u0026#39;; cd /foo/$i/; sudo rm -R .git; # Create the new local repo... echo -e \u0026#39;\\e[32mCreate the new\u0026#39; $i \u0026#39;local repo...\\e[0m\u0026#39;; git init; # Update the local config file... echo -e \u0026#39;\\e[32mUpdate the local\u0026#39; $i \u0026#39;config file...\\e[0m\u0026#39;; echo \u0026#39;[remote \u0026#34;origin\u0026#34;]\u0026#39; \u0026gt;\u0026gt; .git/config; echo \u0026#39; url = git_gitea:Boffin/\u0026#39;$i\u0026#39;.git\u0026#39; \u0026gt;\u0026gt; .git/config; # Push the local files to the remote repo with an initial commit... echo -e \u0026#39;\\e[32mPush the local\u0026#39; $i \u0026#39;files to the remote repo with an initial commit...\\e[0m\u0026#39;; git add .; git commit -m \u0026#34;Initial Commit\u0026#34;; git push -f origin main; # Done echo -e \u0026#39;\\e[1;36m\u0026#39;$i \u0026#39;done\\e[0m\u0026#39;; echo \u0026#39;\u0026#39;; done } # Define the array of all the directory names array=( \u0026#34;Arduino\u0026#34; \u0026#34;Calckey\u0026#34; ) # Loop initGit array # Everything done echo -e \u0026#39;\\e[1;33mEverything done\\e[0m\u0026#39;; git_update_everything.sh Then the second script loops through all of the directory names in the array and updates them.\nIt then ‘pushes’ the local repository to the pre-existing remote repository.\n#!/bin/bash clear # Push everything to Gitea... echo -e \u0026#39;\\e[1;33mPush everything to Gitea...\\e[0m\u0026#39;; echo \u0026#39;\u0026#39;; function updateGit { local -n arr=$1 for i in \u0026#34;${arr[@]}\u0026#34; do # Update the remote git repo... echo -e \u0026#39;\\e[1;36mUpdate the remote\u0026#39; $i \u0026#39;git repo...\\e[0m\u0026#39; cd /foo/$i/ git add . git commit -m \u0026#39;Modified\u0026#39; .; git push -u origin main; # Done echo -e \u0026#39;\\e[1;36m\u0026#39;$i \u0026#39;done\\e[0m\u0026#39; echo \u0026#39;\u0026#39; done } # Define the array of all the directory names array=( \u0026#34;Arduino\u0026#34; \u0026#34;Calckey\u0026#34; ) # Loop updateGit array # Everything done echo -e \u0026#39;\\e[1;33mEverything done\\e[0m\u0026#39;; As the traditional spaces people have used to share code and ideas disappear, or become the input for training models, I truly believe that self hosted applications like Gitea can empower us.\nSo, if you have a website living on a Pi under your telly, there’s no reason you can’t have a git repository living next to it too.\n","permalink":"https://boffinsblog.github.io/posts/installing-gitea/","summary":"Everything from downloading to securing your installation. Now with bash scripts to upload and update Gitea repositories.","title":"Installing Gitea"},{"content":"Shut your Pi-hole! There are a bunch of posts similar to this one but I couldn’t find one that brought all of these elements together, so, here is my brain dump on how to install Pi-hole with Unbound and PiVPN.\nPi-hole with Unbound and PiVPN – Update Everything Ensure that the system is up to date:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade\nFirst, install Pi-hole The Pi-hole[®] is a DNS sinkhole that protects your devices from unwanted content, without installing any client-side software.\nhttps://docs.pi-hole.net/ One-Step Automated Install Start the process by typing the following in a Terminal:\ncurl -sSL https://install.pi-hole.net | bash\nThen, accept all the defaults.\nEither write down the generated password or change the password later by running ‘pihole -a -p‘ in a Terminal.\nAdd Pi-hole rules to ufw Firewall Pi-hole needs to both listen to and sometimes talk to the outside world.\nI use ‘ufw’ or ‘ Uncomplicated Firewall ‘ so I need to allow traffic to the Pi-hole ports:\nsudo ufw allow 53/tcp comment \u0026quot;Pi-hole Port 53 tcp traffic\u0026quot;\nsudo ufw allow 53/udp comment \u0026quot;Pi-hole Port 53 udp traffic\u0026quot;\nsudo ufw allow 67/tcp comment \u0026quot;Pi-hole Port 67 tcp traffic\u0026quot;\nsudo ufw allow 67/udp comment \u0026quot;Pi-hole Port 67 udp traffic\u0026quot;\nAlso add IPv6 Rules (include above IPv4 rules) sudo ufw allow 546:547/udp comment \u0026quot;Pi-hole DHCP traffic\u0026quot;\nTo delete old rules sudo ufw status verbose\nsudo ufw status numbered\nsudo ufw delete number_of_rule\nDHCP on the router can now be turned off.\nPi-hole Blocklists Generally the first step is to think about what you want to block on your network.\nThen, implement lists that satisfy the above.\nThis is a good overview of blocklists:\nhttps://www.avoidthehack.com/best-pihole-blocklists My Blocklists I subscribe to the ‘less is more’ ethos and have a few high quality curated lists rather than blocking millions of URLs.\nhttps://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-gambling-porn/hosts https://big.oisd.nl/ https://raw.githubusercontent.com/EnergizedProtection/EnergizedBlu/master/energized/blu.txt https://adguardteam.github.io/AdGuardSDNSFilter/Filters/filter.txt https://phishing.army/download/phishing_army_blocklist_extended.txt Use nginx over lighttpd I already have a web server up and running (you are reading this because of it) so I don’t really need lighttpd running as well.\nsudo systemctl disable lighttpd\nsudo apt-get purge --auto-remove lighttpd\nsudo systemctl restart nginx\nsudo rm /var/www/html``/index.lighttpd.html\nOr, use both and edit lighttpd.conf and change the listening port:\nsudo nano /etc/lighttpd/lighttpd.conf\nFrom:\nserver.port = 80 To:\nserver.port = 8080 flush_pihole.sh I like bash scripts, so I obviously wrote one to:\nStop the pihole service\nFlush stale \u0026rsquo;neighbouring\u0026rsquo; leases\nFlush current leases\nRemove old lists\nRemove the query database and log (long term storage)\nRemove the dnsmasq log (short term storage)\nStart the pihole service\nUpdate Gravity\nRestart DNS\nIs pihole running\nPi-hole probably has a way to do all of this somewhere but I just like writing scripts.\nFirstly, create the script:\nsudo nano /etc/init.d/flush_pihole.sh\n#!/bin/bash clear # Stop the pihole service echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mStop the pihole service...\\e[0m\u0026#34; service pihole-FTL stop echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Flush stale \u0026#39;neighbouring\u0026#39; leases echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mFlush stale \u0026#39;neighbouring\u0026#39; leases...\\e[0m\u0026#34; ip neigh flush nud stale echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Flush current leases echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mFlush current leases...\\e[0m\u0026#34; rm /etc/pihole/dhcp.leases echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Remove old lists echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRemove old lists...\\e[0m\u0026#34; rm /etc/pihole/list.* echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Remove the query database and log (long term storage) echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRemove the query database and log (long term storage)...\\e[0m\u0026#34; rm /etc/pihole/pihole-FTL.db rm /etc/pihole/pihole-FTL.log echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Remove the dnsmasq log (short term storage) echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRemove the dnsmasq log (short term storage)...\\e[0m\u0026#34; rm /var/log/pihole.log echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Start the pihole service echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mStart the pihole service...\\e[0m\u0026#34; service pihole-FTL start echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Update Gravity echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mUpdate Gravity...\\e[0m\u0026#34; pihole -g echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Restart DNS echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRestart DNS...\\e[0m\u0026#34; pihole restartdns echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Is pihole running echo -e \u0026#34;\\e[1;33mIs pihole running...\\e[0m\u0026#34; systemctl status --no-pager pihole-FTL echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; Then, give the file the correct permissions:\nsudo chmod +x /etc/init.d/flush_pihole.sh\nFinally, add it to root cron and set it to run daily at midnight:\nsudo crontab -e\n# # Flush the Pi-hole logs \u0026amp; db also update Gravity daily at midnight 0 0 * * * sh /etc/init.d/flush_pihole.sh # Then, install Unbound Unbound is a validating, recursive, caching DNS resolver.\nhttps://nlnetlabs.nl/projects/unbound/about/ I followed the Pi-hole Unbound guide, and started with:\nsudo apt install unbound\nConfigure Unbound In a Terminal, type the following:\nsudo nano /etc/unbound/unbound.conf.d/pi-hole.conf\nThis will:\nListen only for queries from the local Pi-hole installation (on port 5335)\nListen for both UDP and TCP requests\nVerify DNSSEC signatures, discarding BOGUS domains\nApply a few security and privacy tricks\nserver: # If no logfile is specified, syslog is used # logfile: \u0026#34;/var/log/unbound/unbound.log\u0026#34; verbosity: 0 interface: 127.0.0.1 port: 5335 do-ip4: yes do-udp: yes do-tcp: yes # May be set to yes if you have IPv6 connectivity do-ip6: no # You want to leave this to no unless you have *native* IPv6. with 6to4 and # Terredo tunnels your web browser should favor IPv4 for the same reasons prefer-ip6: no # Use this only when you downloaded the list of primary root servers! # If you use the default dns-root-data package, unbound will find it automatically #root-hints: \u0026#34;/var/lib/unbound/root.hints\u0026#34; # Trust glue only if it is within the server\u0026#39;s authority harden-glue: yes # Require DNSSEC data for trust-anchored zones, if such data is absent, the zone becomes BOGUS harden-dnssec-stripped: yes # Don\u0026#39;t use Capitalization randomization as it known to cause DNSSEC issues sometimes # see https://discourse.pi-hole.net/t/unbound-stubby-or-dnscrypt-proxy/9378 for further details use-caps-for-id: no # Reduce EDNS reassembly buffer size. # IP fragmentation is unreliable on the Internet today, and can cause # transmission failures when large DNS messages are sent via UDP. Even # when fragmentation does work, it may not be secure; it is theoretically # possible to spoof parts of a fragmented DNS message, without easy # detection at the receiving end. Recently, there was an excellent study # \u0026gt;\u0026gt;\u0026gt; Defragmenting DNS - Determining the optimal maximum UDP response size for DNS \u0026lt;\u0026lt;\u0026lt; # by Axel Koolhaas, and Tjeerd Slokker (https://indico.dns-oarc.net/event/36/contributions/776/) # in collaboration with NLnet Labs explored DNS using real world data from the # the RIPE Atlas probes and the researchers suggested different values for # IPv4 and IPv6 and in different scenarios. They advise that servers should # be configured to limit DNS messages sent over UDP to a size that will not # trigger fragmentation on typical network links. DNS servers can switch # from UDP to TCP when a DNS response is too big to fit in this limited # buffer size. This value has also been suggested in DNS Flag Day 2020. edns-buffer-size: 1232 # Perform prefetching of close to expired message cache entries # This only applies to domains that have been frequently queried prefetch: yes # One thread should be sufficient, can be increased on beefy machines. In reality for most users running on small networks or on a single machine, it should be unnecessary to seek performance enhancement by increasing num-threads above 1. num-threads: 1 # Ensure kernel buffer is large enough to not lose messages in traffic spikes so-rcvbuf: 1m # Ensure privacy of local IP ranges private-address: 192.168.0.0/16 private-address: 169.254.0.0/16 private-address: 172.16.0.0/12 private-address: 10.0.0.0/8 private-address: fd00::/8 private-address: fe80::/10 Limit the packet size sudo nano /etc/dnsmasq.d/99-edns.conf\nedns-packet-max=1232 Restart Unbound sudo service unbound restart\nsudo service unbound status\nTest validation Type the following in a Terminal:\ndig fail01.dnssec.works @127.0.0.1 -p 5335\nThis first command should give a status report of SERVFAIL and no IP address.\nThen, type the following in a Terminal:\ndig dnssec.works @127.0.0.1 -p 5335\nThis second command should give NOERROR plus an IP address.\nConfigure Pi-hole to use Unbound Firstly, log in to the Pi-hole web interface:\nhttp://your_server_ip/admin/login.php\nPi-hole Unbound DNS Settings\nIn Settings | DNS First, Untick any existing Upstream DNS Servers.\nThen, add the Unbound settings.\nIn Upstream DNS Servers | Custom 1 (IPv4) Set to 127.0.0.1#5335\nDisable resolvconf.conf entry for unbound The why is from here and is required for Debian Bullseye+ releases.\nFirstly, check to see if the service is running:\nsystemctl is-active unbound-resolvconf.service\nIf it is, disable it:\nsudo systemctl disable --now unbound-resolvconf.service\nThen, restart Unbound:\nsudo service unbound restart\nsudo service unbound status\nThen, install PiVPN The simplest way to setup and manage a VPN,\ndesigned for Raspberry Pi™.\nhttps://www.pivpn.io/ One line install curl -L https://install.pivpn.io | bash\nThen, step through the screens, choosing:\nUser – Whatever_You_Want\nVPN – WireGuard (or OpenVPN, the choice is yours)\nPort – 51820\nDNS – \u0026lsquo;Your_Public_IP\u0026rsquo;\nunattended-upgrades – Yes\nRemember to forward Port 51820 (UDP traffic) on the Router to the server.\nReboot and add Profiles Firstly, start by adding a new client connection:\nsudo pivpn add\nThen, enter a name for the client.\nThen, to generate a QR code that can be scanned from a mobile device:\nsudo pivpn -qr\nCheck it’s all working Firstly, in a Terminal, check the “last seen” entry by running the following:\nsudo pivpn -c\nThen, check the Pi-hole web interface under ‘Tools | Network‘ to see if the VPN gets filtered, there should be entries for:\nWhatever_You_Want.pivpn That’s how to install Pi-hole with Unbound and PiVPN.\n","permalink":"https://boffinsblog.github.io/posts/pi-hole-with-unbound-and-pivpn/","summary":"Install Pi-hole as a DNS sinkhole to block ads and trackers, Unbound as a recursive DNS resolver and PiVPN.","title":"Pi-hole with Unbound and PiVPN"},{"content":"A rootin’-tootin’ RSS aggregator. Turning an RSS feed into a Mastodon toot.\nI follow quite a few RSS feeds.\nGames, news, that sort of thing.\nSo how to aggregate them all into a Mastodon feed?\nTo create a Mastodon account that was chock full of, say, games news?\nSend a single toot from an RSS feed The workflow concept is quite straightforward.\nThe python code takes an RSS feed.\nIt takes the latest post in the feed.\nIf that post is different to the last post tooted, it crafts a Mastodon toot announcing the new feed content, and updates the Mastodon timeline.\nA cron job then runs every 15 minutes to loop through this workflow, picking up and tooting any new additions to the RSS feed.\nThe Mastodon API bit to authorise sending a toot I’ve already covered how authorise access to the Mastodon API in an earlier post .\nThe same process can be followed again to allow the Python code below to write to Mastodon.\nI have one API access token per feed, and while not strictly necessary, and is slightly more of a time investment it does compartmentalise each accounts access.\nsecrets.py MASTODON_URL = \u0026#39;https://foo-bar.com\u0026#39; MASTODON_TOKEN = \u0026#39;token string\u0026#39; MASTODON_STATUS = MASTODON_URL + \u0026#39;/api/v1/statuses\u0026#39; MASTODON_MEDIA = MASTODON_URL + \u0026#39;/api/v1/media\u0026#39; FEED_URL = \u0026#39;https://www.foo-bar.com/rss/\u0026#39; FILE_PATH = \u0026#39;/home/foo-bar/\u0026#39; FILE_NAME = \u0026#39;logo.jpg\u0026#39; FILE_DESC = \u0026#39;Online Logo Description\u0026#39; bot_with_image.py #!/usr/bin/env python # -*- coding: utf-8 -*- # A bot that takes an RSS feed and posts to Mastodon # Parses the latest entry in the RSS feed and if it doesn\u0026#39;t match the last toot, sends the latest entry from secrets import * from pathlib import Path import requests import feedparser RSS_FEED = feedparser.parse(FEED_URL) RSS_ENTRY = RSS_FEED.entries[0] # Function to build the toot, send it and update the \u0026#39;last_toot.txt\u0026#39; and \u0026#39;last_post.txt\u0026#39; file def send_toot(): # Send image file to \u0026#39;MASTODON_MEDIA\u0026#39; to generate a \u0026#39;media_id\u0026#39; media_ids = [] media = Path(FILE_PATH) / FILE_NAME # Build the authorisation header auth = { \u0026#34;Authorization\u0026#34; : f\u0026#34;Bearer {MASTODON_TOKEN}\u0026#34; } # Define the kind of file to be sent media = { \u0026#34;file\u0026#34;: (FILE_NAME, media.open(\u0026#39;rb\u0026#39;), \u0026#39;application/octet-stream\u0026#39;) } # Add a description to the file (probably a logo / image) desc = { \u0026#34;description\u0026#34;: FILE_DESC } # Send toot (to \u0026#39;MASTODON_MEDIA\u0026#39;) and collect the json response r = requests.post(MASTODON_MEDIA, headers=auth, files=media, data=desc) json_data = r.json() # This is the id identifier of the file just uploaded media_id = json_data[\u0026#39;id\u0026#39;] # Build the text part of the toot from the chosen \u0026#39;RSS_ENTRY\u0026#39; toot_str = \u0026#39;\u0026#39; toot_str += f\u0026#34;{RSS_ENTRY[\u0026#39;title\u0026#39;]}\\n\\n\u0026#34; toot_str += f\u0026#34;{RSS_ENTRY[\u0026#39;description\u0026#39;]}\\n\u0026#34; toot_str += f\u0026#34;\\n\\n{RSS_ENTRY[\u0026#39;link\u0026#39;]}\\n\\n\u0026#34; toot_str += f\u0026#34;\\n\\nThis is an automated post from the #foo-bar RSS feed dated {RSS_ENTRY[\u0026#39;published\u0026#39;]}\\n\\n\u0026#34; toot_str += f\u0026#34;\\n\\n#foo #bar #foo-bar\\n\\n\u0026#34; # Build the payload, including the newly generated \u0026#39;media_id\u0026#39; payload = { \u0026#34;status\u0026#34;: toot_str, \u0026#34;media_ids[]\u0026#34;: media_id } # Send toot (to \u0026#39;MASTODON_STATUS\u0026#39;) including the newly generated \u0026#39;media_id\u0026#39; requests.post(MASTODON_STATUS, headers=auth, data=payload) # Update the \u0026#39;last_toot.txt\u0026#39; file with \u0026#39;latest_entry\u0026#39; last_toot = open(Path(FILE_PATH) / \u0026#34;last_toot.txt\u0026#34;,\u0026#34;w\u0026#34;) last_toot.write(latest_entry) last_toot.close() # Update the \u0026#39;last_post.txt\u0026#39; file with \u0026#39;published\u0026#39; date last_post = open(Path(FILE_PATH) / \u0026#34;last_post.txt\u0026#34;,\u0026#34;w\u0026#34;) last_post.write(published) last_post.close() # Output a response print(\u0026#34;New toot sent from the foo-bar RSS feed!\u0026#34;) # Find the latest RSS entry latest_entry = RSS_ENTRY.id # Find the latest published date published = RSS_ENTRY.published # Find the \u0026#39;last_toot\u0026#39; sent with open(Path(FILE_PATH) / \u0026#34;last_toot.txt\u0026#34;) as last_toot: last_toot = last_toot.read() # If \u0026#39;latest_entry\u0026#39; does not equal \u0026#39;last_toot\u0026#39; then send a new toot based on \u0026#39;latest_entry\u0026#39; if latest_entry != last_toot: send_toot() elif latest_entry == last_toot: # Output a response print(\u0026#34;Nothing new to add from the foo-bar RSS feed!\u0026#34;) Sending a stream of toots from an RSS feed What if we want to populate a Mastodon timeline with a whole stream of RSS content?\nThe following works in a similar way to the ‘send a single toot’ approach, except it starts at the bottom of the RSS feed.\nAnd then loops upwards through the feed items.\nThat way, the latest RSS post will never be the latest toot, until the Python loop has reached the start of the RSS content.\nbot_all_with_image.py #!/usr/bin/env python # -*- coding: utf-8 -*- # A bot that takes an RSS feed and posts to Mastodon # Parses the latest entry in the RSS feed and if it doesn\u0026#39;t match the last toot, sends the latest entry from secrets import * from pathlib import Path import requests import feedparser import time RSS_FEED = feedparser.parse(FEED_URL) # Function to build the toot, send it and update the \u0026#39;last_toot.txt\u0026#39; and \u0026#39;last_post.txt\u0026#39; file def send_toot(): # Send image file to \u0026#39;MASTODON_MEDIA\u0026#39; to generate a \u0026#39;media_id\u0026#39; media_ids = [] media = Path(FILE_PATH) / FILE_NAME # Build the authorisation header auth = { \u0026#34;Authorization\u0026#34; : f\u0026#34;Bearer {MASTODON_TOKEN}\u0026#34; } # Define the kind of file to be sent media = { \u0026#34;file\u0026#34;: (FILE_NAME, media.open(\u0026#39;rb\u0026#39;), \u0026#39;application/octet-stream\u0026#39;) } # Add a description to the file (probably a logo / image) desc = { \u0026#34;description\u0026#34;: FILE_DESC } # Send toot (to \u0026#39;MASTODON_MEDIA\u0026#39;) and collect the json response r = requests.post(MASTODON_MEDIA, headers=auth, files=media, data=desc) json_data = r.json() # This is the id identifier of the file just uploaded media_id = json_data[\u0026#39;id\u0026#39;] # Build the text part of the toot from the chosen \u0026#39;RSS_ENTRY\u0026#39; toot_str = \u0026#39;\u0026#39; toot_str += f\u0026#34;{RSS_ENTRY[\u0026#39;title\u0026#39;]}\\n\\n\u0026#34; toot_str += f\u0026#34;{RSS_ENTRY[\u0026#39;description\u0026#39;]}\\n\u0026#34; toot_str += f\u0026#34;\\n\\n{RSS_ENTRY[\u0026#39;link\u0026#39;]}\\n\\n\u0026#34; toot_str += f\u0026#34;\\n\\nThis is an automated post from the #foo-bar RSS feed dated {RSS_ENTRY[\u0026#39;published\u0026#39;]}\\n\\n\u0026#34; toot_str += f\u0026#34;\\n\\n#foo #bar #foo-bar\\n\\n\u0026#34; # Build the payload, including the newly generated \u0026#39;media_id\u0026#39; payload = { \u0026#34;status\u0026#34;: toot_str, \u0026#34;media_ids[]\u0026#34;: media_id } # Send toot (to \u0026#39;MASTODON_STATUS\u0026#39;) including the newly generated \u0026#39;media_id\u0026#39; requests.post(MASTODON_STATUS, headers=auth, data=payload) # Update the \u0026#39;last_toot.txt\u0026#39; file with \u0026#39;latest_entry\u0026#39; last_toot = open(Path(FILE_PATH) / \u0026#34;last_toot.txt\u0026#34;,\u0026#34;w\u0026#34;) last_toot.write(latest_entry) last_toot.close() # Update the \u0026#39;last_post.txt\u0026#39; file with \u0026#39;published\u0026#39; date last_post = open(Path(FILE_PATH) / \u0026#34;last_post.txt\u0026#34;,\u0026#34;w\u0026#34;) last_post.write(published) last_post.close() # How many RSS entries are there (-1 as the indexing starts at 0) count = len(feedparser.parse(FEED_URL)[\u0026#39;entries\u0026#39;])-1 # Loop through all of the RSS entries (until it is less than 0, where the indexing started) print(\u0026#34;There are \u0026#34;, count+1, \u0026#34; RSS entries to toot\u0026#34;) while count != -1: latest_entry = RSS_FEED.entries[count].id # Find the published date published = RSS_FEED.entries[count].published RSS_ENTRY = RSS_FEED.entries[count] print(\u0026#34;Tooting - \u0026#34; + RSS_FEED.entries[count].title + \u0026#34; \u0026#34;, count, \u0026#34; toots to go\u0026#34;) send_toot() count = count -1 time.sleep(180) # Output a response print(\u0026#34;All done!\u0026#34;) The following two files need to be in place and writable.\nThe files do not have to contain the exact content as below, they just need to exist so that the Python script can write:\nThe last post from the RSS feed\nThe last toot sent to Mastodon\nlast_post.txt Wed, 12 Apr 2023 20:30:00 +0000 last_toot.txt https://www.foo-bar.com/rss/100 cron job I have more than one Python script set up and they are all called from a bash script every 15 minutes using cron.\nThis is what calling one of those jobs looks like.\n# Run the Mastodon bots that turn RSS feeds into toots every N minutes */5 * * * * python /home/foo/Dev/RasPi/GamesFeedRSS/bot_with_image.py \u0026amp; Finally, RSS content in a toot! Screenshot of the GamesFeedRSS Mastodon Account\nAll of the Mastodon code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/rss-to-toot/","summary":"Turn an RSS feed in to a Mastodon toot. Send either the latest RSS entry or a stream of Mastodon posts.","title":"RSS to Toot"},{"content":"Moar backup Moar restore\nAn Iceshrimp backup and restore process, now with an added Menu.\nWhy? I’ve written in the past about backing up and restoring my Calckey instance and more recently about my Firefish instance.\nAs I refuse to stop tinkering I recently moved to Iceshrimp .\nClearly this needs a backup and restore process too.\nThe Iceshrimp backup and restore process This whole process is a bit simpler than the previous ones.\nFirstly, a backup process takes a copy of the database and creates a tiny file as a flag to say that it completed successfully.\nThe second backup process takes copies of files and creates another tiny file as a flag to say that it has completed successfully.\nThe restore process is a menu asking whether you wish to restore either the database or the files.\nNeither restore process will continue unless the successful completion flag is in place.\nIceshrimp backup The backup process is split into two parts:\nA backup of the Iceshrimp database\nA backup of:\nthe default.yml file\nthe nginx conf\nthe iceshrimp.service file\nthe iceshrimp /files/ directory\nCreate server directories Firstly, ssh into the server and:\nmkdir -p /home/foo/Backups/00.logs\nmkdir -p /home/foo/Backups/10.sites/iceshrimp/database\nmkdir -p /home/foo/Backups/10.sites/iceshrimp/configs/\nmkdir -p /home/foo/Backups/10.sites/iceshrimp/files\n.pgpass .pgpass is a PostgreSQL feature that allows you to securely store the connection information.\nThe file .pgpass in a user’s home directory can contain passwords to be used if the connection requires a password\nhttps://www.postgresql.org/docs/current/libpq-pgpass.html Obviously the .pgpass file needs creating, and appropriate permissions granting.\nFirstly, create the .pgpass file:\nsudo nano /home/$USER/.pgpass\n#hostname:port:database:username:password :hostname:port:ThePostgresAccount:ThePostgresAccountPassword Then, own it:\nsudo chown $USER:$USER /home/$USER/.pgpass\nFinally, give the file the correct permissions:\nchmod 600 /home/$USER/.pgpass\nWrite it to the environment variables:\nexport PGPASSFILE='/home/$USER/.pgpass'\nCheck the environment variable using:\nenv\nThe ‘pg_dump’ command will now not ask for a password:\npg_dump 'iceshrimp' -U postgres -h localhost -p 5432 \u0026gt; \u0026quot;/home/foo/Backups/10.sites/iceshrimp/database/iceshrimp_backup_database.sql\u0026quot; -Fc\nBackup the Iceshrimp database This is a flat copy of the Iceshrimp PostgreSQL database, not the whole database as previous backup scripts have created.\nThe script first removes the file showing that a previous backup was successful\nThe removed file is replaced at the end to show that the backup has successfully completed\nThe iceshrimp_restore.sh ‘Restore the ‘iceshrimp_backup_database’ menu option will not run if this file is not in place\nFirstly, create the database backup script:\nsudo nano /etc/init.d/backup_iceshrimp_db.sh\nThen give the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_iceshrimp_db.sh\nbackup_iceshrimp_db.sh #!/bin/bash clear # The iceshrimp database backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # Remove the file showing that a previous backup was successful echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRemove the file showing that a previous backup was successful...\\e[0m\u0026#34; rm /home/foo/Backups/00.logs/iceshrimp_backup_database; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Create a new \u0026#39;iceshrimp_backup_database\u0026#39; file echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mCreate a new \u0026#39;iceshrimp_backup_database\u0026#39; file...\\e[0m\u0026#34; pg_dump \u0026#39;iceshrimp\u0026#39; -U postgres -h localhost -p 5432 \u0026gt; \u0026#34;/home/foo/Backups/10.sites/iceshrimp/database/iceshrimp_backup_database.sql\u0026#34; -Fc; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Create a file to show that the backup has successfully completed echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mCreate a file to show that the backup has successfully completed...\\e[0m\u0026#34; touch /home/foo/Backups/00.logs/iceshrimp_backup_database; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; Backup the Iceshrimp files The iceshrimp /files/ directory holds all of the ‘Drive’ files.\n(All of those avatars, headers, wallpapers and meme collections.)\nThe script first removes the file showing that a previous backup was successful\nThe removed file is replaced at the end to show that the backup has successfully completed\nThe iceshrimp_restore.sh ‘Restore the iceshrimp /files/ directory’ menu option will not run if this file is not in place\nFirstly, create the /files/ backup script:\nsudo nano /etc/init.d/backup_iceshrimp_files.sh\nThen, give the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_iceshrimp_files.sh\nbackup_iceshrimp_files.sh #!/bin/bash clear # The files backup process # ~~~~~~~~~~~~~~~~~~~~~~~~ # Remove the file showing that a previous backup was successful echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRemove the file showing that a previous backup was successful...\\e[0m\u0026#34; rm /home/foo/Backups/00.logs/iceshrimp_backup_files; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Backup all of the iceshrimp files echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mBackup the iceshrimp files...\\e[0m\u0026#34; # Remove old configs rm -r /home/foo/Backups/10.sites/iceshrimp/configs/*; # # Backup the default.yml cp -r /home/foo/iceshrimp/.config/default.yml /home/foo/Backups/10.sites/iceshrimp/configs/; # # Backup the iceshrimp.nginx.conf cp -r /etc/nginx/sites-available/iceshrimp.nginx.conf /home/foo/Backups/10.sites/iceshrimp/configs/; # # Backup the iceshrimp.service cp -r /etc/systemd/system/iceshrimp.service /home/foo/Backups/10.sites/iceshrimp/configs/; # # Remove old /files/ rm -r /home/foo/Backups/10.sites/iceshrimp/files/*; # # Backup the iceshrimp /files/ directory cp -r /home/foo/iceshrimp/files/. /home/foo/Backups/10.sites/iceshrimp/files/; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; # Create a file to show that the backup has successfully completed echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mCreate a file to show that the backup has successfully completed...\\e[0m\u0026#34; touch /home/foo/Backups/00.logs/iceshrimp_backup_files; echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; Iceshrimp Restore This script creates a small menu that presents two options:\n“Restore the iceshrimp database”: This process will restore the iceshrimp database (if a backup exists)\nCheck to see if a backup has previously completed successfully\nIf a backup is in place then carry on and restore it\n“Restore the iceshrimp /files/ directory”: This process will restore the iceshrimp /files/ (if a backup exists)\nCheck to see if a backup has previously completed successfully\nIf a backup is in place then carry on and restore it\nNOTE: The backup_iceshrimp_files.sh script also creates a copy of the default.yml, the iceshrimp.nginx.conf and the iceshrimp.service files.\nThese files are not part of this restore script and will have to be restored manually.\nTo restore both the database and the /files/ directory, the menu will have to be cycled through twice.\nFirstly, create the restore script:\nsudo nano /etc/init.d/restore_iceshrimp.sh\nThen, give the file the correct permissions:\nsudo chmod +x /etc/init.d/restore_iceshrimp.sh\nrestore_iceshrimp.sh #!/bin/bash HEIGHT=15 WIDTH=75 CHOICE_HEIGHT=4 BACKTITLE=\u0026#34;Restore iceshrimp database and / or files\u0026#34; TITLE=\u0026#34;Restoring Iceshrimp\u0026#34; MENU=\u0026#34;Choose one of the following options:\u0026#34; OPTIONS=(1 \u0026#34;Restore the iceshrimp database\u0026#34; 2 \u0026#34;Restore the iceshrimp /files/ directory\u0026#34;) CHOICE=$(dialog --clear \\ --backtitle \u0026#34;$BACKTITLE\u0026#34; \\ --title \u0026#34;$TITLE\u0026#34; \\ --menu \u0026#34;$MENU\u0026#34; \\ $HEIGHT $WIDTH $CHOICE_HEIGHT \\ \u0026#34;${OPTIONS[@]}\u0026#34; \\ 2\u0026gt;\u0026amp;1 \u0026gt;/dev/tty) clear case $CHOICE in 1) echo -e \u0026#34;\\e[1;33mRestoring the iceshrimp database\\e[0m\u0026#34; echo -e \u0026#34;\\e[1;33mThis process will restore the iceshrimp database (if a backup exists)\\e[0m\u0026#34; # Check to see if a backup has previously completed successfully # If a backup is in place then carry on and restore it if [ ! -f /home/foo/Backups/00.logs/iceshrimp_backup_database ]; then # Message echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[31mNo previous backup to restore!\\e[0m\u0026#34; else # Restore the \u0026#39;iceshrimp_backup_database\u0026#39; file echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRestore the \u0026#39;iceshrimp_backup_database\u0026#39; file\u0026#39; file...\\e[0m\u0026#34; sleep 2 pg_restore -U postgres -h localhost -p 5432 -d iceshrimp -v \u0026#34;/home/foo/Backups/10.sites/iceshrimp/database/iceshrimp_backup_database.sql\u0026#34; # Message echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[32mRestored the iceshrimp database backup\\e[0m\u0026#34; fi # Pause sleep 2 # Reload the menu bash /etc/init.d/restore_iceshrimp.sh ;; 2) echo -e \u0026#34;\\e[1;33mRestoring the iceshrimp /files/ directory\\e[0m\u0026#34; echo -e \u0026#34;\\e[1;33mThis process will restore the iceshrimp /files/ (if a backup exists)\\e[0m\u0026#34; # Check to see if a backup has previously completed successfully # If a backup is in place then carry on and restore it if [ ! -f /home/foo/Backups/00.logs/iceshrimp_backup_files ]; then # Message echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[31mNo previous backup to restore!\\e[0m\u0026#34; else # Restore the iceshrimp /files/ directory echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[1;33mRestore the iceshrimp /files/ directory...\\e[0m\u0026#34; sleep 2 sudo cp /home/foo/Backups/10.sites/iceshrimp/files/* /home/foo/iceshrimp/files # Message echo \u0026#34;\u0026#34; echo -e \u0026#34;\\e[32mRestored the iceshrimp /files/ directory\\e[0m\u0026#34; fi # Pause sleep 2 # Reload the menu bash /etc/init.d/restore_iceshrimp.sh ;; esac crontab -e Backup the iceshrimp database daily at midnight by running the script in a normal crontab.\n# # Backup the iceshrimp database daily at midnight 0 0 * * * sh /etc/init.d/backup_iceshrimp_db.sh # sudo crontab -e Backup the iceshrimp files daily at midnight by running the script in a root crontab.\n# # Backup the iceshrimp files daily at midnight 0 0 * * * sh /etc/init.d/backup_iceshrimp_files.sh # Finally (I did mention this in a prior post but it bears repeating.)\nFor anyone interested in getting into the SQL side, pgAdmin is a useful tool.\nI installed it using:\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\nsudo sh -c 'echo \u0026quot;deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/jammy pgadmin4 main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgadmin4.list \u0026amp;\u0026amp; apt update'\nsudo apt update\nsudo apt install pgadmin4\npgAdmin Screenshot\nHowever you Fediverse, Enjoy!\n","permalink":"https://boffinsblog.github.io/posts/iceshrimp-backup-and-restore/","summary":"Scripts and cron jobs to backup and restore the Iceshrimp database and Iceshrimp files.","title":"Iceshrimp Backup and Restore"},{"content":"A DIY web server Building a Nanode from a kit and getting it to talk to the world.\nIt was 2011 and the age of Arduino.\nEmbedded programmable chips were all the rage (remember this was a year before the first iteration of the now ubiquitous Raspberry Pi.)\nAt the inaugural Brighton Mini Maker Faire I bought my first Arduino R3 and saw the Nanode on a table.\nI can’t remember if one available to buy at that point as I think Ken Boak was putting the kits together by hand.\n(I had to resort to buying one from the London Hackspace and then having the postie deliver it.)\nSo, what is a Nanode? Nanode Finished\n\u0026ldquo;Nanode is an open source Arduino-like board that has in-built web connectivity.\nIt connects to a range of wireless, wired and ethernet interfaces.\nIt allows you to develop web based sensor and control systems - giving you web access to six analogue sensor lines and six digital I/O lines.\nIt costs under £20 as an easy build it yourself kit.\nNanode was designed with Hacking in mind.\u0026rdquo;\nhttps://wiki.london.hackspace.org.uk/view/Project:Nanode \u0026ldquo;Nanode is an Arduino like 8 bit microcontroller board with integrated Ethernet connectivity.\nIt has 6 analogue sensing channels and 6 spare digital I/O lines.\nIt can be used for a variety of web connected applications including remote control and monitoring, as a smart sensor, home automation, as a Tweeting device, and for publishing small amounts of data to the web.\nThe existing software allows it to be set up as an I/O webserver - a device which sends the state of its analogue and digital I/O to a web page - and which can be controlled from that webpage.\nHowever it is not restricted to just that role, it can act as a publisher device and send data to an intermediary open data service such as Pachube, and it can act as a web connected I/O device, which subscribes to a Pachube data feed and acts according to the data received from that feed - for example setting its I/O, updating a display or controlling some actuator device.\u0026rdquo;\nhttps://wiki.london.hackspace.org.uk/view/Project:Nanode/Applications Even though the board is capable of collecting an input from say a thermal sensor and reporting that via an API to a third party (it used to be Pachube which has long since vanished), I use mine to present a simple web page.\nI had a lot of fun assembling mine and everything it entailed.\nRelearning how to solder.\nThen bungling that by bridging two connections and remembering why I gave up soldering.\n(You can see the bridged connection in the photo, just above the six holes that look like a Lego brick.)\nBungled Soldering\nHowever, a couple of photos and an IRC session later the problem was identified and I dug out the solder sucker to rectify my mistake.\nProgramming a Nanode To keep the costs low the board does not have an FTDI serial/USB chip on board, it needs an FTDI cable to be able to connect to a host computer.\nIn this respect it is also similar the Arduino Pro and Pro Mini (or any number of the clones available).\n(One alternative would be to remove the ATmega328 chip, slot it into an Arduino and send code to it that way.)\nHowever, like the Arduino, the board can programmed using the standard Arduino IDE.\nI’m_Alive.ino // Based on the EtherCard library example \u0026#39;backSoon.ino\u0026#39; // 2011-01-30 \u0026lt;jc@wippler.nl\u0026gt; // // License: GPLv2 // // Present an \u0026#34;I\u0026#39;m Alive\u0026#39; web page\u0026#34; #include \u0026lt;EtherCard.h\u0026gt; #define STATIC 1 // set to 1 to disable DHCP (adjust myip/gwip values below) #if STATIC // ethernet interface ip address static byte myip[] = { 192,168,0,250 }; // gateway ip address static byte gwip[] = { 192,168,0,1 }; #endif // ethernet mac address - must be unique on your network static byte mymac[] = { 0x4e,0x61,0x6e,0x6f,0x64,0x65 }; byte Ethernet::buffer[500]; // tcp/ip send and receive buffer const char page[] PROGMEM = \u0026#34;HTTP/1.0 200 OK\\r\\n\u0026#34; \u0026#34;Content-Type: text/html\\r\\n\u0026#34; \u0026#34;Retry-After: 600\\r\\n\u0026#34; \u0026#34;\\r\\n\u0026#34; \u0026#34;\u0026lt;html\u0026gt;\u0026#34; \u0026#34;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;\u0026#34; \u0026#34;I\u0026#39;m Alive\u0026#34; \u0026#34;\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026#34; \u0026#34;\u0026lt;body\u0026gt;\u0026#34; \u0026#34;\u0026lt;h3\u0026gt;The Nanode Lives Again !!\u0026lt;/h3\u0026gt;\u0026#34; \u0026#34;\u0026lt;p\u0026gt;\u0026lt;em\u0026gt;\u0026#34; \u0026#34;OK, so not exactly a lot to see here,\u0026lt;br /\u0026gt;\u0026#34; \u0026#34;But the Nanode is alive.\u0026lt;br /\u0026gt;\u0026#34; \u0026#34;\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt;\u0026#34; \u0026#34;Link back to \u0026lt;a rel=\u0026#34;\u0026#34;me\u0026#34;\u0026#34; href=\u0026#34;\u0026#34;https://botsin.space/@is_now_gone\u0026#34;\u0026#34;\u0026gt;Mastodon\u0026lt;/a\u0026gt;\u0026#34; \u0026#34;\u0026lt;/body\u0026gt;\u0026#34; \u0026#34;\u0026lt;/html\u0026gt;\u0026#34; ; void setup(){ Serial.begin(57600); Serial.println(\u0026#34;\\n[ImAlive]\u0026#34;); // Change \u0026#39;SS\u0026#39; to your Slave Select pin, if you arn\u0026#39;t using the default pin if (ether.begin(sizeof Ethernet::buffer, mymac, 8) == 0) Serial.println( \u0026#34;Failed to access Ethernet controller\u0026#34;); #if STATIC ether.staticSetup(myip, gwip); #else if (!ether.dhcpSetup()) Serial.println(\u0026#34;DHCP failed\u0026#34;); #endif ether.printIp(\u0026#34;IP: \u0026#34;, ether.myip); ether.printIp(\u0026#34;GW: \u0026#34;, ether.gwip); ether.printIp(\u0026#34;DNS: \u0026#34;, ether.dnsip); } void loop(){ // wait for an incoming TCP packet, but ignore its contents if (ether.packetLoop(ether.packetReceive())) { memcpy_P(ether.tcpOffset(), page, sizeof page); ether.httpServerReply(sizeof page - 1); } } All of the Nanode code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/nanode/","summary":"Using an Arduino like Nanode board to present a simple web page.","title":"Nanode"},{"content":"The road even less travelled I’ve always made life difficult for myself, and my self-hosting journey is no different; from Calckey through Firefish, my Fediverse journey has led me to here; installing Iceshrimp.\nWhat is Iceshrimp? From the project web page.\nIceshrimp is yet another Misskey fork (you know the drill by now) Need help or want to contribute? Join the matrix room ! Documentation on installing (and updating) Iceshrimp: Bare metal Docker Compose Don’t like the Web UI? We test our Mastodon-compatible API against the following clients: Elk , Phanpy , Enafore , Masto-FE-standalone (Web) Mona , Toot! , Ice Cubes , Tusker , Feditext , Mastodon (iOS) Tusky , Moshidon , Megalodon, Mastodon (Android) Project goals: No-nonsense bug fixes QoL improvements Better performance Change of focus to actual community needs Project anti-goals: Flashy marketing Commercialization of any kind https://iceshrimp.dev/iceshrimp/iceshrimp There is a full ‘ How-To ‘ in the Iceshrimp git repo but these are my notes on how I installed Iceshrimp on my Pi.\nInstalling Iceshrimp – Overclocking I’d seen from my Calckey and Firefish hosting that I need to overclock my Pi.\nRun the following in a Terminal to overclock:\nsudo nano /boot/firmware/config.txt\n# Overclocking over_voltage=6 arm_freq=2000 gpu_freq=700 And reboot using sudo reboot\nRun the following in a Terminal to install a monitor and see the CPU speed in real-time:\nwatch -n 1 cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq\nInstalling Iceshrimp – Preparations Firstly update everything and add a couple of packages that may or may not already be present:\nsudo apt update; sudo apt install git wget; sudo apt full-upgrade -y; sudo reboot\nBuild Dependencies The following may already be in place:\nC/C++ compiler like GCC or Clang\n- Can be checked by running `gcc --version` in a Terminal Build tools like make\n- Can be checked by running `make --version` in a Terminal Python 3\n- Can be checked by running `python3 --version` in a Terminal The meta package ‘build-essential’ is useful too:\nsudo apt install build-essential\nInstall PostgreSQL PostgreSQL: The World’s Most Advanced Open Source Relational Database\nhttps://www.postgresql.org/ Full installation instructions are on the PostgreSQL website .\nCreate the file repository configuration:\nsudo sh -c 'echo \u0026quot;deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgdg.list'\nImport the repository signing key:\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nUpdate the package lists:\nsudo apt-get update\nInstall the latest version of PostgreSQL.\nIf you want a specific version, use \u0026lsquo;postgresql-12\u0026rsquo; or similar instead of \u0026lsquo;postgresql\u0026rsquo;:\nsudo apt-get -y install postgresql\nConfigure PostgreSQL for Iceshrimp Iceshrimp also needs a database to store its info in.\nTo add the Iceshrimp database run the following in a Terminal:\nsudo -u postgres psql\ncreate database iceshrimp with encoding = 'UTF8';\ncreate user iceshrimp with encrypted password 'super_long_postgres_password';\ngrant all privileges on database iceshrimp to iceshrimp;\n\\q\nThe ‘iceshrimp’ db needs to be owned by role ‘iceshrimp’ so ALTER the database OWNER by running the following:\nsudo -u postgres psql\n\\l\nALTER DATABASE iceshrimp OWNER TO iceshrimp;\nInstall Redis The open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker.\nhttps://redis.io/ As with PostgreSQL I already have a working Redis server as I use it alongside my WordPress installation.\nHowever, if Redis is not installed, full instructions are on this website .\ncurl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\necho \u0026quot;deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\u0026quot; | sudo tee /etc/apt/sources.list.d/redis.list\nsudo apt-get update\nsudo apt-get install redis\nInstall NodeJS Node.js® is an open-source, cross-platform JavaScript runtime environment.\nhttps://nodejs.org/en/ NodeJS is responsible for the interaction between the client (the browser) and the server (my Pi).\nNodeJS creates the dynamic page content and opens, modifies and closes files on the server: it is managed by Node Version Manager (NVM):\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash\nConfirm the installed version by running nvm -v in a Terminal.\nThe result should be ‘0.39.3’ or higher.\nTo list the available Node versions run nvm ls-remote in a Terminal.\nPick the latest version to install, in my case v21.1.0, install and use:\nnvm install v21.1.0 \u0026amp;\u0026amp; nvm use v21.1.0\nThis can be checked by running node -v and looking at the result (it should be the same as the version in the nvm install command).\nNOTE: nvm is not ‘persistent’, it will only use that version for the lifetime of the current shell.\nTo ensure that the latest version is remembered run the following in a Terminal:\nnvm alias default v21.1.0\nCreate a ‘foo’ user I prefer to run dedicated processes with a dedicated user.\nTo add a new user run the following in a Terminal:\nsudo adduser foo\nAnd password protect it:\nsudo passwd foo\nInstalling Iceshrimp Finally, it’s time to install Iceshrimp.\nChange to the newly created user:\nsudo su foo\nAnd clone the latest repository:\ngit clone https://iceshrimp.dev/iceshrimp/iceshrimp\nCopy the ‘example.yml‘ and edit:\ncp /home/foo/iceshrimp/.config/example.yml /home/foo/iceshrimp/.config/default.yml\nsudo nano /home/foo/iceshrimp/.config/default.yml\n# Final accessible URL seen by a user. url: https://social.example.com/ # PostgreSQL configuration db: host: localhost port: 5432 db: iceshrimp user: iceshrimp pass: super_long_postgres_password # Redis configuration redis: host: localhost port: 6379 pass: super_long_redis_password NOTE: There is a ‘super_long_redis_password’ here.\nThis is not the password that restricts access to the Redis configuration settings found in /etc/redis/redis.conf\nRedis works differently to other databases in that there are no predefined schemas, tables etc.\nOut of the box, a Redis instance supports 16 logical databases. These databases are effectively siloed off from one another, and when you run a command in one database, it doesn’t affect any of the data stored in other databases in your Redis instance.\nhttps://www.digitalocean.com/community/cheatsheets/how-to-manage-redis-databases-and-keys So that ‘super_long_redis_password’ is the password for the logical database that Iceshrimp is using.\nTo watch Redis working, run the following in a Terminal:\nredis-cli\nmonitor\nInstall project dependencies Corepack is an experimental tool to help with managing versions of your package managers.\nhttps://nodejs.org/api/corepack.html As it is experimental, corepack needs enabling:\ncorepack enable\ncorepack prepare yarn@stable --activate\nyarn\nBuild Iceshrimp yarn build\nFirst migration yarn run init\nCertbot Certbot is a free, open source software tool for automatically using Let’s Encrypt certificates on manually-administrated websites to enable HTTPS.\nCertbot is made by the Electronic Frontier Foundation (EFF), a 501(c)3 nonprofit based in San Francisco, CA, that defends digital privacy, free speech, and innovation.\nhttps://certbot.eff.org/pages/about I already have a HTTPS site running on this server (you are reading it) so there was no need for me to install the extra Certbot software.\nThough, if needed Certbot is a Terminal command away:\nsudo apt install certbot python3-certbot-nginx\nTo only get a certificate before editing the nginx conf file:\nsudo certbot certonly\nThen put in the domain name e.g. social.example.com to create the certificate.\nConfigure nginx Copy the ‘iceshrimp.nginx.conf‘:\nsudo cp /home/foo/iceshrimp/docs/examples/iceshrimp.nginx.conf /etc/nginx/sites-available/EXAMPLE\nEdit accordingly:\nsudo nano /etc/nginx/sites-available/EXAMPLE\nserver { listen 80; listen [::]:80; server_name social.example.com; # For SSL domain validation root /var/www/html; location /.well-known/acme-challenge/ { allow all; } location /.well-known/pki-validation/ { allow all; } location / { return 301 https://$server_name$request_uri; } } And:\nserver { listen 443 ssl http2; listen [::]:443 ssl http2; server_name social.example.com; ssl_session_timeout 1d; ssl_session_cache shared:ssl_session_cache:10m; ssl_session_tickets off; # To use Let\u0026#39;s Encrypt certificate ssl_certificate /etc/letsencrypt/live/social.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/social.example.com/privkey.pem; } Then, ‘enable’ the site by creating a symlink:\nsudo ln -s /etc/nginx/sites-available/EXAMPLE /etc/nginx/sites-enabled/\nCheck and reload nginx Then make sure it all works:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx\nRunning iceshrimp using systemd I want Iceshrimp to start up after a reboot so I create a systemd unit file and enable it.\nCopy the ‘iceshrimp.service‘ and edit:\nsudo cp /home/foo/iceshrimp/docs/examples/iceshrimp.service /etc/systemd/system/\nEdit accordingly:\nsudo nano /etc/systemd/system/iceshrimp.service\n[Unit] Description=Iceshrimp daemon [Service] Type=simple User=foo ExecStart=/usr/bin/yarn start WorkingDirectory=/home/foo/iceshrimp Environment=\u0026#34;NODE_ENV=production\u0026#34; TimeoutSec=60 SyslogIdentifier=iceshrimp Restart=always [Install] WantedBy=multi-user.target Reload the systemctl daemon and enable the iceshrimp service:\nsudo systemctl daemon-reload\nsudo systemctl enable --now iceshrimp\nAnd that should pretty much be everything!\nInstalling Iceshrimp – Updating Updates are regular and some of them significant.\nI keep an eye on the releases page to see when this needs doing.\nEDIT: I’ve just noticed that there is a notification on the Admin Control Panel saying ‘There might be an update available!’, although I rarely get to see this as I hardly ever log in as an Admin.\nUpdate everything sudo apt update \u0026amp;\u0026amp; sudo apt upgrade\nThere may be a sudo systemctl daemon-reload after an apt update if a package with a systemd unit file has been updated.\nHowever, if the kernel has been updated, there will need to be a sudo reboot.\nUpdate node.js node -v\nnvm ls-remote\nThis will list the updates available; choose the latest one.\nnvm install 21.1.0 \u0026amp;\u0026amp; nvm use 21.1.0\nStop the iceshrimp service sudo systemctl stop iceshrimp\nsudo systemctl status iceshrimp\nChange to the foo user sudo su foo\ncd /home/foo/iceshrimp\nGit commands Git commands are a bit of dark art that I don’t fully comprehend, so these commands are from the Iceshrimp installation instructions .\nI pull the latest repository by running the following Git commands in a Terminal:\ngit pull\nyarn\nyarn build \u0026amp;\u0026amp; yarn migrate\nReturn to the normal user exit\nStart the iceshrimp service sudo systemctl start iceshrimp\nsudo systemctl status iceshrimp\nFinally (I did mention this in a prior post but it bears repeating.)\nFor anyone interested in getting into the SQL side, pgAdmin is a useful tool.\nI installed it using:\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\nsudo sh -c 'echo \u0026quot;deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/jammy pgadmin4 main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgadmin4.list \u0026amp;\u0026amp; apt update'\nsudo apt update\nsudo apt install pgadmin4\npgAdmin Screenshot\nHowever you Fediverse, Enjoy!\nUPDATE 1 As of version 2023.11.1 Iceshrimp uses Git-LFS.\nGit LFS is a command line extension and specification for managing large files with Git.\nhttps://github.com/git-lfs/git-lfs/tree/main Iceshrimp will not update past 2023.11 without installing Git-LFS.\nThis repository uses Git LFS. Please make sure it is installed before cloning this repository.\nhttps://iceshrimp.dev/iceshrimp/iceshrimp/wiki/Git-LFS Update everything once more sudo apt update \u0026amp;\u0026amp; sudo apt upgrade\nInstall Git-LFS I installed Git-LFS by running the following in a Terminal:\ncurl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\nsudo apt-get install git-lfs\nStop the iceshrimp service once more sudo systemctl stop iceshrimp\nsudo systemctl status iceshrimp\nChange to the foo user once more sudo su foo\ncd /home/foo/iceshrimp\nGit commands once more I pulled the latest repository by running the following Git commands in a Terminal:\ngit fetch\ngit reset --hard origin/dev\nyarn\nyarn build \u0026amp;\u0026amp; yarn migrate\nReturn to the normal user once more exit\nStart the iceshrimp service once more sudo systemctl start iceshrimp\nsudo systemctl status iceshrimp\nUPDATE 2 Iceshrimp is moving to a .NET framework.\nA further comprehensive walk through can be found here .\n","permalink":"https://boffinsblog.github.io/posts/installing-iceshrimp/","summary":"How I installed and finally settled on Iceshrimp as my Fediverse software of choice, a full bare-metal guide.","title":"Installing Iceshrimp"},{"content":"Still paranoid Within days of posting about how to backup and restore a Calckey server there was a rebrand and everything changed to Firefish; I’m still paranoid though, so I still need a Firefish backup and restore process.\nMake the directories on the server to send the Firefish backups to ssh into the server and:\nmkdir /home/foo/Backups/firefish\nmkdir /home/foo/Backups/firefish/files\nmkdir /home/foo/Backups/firefish/postgresql\nThe following method uses ‘pg_basebackup’ to take a full copy of the whole PostgreSQL database.\nThis process can be run whilst Firefish (or any other application using PostgreSQL) is running, so is ideal for scheduling with cron.\nI tried ‘pg_dump’ and ‘pg_restore’ for the single database but I couldn’t get it to restore properly.\nAlso:\nThe command ‘pg_basebackup’ needs a username / password for the command to run.\nFortunately, PostgreSQL has the ability to store this username / password combo in a file.\nThis is the .pgpass file.\nThe postgres user password may be already known but I found that in the Calckey / Firefish installation it wasn’t set, so to update or change the postgres user password:\nsudo su - postgres\npsql\nAt the postgres prompt type:\n\\password\nEnter new password twice.\nType:\n\\q\nTo quit.\n.pgpass The file .pgpass in a user’s home directory can contain passwords to be used if the connection requires a password\nhttps://www.postgresql.org/docs/current/libpq-pgpass.html Obviously the .pgpass file needs creating, and appropriate permissions granting.\nFirstly, create the .pgpass file:\nsudo nano /home/$USER/.pgpass\n#hostname:port:database:username:password :hostname:port:ThePostgresAccount:ThePostgresAccountPassword Then, own it:\nsudo chown $USER:$USER /home/$USER/.pgpass\nFinally, give the file the correct permissions:\nchmod 600 /home/$USER/.pgpass\nWrite it to the environment variables:\nexport PGPASSFILE='/home/$USER/.pgpass'\nCheck the environment variable using:\nenv\nThe ‘pg_basebackup’ command will now not ask for a password:\npg_basebackup -h localhost -p 5432 -U postgres_user -D /home/foo/Backups/firefish/postgresql -Fp -Xs -P\nBackup the Firefish database Firstly, create the database backup script:\nsudo nano /etc/init.d/backup_firefish_db.sh\nThen give the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_firefish_db.sh\nbackup_firefish_db.sh #!/bin/bash # The remote backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~ # Backup the whole of the PostgreSQL database rm -R /home/foo/Backups/firefish/postgresql/*; pg_basebackup -h localhost -p 5432 -U postgres -D /home/foo/Backups/firefish/postgresql -Fp -Xs -P; Backup the Firefish files directory The firefish /files/ directory holds all of the ‘Drive’ files.\n(All of those avatars, headers, wallpapers and meme collections.)\nFirstly, create the /files/ backup script:\nsudo nano /etc/init.d/backup_firefish_files.sh\nThen, give the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_firefish_files.sh\nbackup_firefish_files.sh #!/bin/bash # The remote backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~ # # Remove the file to show that a previous backup was created rm /home/foo/Backups/firefish_backup_created # # Backup the Firefish configs rm -r /home/foo/Backups/firefish/configs/* cp -r /home/firefish/firefish/.config/default.yml /home/foo/Backups/firefish/configs/ # # Backup the Firefish /files/ directory rm -r /home/foo/Backups/firefish/files/ cp -r /home/firefish/firefish/files /home/foo/Backups/firefish/files/ Because the /files/ directory is owned by the Firefish user it needs a sudo cron job.\nsudo crontab -e\n# # Backup the Firefish /files/ directory hourly 0 * * * * sh /etc/init.d/backup_firefish_files.sh # Archive the two backups Firstly, create the archive script:\nsudo nano /etc/init.d/backup_firefish_zipped.sh\nThen give the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_firefish_zipped.sh\nbackup_firefish_zipped.sh #!/bin/bash # The remote backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~ # # The backup the whole of the PostgreSQL database # is called by the user crontab \u0026#39;/etc/init.d/backup_firefish_db.sh\u0026#39; # # The backup of the Firefish \u0026#39;default.yml\u0026#39; and the /files/ directory # is called by the sudo crontab \u0026#39;/etc/init.d/backup_firefish_files.sh\u0026#39; # # So call this in normal crontab after \u0026#39;/etc/init.d/backup_firefish_db.sh\u0026#39; rm /home/firefish/Backups/backup_firefish.zip; zip -r /home/foo/Backups/backup_firefish.zip /home/foo/Backups/firefish # # Create a file to show that the backup has successfully completed touch /home/foo/Backups/firefish_backup_created I backup the whole PostgreSQL and the /files/ database hourly using cron.\nThen I archive (zip) the whole lot.\ncrontab -e\n# # Backup the whole of the PostgreSQL database hourly # Then put the previous /files/ backup and this db backup into one zip 0 * * * * sh /etc/init.d/backup_firefish_db.sh; sh /etc/init.d/backup_firefish_zipped.sh # Firefish Restore Process Obviously there is no sense in keeping a server backup on the server where the original files are held.\nThat’s why there are two backups:\nA pair of folders on the server containing the database and the needed files\nA single zipped file that can be copied as part of a daily backup routine off the server\nThis way, I can easily restore from the hourly created server files and in the case of a yet-to-happen catastrophic event I can copy over the last locally held zip archive and restore from there.\nFirefish Script Screenshot\nI’m not going to cover how to get the zip archive on and off the server (hint: I use ssh keys and scp).\nThe following script executed server-side will restore (and give the correct permissions to) a Firefish PostgreSQL database and the /files/ directory.\nFirstly, create the restore script:\nsudo nano /etc/init.d/restore_firefish.sh\nThen, give the file the correct permissions:\nsudo chmod +x /etc/init.d/restore_firefish.sh\nrestore_firefish.sh #!/bin/bash # The remote restore process # ~~~~~~~~~~~~~~~~~~~~~~~~~~ # # Restore from the hourly backup on the server clear # Check to see if a backup has previously completed successfully # If a backup is in place then carry on and restore it if [ ! -f /home/foo/Backups/firefish_backup_created ]; then echo \u0026#34;No previous backup to restore!\u0026#34; exit 0 fi # Restore the Firefish \u0026#39;default.yml\u0026#39; config echo -e \u0026#34;\\e[1;33mRestore the Firefish \u0026#39;default.yml\u0026#39; config...\\e[0m\u0026#34; sudo cp /home/foo/Backups/firefish/configs/* /home/firefish/firefish/.config/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Restore the Firefish /files/ directory echo -e \u0026#34;\\e[1;33mRestore the Firefish /files/ directory...\\e[0m\u0026#34; sudo cp /home/foo/Backups/firefish/files/* /home/firefish/firefish/files echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Stop the running PostgreSQL service echo -e \u0026#34;\\e[1;33mStop the running PostgreSQL service...\\e[0m\u0026#34; sudo systemctl stop postgresql.service echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Current PostgreSQL status echo -e \u0026#34;\\e[1;33mCurrent PostgreSQL status...\\e[0m\u0026#34; sudo systemctl status postgresql echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # chown the existing PostgreSQL installation echo -e \u0026#34;\\e[1;33mchown the existing PostgreSQL installation...\\e[0m\u0026#34; sudo chown -R foo:foo /var/lib/postgresql/15/main/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Remove the existing PostgreSQL files echo -e \u0026#34;\\e[1;33mRemove the existing PostgreSQL files...\\e[0m\u0026#34; sudo rm -fr /var/lib/postgresql/15/main/* echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Copy the latest backup to the PostgreSQL target location echo -e \u0026#34;\\e[1;33mCopy the latest backup to the PostgreSQL target location...\\e[0m\u0026#34; sudo cp -R /home/foo/Backups/firefish/postgresql/* /var/lib/postgresql/15/main/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Make the PostgreSQL user take ownership of the target location echo -e \u0026#34;\\e[1;33mMake the PostgreSQL user take ownership of the target location...\\e[0m\u0026#34; sudo chown -R postgres:postgres /var/lib/postgresql/15/main/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Start the PostgreSQL service echo -e \u0026#34;\\e[1;33mStart the PostgreSQL service...\\e[0m\u0026#34; sudo systemctl start postgresql.service echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Current PostgreSQL status echo -e \u0026#34;\\e[1;33mCurrent PostgreSQL status...\\e[0m\u0026#34; sudo systemctl status postgresql echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Is Firefish running echo -e \u0026#34;\\e[1;33mIs Firefish running...\\e[0m\u0026#34; systemctl status --no-pager site.example.net.service echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; However, rather than using the sh command, the script needs running using bash:\nbash /etc/init.d/firefish_restore.sh\nTo see the correctly coloured prompts.\nFinally (I did mention this in a prior post but it bears repeating.)\nFor anyone interested in getting into the SQL side, pgAdmin is a useful tool.\nI installed it using:\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\nsudo sh -c 'echo \u0026quot;deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/jammy pgadmin4 main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgadmin4.list \u0026amp;\u0026amp; apt update'\nsudo apt update\nsudo apt install pgadmin4\npgAdmin Screenshot\nHowever you Fediverse, Enjoy!\nAll of the Firefish code examples can be found at , along with some SQL Snippets .\n","permalink":"https://boffinsblog.github.io/posts/firefish-backup-and-restore/","summary":"Scripts and cron jobs to backup and restore the PostgreSQL database and Firefish files.","title":"Firefish Backup and Restore"},{"content":"Calckey? Clacky? Is this thing on? I had been itching to set up a Fediverse server for some time but was undecided on the flavour; Mastodon or Calckey or something entirely different?\nCalckey was the eventual winner.\nWhy?\nBecause the Mastodon install was “follow these quick 45 steps in a Terminal”.\nWhich then breaks at step 44.\nCalckey was “run this one script and answer a couple of questions”.\nThis sounds like I’m not a Mastodon fan.\nI am.\nBut.\nCalckey vs Mastodon There are a couple of things that I think Mastodon does really well especially around how to manage your social graph.\nYou can bulk select people to follow and unfollow\nYou can see when someone was last active\nPosts can be set to auto delete after a set period of time\nCalckey does none of the above and I wish it did.\nInstead, Calckey presents who you follow in a lovely looking grid but you have no idea if the following is mutual or when they were last active until you click through to the account details.\nEdit: the Android ‘Milktea’ app currently does show you if the account you are following is following you back.\nFor perspective, I’ve hopped around the Fediverse for a while now with different usernames on different services running different software.\nI have bots that run when my lights turn on and I’ve already written about how to nuke a Mastodon account .\nSo why my delay in setting a Fediverse server up?\nWell, one of the reasons for my reticence around self-hosting was moving my social graph to a server that lives under the telly.\nAnd then something catastrophic happening.\nWhich will occur at some point.\nSo for me to feel comfortable there needed to be a robust backup process in place.\n(And obviously a working restore process.)\nFediverse Server Backup Process I use ‘pg_basebackup’ to take a full copy of the PostgreSQL database.\nI also tried ‘pg_dump’ and ‘pg_restore’ for the Calckey database but I couldn’t get it to restore properly.\nTo run ‘pg_basebackup’ it needs a username / password combination to run.\nFortunately, PostgreSQL has the ability to store this username / password combo in a file.\n.pgpass The file .pgpass in a user’s home directory can contain passwords to be used if the connection requires a password\nhttps://www.postgresql.org/docs/current/libpq-pgpass.htm Obviously it needs creating, and appropriate permissions granting.\nFirst, create the .pgpass file:\nsudo nano /home/$USER/.pgpass\n#hostname:port:database:username:password :hostname:port:ThePostgresAccount:ThePostgresAccountPassword Then own it:\nsudo chown $USER:$USER /home/$USER/.pgpass\nThen give the file the correct permissions:\nchmod 600 /home/$USER/.pgpass\nFinally, write it to the environment variables:\nexport PGPASSFILE='/home/$USER/.pgpass'\nThen, check the environment variable using:\nenv\nThe ‘pg_basebackup’ command will now not ask for a password:\npg_basebackup -h localhost -p 5432 -U postgres_user -D /path/to/the/remote/backup -Fp -Xs -P\nBackup the Calckey database Create the database backup script:\nsudo nano /etc/init.d/backup_calckey_db.sh\nGive the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_calckey_db.sh\nbackup_calckey_db.sh #!/bin/bash # The remote backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~ # Backup the whole of the PostgreSQL database rm -R /home/foo/Backups/calckey/postgresql/*; pg_basebackup -h localhost -p 5432 -U postgres -D /home/foo/Backups/calckey/postgresql -Fp -Xs -P; Backup the Calckey /files/ directory The calckey/files/ directory holds all of the ‘Drive’ files.\n(All of those avatars, headers, wallpapers and meme collections.)\nCreate the /files/ backup script:\nsudo nano /etc/init.d/backup_calckey_files.sh\nGive the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_calckey_files.sh\nbackup_calckey_files.sh #!/bin/bash # The remote backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~ # Backup the Calckey /files/ directory rm -r /home/foo/Backups/calckey/files/ cp -r /home/calckey/calckey/files /home/foo/Backups/calckey/files/ Because the /files/ directory is owned by the Calckey user it needs a sudo cron job.\nsudo crontab -e\n# # Backup the Calckey /files/ directory hourly 0 * * * * sh /etc/init.d/backup_calckey_files.sh # Archive the two backups Obviously the database and the files directory backups need to be in place prior to archiving.\nCreate the archive script:\nsudo nano /etc/init.d/backup_calckey_zipped.sh\nGive the file the correct permissions:\nsudo chmod +x /etc/init.d/backup_calckey_zipped.sh\nbackup_calckey_zipped.sh #!/bin/bash # The remote backup process # ~~~~~~~~~~~~~~~~~~~~~~~~~ # # Backup the whole of the PostgreSQL database # Is called by the user crontab \u0026#39;/etc/init.d/backup_calckey_db.sh\u0026#39; # # Backup the Calckey /files/ directory # Is called by the sudo crontab \u0026#39;/etc/init.d/backup_calckey_files.sh\u0026#39; # # So call this in normal crontab after \u0026#39;/etc/init.d/backup_calckey_db.sh\u0026#39; rm /home/foo/Backups/backup_calckey.zip; zip -r /home/foo/Backups/backup_calckey.zip /home/foo/Backups/calckey I backup the whole PostgreSQL and the /files/ database hourly using cron.\nThen archive (zip) the whole lot.\ncrontab -e\n# # Backup the whole of the PostgreSQL database hourly # Then put the previous /files/ backup and this db backup into one zip 0 * * * * sh /etc/init.d/backup_calckey_db.sh; sh /etc/init.d/backup_calckey_zipped.sh # Fediverse Server Restore Process Obviously there is no sense in keeping a server backup on the server where the original files are held.\nThat’s why there are two backups:\nA pair of folders on the server containing the database and the needed files\nA single zipped file that can be copied as part of a daily backup routine off the server\nThis way, I can easily restore from the hourly created server files and in the case of a yet-to-happen catastrophic event I can copy over the last locally held zip archive and restore from there.\nI’m not going to cover how to get the zip archive on and off the server (hint: I use ssh keys and scp).\nHowever the following script executed server-side will restore (and give the correct permissions to) a Calckey PostgreSQL database and the /files/ directory.\nFirst, create the restore script:\nsudo nano /etc/init.d/restore_calckey.sh\nThen give the file the correct permissions:\nsudo chmod +x /etc/init.d/restore_calckey.sh\nFiles restore_calckey.sh #!/bin/bash # The remote restore process # ~~~~~~~~~~~~~~~~~~~~~~~~~~ # # Restore from the hourly backup on the server clear # Stop the running PostgreSQL service echo -e \u0026#34;\\e[1;33mStop the running PostgreSQL service...\\e[0m\u0026#34; sudo systemctl stop postgresql.service echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Current PostgreSQL status echo -e \u0026#34;\\e[1;33mCurrent PostgreSQL status...\\e[0m\u0026#34; sudo systemctl status postgresql echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # chown the existing PostgreSQL installation echo -e \u0026#34;\\e[1;33mchown the existing PostgreSQL installation...\\e[0m\u0026#34; sudo chown -R foo:foo /var/lib/postgresql/15/main/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Remove the existing PostgreSQL files echo -e \u0026#34;\\e[1;33mRemove the existing PostgreSQL files...\\e[0m\u0026#34; sudo rm -fr /var/lib/postgresql/15/main/* echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Copy the latest backup to the PostgreSQL target location echo -e \u0026#34;\\e[1;33mCopy the latest backup to the PostgreSQL target location...\\e[0m\u0026#34; sudo cp -R /home/foo/Backups/calckey/postgresql/* /var/lib/postgresql/15/main/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Make the PostgreSQL user take ownership of the target location echo -e \u0026#34;\\e[1;33mMake the PostgreSQL user take ownership of the target location...\\e[0m\u0026#34; sudo chown -R postgres:postgres /var/lib/postgresql/15/main/ echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Start the PostgreSQL service echo -e \u0026#34;\\e[1;33mStart the PostgreSQL service...\\e[0m\u0026#34; sudo systemctl start postgresql.service echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Current PostgreSQL status echo -e \u0026#34;\\e[1;33mCurrent PostgreSQL status...\\e[0m\u0026#34; sudo systemctl status postgresql echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; # Restore the calckey /files/ directory echo -e \u0026#34;\\e[1;33mRestore the calckey /files/ directory...\\e[0m\u0026#34; sudo cp /home/foo/Backups/calckey/files/* /home/calckey/calckey/files echo -e \u0026#34;\\e[32mDone\\e[0m\u0026#34; echo \u0026#34;\u0026#34; The script needs running using:\nbash /etc/init.d/restore_calckey.sh\nTo see the correctly coloured prompts.\nSome extra notes on running my Calckey Fediverse Server Federation Federation was straightforward and happened rather quickly.\nI posted that this was a new, self-hosted instance and would appreciate a boost; within a couple of days I was federated with over 300 other Fediverse servers (instances).\nTweaking my Calckey Fediverse Server Right now this is a single user instance so it’s just me.\nI’ve turned off the Global Timeline as it was whizzing past far too quickly\nI’ve also turned off the Local Timeline as there are no other people on the server.\nTo save space I’ve also set the files to not cache.\nFinally For anyone interested in getting into the SQL side, pgAdmin is useful.\nI installed it using.\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\nsudo sh -c 'echo \u0026quot;deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/jammy pgadmin4 main\u0026quot; \u0026gt; /etc/apt/sources.list.d/pgadmin4.list \u0026amp;\u0026amp; apt update'\nsudo apt update\nsudo apt install pgadmin4\npgAdmin Screenshot\nAnyway, if you feel like following I’m no longer using Calckey; it rebranded to Firefish and then died a slow death.\nAll of the Calckey code examples can also be found at , along with some SQL Snippets .\nHowever you Fediverse, Enjoy!\n","permalink":"https://boffinsblog.github.io/posts/calckey-fediverse-server/","summary":"Scripts and cron jobs to backup and restore the PostgreSQL database and Calckey files.","title":"Calckey Fediverse Server"},{"content":"If using Mastodon Then use bot. How to create a bot to send alerts from IFTTT to Mastodon.\nBack when Twitter was fun I coded a handful of bots.\nNothing massively complicated.\nLots of If / Then / Else loops that were designed to start a conversation and elicit a response.\nLets just say they got a reaction.\nI wasn’t really too interested on repeating the process on Mastodon and anyway the Fediverse audience is wildly different to the Twitter one.\nBut I was still interested in automation and wondered what else could be done.\nMy homemade Googlebot was already controlling my Hue lights via IFTTT and I was pretty sure that IFTTT could talk to Mastodon.\nThe Mastodon API bit First things first, we need to authorise IFTTT to be able to write to the Mastodon account.\nFollowing the path ‘Edit profile | Development | New application’ will get you to the screen where you can create an application to interact with Mastodon.\nThere are a host of available API options but for this purpose, IFTTT just needs to able to ‘write’ to Mastodon i.e. send a ‘Status’ message.\nPress ‘Submit’ to create the new application.\nMastodon API Screen\nNow when you go into the newly created application the authorisation keys will be visible.\nThe ‘access token’ is the important bit here and the string we will need to paste into the IFTTT interface to authorise interaction with Mastodon.\nMastodon Add Application Screen\nThe IFTTT bit Once we have authorised an application to be able to write to Mastodon we need to have a step in the IFTTT flow that is a ‘write to Mastodon’ step.\nI had previously gone through a process to set up the Hue lights.\nI needed to add a further step that was ‘send some text to Mastodon’.\nIFTTT Add Webhook Screen\nThis extra step is a ‘webhook’.\nIFTTT Add Action Screen\nURL However, the mastodon end point (URL) will depend on the server you are writing to.\nI am was writing to botsin.space and my end point is was:\nbotsin.space/api/v1/statuses Closed\nMethod Next, we are sending a ‘POST’ request as we are trying to ‘write’ or POST to Mastodon.\nContent Type We are sending the form using the POST method.\napplication/x-www-form-urlencoded Additional Headers It is the form headers that contain the Mastodon authorisation.\nNote: Make sure that the access token has the words ‘Authorization: Bearer ‘ before it, and only one (1) space between the words and the token string.\nAuthorization: Bearer \u0026lt;Your access token\u0026gt; Body Lastly, the form body contains the content of what we are wanting to POST preceded by ‘status=’.\nstatus=Somebody turned the kitchen lights on Finally, to create the webhook, click ‘Update action’.\nFinally A bot that sends whatever text was in the IFTTT ‘status=’ step to the Mastodon account using the Mastodon API.\nHowever, if you would like to see the real world bot in action it can could be found at botsin.space (no longer alive).\nMastodon Bot Results\nAnd that is how you create a Mastodon IFTTT bot.\nThis one is straightforward and I’ll look at more complex interactions at a later date.\n","permalink":"https://boffinsblog.github.io/posts/mastodon-ifttt-bot/","summary":"Twitter bots were fun while they lasted but Mastodon bots can be informative and fun too.","title":"Mastodon / IFTTT Bot"},{"content":"Tidying up. I wanted to delete an old Mastodon account (and especially nuke those pesky Favourites).\nTotally delete.\nAvatars, banners, favourites, images, posts, everything.\nRemoving old posts is a trivial matter now that Mastodon allows for automated post deletions.\nYou can set the expiry threshold to a week and just wait.\nMastodon Post Deletion Screen\nBut those ‘Favourites’ remained, and I had thousands of them to delete.\nThousands.\nI could have sat there for a couple of hours unclicking all of of those little stars but I wasn’t about to do that.\nUsing ‘mastodon-archive’ to delete those pesky Favourites Thankfully I came across ‘ mastodon-archive ‘ which says it:\n“allows you to make an archive of your statuses, your favourites, bookmarks and the media in both your statuses, your favourites and your bookmarks.”\nhttps://pypi.org/project/mastodon-archive/ Additionally there is a process around “Expiring your toots and favourites”, which Mastodon does not do at all.\nIt comes with a warning, which sounds like it will:\n“Warning: This is a destructive operation. You will delete your toots on your instance, or unfavour your favourites, or dismiss your notifications on your instance.”\nThe full documentation is in the mastodon project and there are a slew of options available, including downloading an archive for offline processing with jq , which is:\n“like sed for JSON data – you can use it to slice and filter and map and transform structured data with the same ease that sed, awk, grep and friends let you play with text.”\nhttps://jqlang.org/ But I was only interested in destroying things so got straight to work, issuing the below in a terminal:\nsudo apt install python3-pip\nsudo pip3 install mastodon-archive\nI didn’t bother updating my $PATH and ran the command below which will, after an authorisation prompt, create the initial Mastodon archive:\n/home/foo/.local/bin/mastodon-archive archive my_unwanted account@mastodon.social\nThen, running the below will loop through that Mastodon archive and remove all the posts that have been favourited:\n/home/foo/.local/bin/mastodon-archive expire --collection favourites --older-than 0 --confirmed my_unwanted account@mastodon.social\nSimple.\nThe whole process took about 10 minutes to create the initial archive and another 20 to loop through and delete them all.\nAs is usual with a Linux terminal command, there is no verbose output.\nIf I had one very minor gripe, it would be to include a -v flag.\n","permalink":"https://boffinsblog.github.io/posts/delete-your-mastodon-favourites/","summary":"Removing old posts is a trivial matter now that Mastodon allows for automated post deletions, removing old favourites is not so straighforward.","title":"Delete Your Mastodon Favourites"},{"content":"Not everyone has something to hide I use PGP for the encryption and decryption of my email, I also use Geany as my text editor, it seemed sensible to combine the two.\nWhat is PGP encryption? Pretty Good Privacy (PGP) is an encryption program that provides cryptographic privacy and authentication for data communication.\nhttps://en.wikipedia.org/wiki/Pretty_Good_Privacy PGP encryption is almost impossible to hack. That’s why it’s still used by entities that send and receive sensitive information, such as journalists and hacktivists.\nhttps://www.upguard.com/blog/what-is-pgp-encryption What is Geany ? Geany is a powerful, stable and lightweight programmer’s text editor that provides tons of useful features without bogging down your workflow. It runs on Linux, Windows and macOS is translated into over 40 languages, and has built-in support for more than 50 programming languages.\nhttps://www.geany.org/ I like PGP and have had a key since 2000, (in 1997 PGP morphed into OpenPGP, moving away from a proprietary product to an open standard).\nSo it’s only natural that I combine Geany and PGP encryption via the GeanyPG plugin to encrypt / decrypt blocks of text on the fly.\nSo, what is GeanyPG? GeanyPG is a plugin for Geany that allows the user to encrypt, decrypt and verify signatures with GnuPG.\nhttps://plugins.geany.org/geanypg.html Install Geany To install Geany it’s the usual dance of update and install.\nI’m on a Debian based distro so my terminal commands are:\nsudo apt-get update\nsudo apt-get install geany\nOther installation methods are available, like the official repository if you need the latest version:\nsudo add-apt-repository ppa:geany-dev/ppasudo\napt-get update\nsudo apt-get install geany\nThe GeanyPG plugin is part of the base version but if you fancy looking at all the other plugins there is a whole GitHub repo .\nTo add the plugin, open Geany and using the Menu go to ‘Tools | Plugin Manager’.\nFrom there, tick the box to activate the GeanyPG plugin.\nActivating the GeanyPG plugin\nEncrypting using PGP and Geany To start encrypting and decrypting, type something and using the menu select ‘Tools | Geany | Encrypt’.\nNote: Either the contents of the whole Geany window or any highlighted text will be encrypted.\nThe popup window will prompt you to decide whose Public key you want to encrypt to.\nEncrypting to a PGP Key\nThen, hey presto, an encrypted PGP block, ready to be copy and pasted into an email.\nIf you receive a message that has been encrypted to your Public key, select ‘Tools | Geany | Decrypt’ and as long as you have the corresponding Private key it can be decrypted and read.\nPGP is an older, secure form of encryption that has failed to be adopted widely for personal use, probably due to poor UI e.g. nobody wants to have to use a Terminal to examine their keyring.\nMore communications should be secure.\nMore email clients should incorporate it, and Thunderbird has it baked in.\nSecurity is a hobbyhorse of mine and I’ll add more posts around this subject.\n","permalink":"https://boffinsblog.github.io/posts/encryption-using-pgp-and-geany/","summary":"How to use the popular text editor to encrypt and decrypt a block of text.","title":"Encryption Using PGP and Geany"},{"content":"I love graphs. I wanted to add some traffic analysis to my Pi based Apache web server and had a couple of prerequisites; real time but good looking; enter GoAccess .\nI’m already using the old school AWStats installed and configured but it’s not real-time and I wanted something funky looking.\nHowever, none of the WordPress plugins I was looking into were doing it for me.\nMost of them either send a cookie (which I am against doing, as I want visitors to have confidence that the site has zero ads) or were really basic.\nWhat is it? GoAccess is an open source real-time web log analyzer and interactive viewer that runs in a terminal in *nix systems or through your browser.\nhttps://goaccess.io/ Sounds straightforward ?\nWell, it kinda was.\nI used my dynamic DNS service to forward a new domain name to my routers IP.\n(I could have attached the traffic reports to the existing sites but I’m thinking ahead and planning a central site that is a portal to all of the analysis.)\nBack on my sever I needed somewhere to direct this traffic to.\nApache server setup Setup the new vHost sudo nano /etc/apache2/sites-available/newsite.conf\nThis is a copy / paste from an existing vHosts file that I recycled, it may have too many options enabled.\n\u0026lt;VirtualHost *:80\u0026gt; ServerAdmin admin@newsite.net DocumentRoot /var/www/html/newsite.net/ ServerName newsite.net ServerAlias newsite.net *.newsite.net CustomLog /var/log/apache2/newsite_access.log combined ErrorLog /var/log/apache2/newsite_error.log \u0026lt;Directory /var/www/html/newsite.net\u0026gt; Options Indexes Includes FollowSymLinks MultiViews AllowOverride All Require all granted \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; There are various arguments about having discrete logs per site.\nSome camps are in the ‘dump it all in one log and slice it up later’.\nI’m in the ‘slice it up now, I’ll consolidate it all later if needed’ camp.\nUpdate the hosts file sudo nano /etc/host\n192.168.0.90 newsite.net newsite Make the Apache site live sudo a2ensite newsite.conf\nThen, time to create some temporary content, just to check that the name is resolving, and Apache is responding with something.\nCreate the www directory and a holding page sudo mkdir /var/www/html/newsite.net/\nsudo nano /var/www/html/newsite.net/index.html\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;newsite\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;br\u0026gt; blah, blah, blah\u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; So far, so good, http://newsite.net seems to be a working webpage.\n(GoAccess can deal with https:// sites by passing the certificate in the reporting string but I’m keeping it simple right now.)\nI’m not particularly concerned about random people viewing the logs but I’m going to require authentication to view the page anyway.\nCreate a password In order to password protect the folder, I had to create a password file.\nsudo htpasswd -c /etc/apache2/.htpasswd foo\nGo through the usual dance of entering the password twice.\nAdd the .htaccess file sudo nano /var/www/html/newsite.net/.htaccess\nAuthType Basic AuthName \u0026#34;Password Required\u0026#34; Require valid-user AuthUserFile /etc/apache2/.htpasswd Finally, restart Apache sudo /etc/init.d/apache2 restart\nSo, now I have a working webpage that requires a password to view.\nThe ‘goaccess’ package is available via the package manager of my Linux distribution but it’s an older version.\nHowever, there is a Debian/Ubuntu repository containing the latest stable version.\nI fancied living on the edge, so installed by the tried and tested ‘build’ method.\nBuilding GoAccess from source sudo apt install libncursesw5-dev libgeoip-dev libtokyocabinet-dev build-essential\nwget https://tar.goaccess.io/goaccess-1.7.2.tar.gz\ntar -xzvf goaccess-1.7.2.tar.gz\ncd goaccess-1.7.2/\n./configure --enable-utf8 --enable-geoip=legacy\nmake\nsudo make install\ngoaccess --version\nConfigure GoAccess To find the config file I used:\ngoaccess --dcf\nWhich told me where mine is located (it could have been in a number of places).\nsudo /usr/local/etc/goaccess/goaccess.conf\nHowever, I did the very minimum and uncommented only the ‘Time’, ‘Date’ and ‘Log Format’ options.\n# The following time format works with any of the # Apache/NGINX\u0026#39;s log formats below. # time-format %H:%M:%S # The following date format works with any of the # Apache/NGINX\u0026#39;s log formats below. # date-format %d/%b/%Y # NCSA Combined Log Format log-format %h %^[%d:%t %^] \u0026#34;%r\u0026#34; %s %b \u0026#34;%R\u0026#34; \u0026#34;%u\u0026#34; Traffic data can be output to a terminal, a real-time webpage, a static html file or any number of output formats (including json, csv).\nApache traffic analysis Terminal reporting goaccess -f /var/log/apache2/newsite_access.log --log-format=COMBINED\nReal-time reporting The real-time reporting was where I came unstuck.\nI have to stress this was 100% my fault.\nI’d forgotten that I have a tightly locked down Debian / Apache system.\nOnly really basic ports are open with a very limited number of services allowed to connect and allow traffic through.\nSo, after several hours of banging my head on the desk, I remembered that I need to open a port on the router to allow GoAccess traffic in, and I need to open a port on the firewall to allow traffic through.\nUpdate port forwarding on the router Port 7890 is the default GoAccess port, so I sent traffic on 7890 back to 192.168.0.90 (my server IP).\nAllow access through the firewall Debian nftables is my firewall of choice so I needed to allow that traffic to pass through; however I have recently changed over to UFW and the configuration is a lot simpler.\nI added the GoAccess listening port to the list of ports that can pass traffic through the firewall.\nsudo nano /etc/nftables.conf\n# activate the following line to accept common local services tcp dport { 22, 80, 443, 7890 } ct state new accept Real-time reporting commands This takes only the latest ‘newsite_access.log‘ and sends it to ‘newsite.net/index.html‘.\ngoaccess /var/log/apache2/newsite_access.log -o /var/www/html/newsite.net/index.html --log-format=COMBINED --real-time-html\nIn order to look at all traffic, this command uses zcat to decompress every archived ‘newsite_access.log‘ files, pipe ( | ) them to goaccess where it adds in the latest uncompressed ‘newsite_access.log‘ and sends everything to ‘newsite.net/index.html‘.\nzcat -f /var/log/apache2/newsite_access.log* | goaccess /var/log/apache2/newsite_access.log -o /var/www/html/newsite.net/index.html --log-format=COMBINED --real-time-html\nStatic reporting commands A static report can be generated by removing ‘–real-time-html’ from the above command.\ncron syntax When a cron job is needed the command string is slightly different.\nThe full path to zcat and goaccess is required, also notice the ‘- -o’ in the string rather than ‘-o’.\nThis is because goaccess needs to know that you are piping data by using the extra ‘-‘.\n/usr/bin/zcat -f /var/log/apache2/newsite_access.log* | /usr/local/bin/goaccess /var/log/apache2/newsite_access.log - -o /var/www/html/newsite.net/index.html\n@reboot I wanted my logging to restart when the server reboots.\nSo I added a quick script sudo nano /etc/init.d/reboot_goaccess.sh\n#!/bin/bash # Refresh the GoAccess REAL-TIME webpage from the Apache access logs # Update newsite_access.net webpage - (Notice the - -o that cron needs) /usr/bin/zcat -f /var/log/apache2/newsite_access_access.log* | /usr/local/bin/goaccess /var/log/apache2/newsite_access.log - -o /var/www/html/newsite.net/index.html --log-format=COMBINED --real-time-html Made it executable sudo chmod +x /etc/init.d/reboot_goaccess.sh\nUpdated root cron sudo crontab -e\n# # Start the GoAccess real-time reporting webpage on boot @reboot sh /etc/init.d/reboot_goaccess.sh # Tinkering with traffic analysis using GoAccess and Apache There are a wealth of tinkering possibilities.\nEnabling / disabling the default info panels\nCustom colouring for the panels\nSlicing up the logs to include / exclude date ranges, crawlers, bots and more\nLoads more, really, lots to tinker with\nThe list seems endless.\nFinally I really like it but then I’m the target audience for these kind of things.\nIf I hadn’t bungled the port forwarding / firewall bit, the whole process would have taken about half an hour.\nApache Traffic Analysis Using GoAccess\nUpdate: I’ve been using GoAccess for Apache traffic analysis for a few months now and it has been flawless.\nAlso, I’ve recently moved all my sites over to nginx and the only change I had to make was to the path to the log files to keep everything running perfectly.\n","permalink":"https://boffinsblog.github.io/posts/apache-traffic-analysis-using-goaccess/","summary":"A guide on how to implement real-time visitor traffic analysis on an Apache web server using GoAccess.","title":"Apache Traffic Analysis Using GoAccess"},{"content":"Still wondering why I bought this. I’m always trying out different ways of upgrading Debian (or installing a different OS) on my Gemini PDA; anything more recent that the ancient shipped version would be nice.\nI’ve had one of these devices from Planet Computers since their Indiegogo campaign, but upgrading any OS onto it is not straightforward at all.\nI love the idea of it but for me, it’s a device looking for a purpose.\nHowever every so often I forget about the hours I’ve sunk into finding a use for it, dig it out of the cupboard of forgotten tech and boot it up.\nCurrent Debian state on the Gemini PDA Stuck on Debian 9 (stretch)\nThere is a path to upgrading to Debian 10 (buster) but a simple apt-get upgrade will fail\nNo further updates including security updates\nCannot set a static IP so no easy way to use SSH\nCannot set a wpa_supplicant so no WiFi auto-connect\nTo cut down on some of the time I spend tinkering with this thing, I put together a couple of scripts that:\nUpdate the base system, install some tools and apps and create a new user\nAdd scripts, configure existing apps and add some new ones\nRun these using bash to get the nice colourful output in the Terminal:\nbash 01.Install.sh\nbash 02.Configure.sh\n01.Install.sh This will:\nDisable ancient repos\nUpdate APT repos and install apt-transport-https\nPurge VLC before installing again later (causes upgrading error)\nUpgrade the system, this one will fail because of the LibreOffice nonsense (causes upgrading error)\nRepair LibreOffice\nUpdate APT repos and install some tools\nUpdate APT repos and install some apps\nUpgrade the system – system wide upgrade\nSet Timezone\nSet Locale as en_GB.UTF-8\nCreate a new user\nChange the hostname\nEnable avahi\nConfigure wpa_supplicant for automatic login\nSymlink the config into -wlan0.conf\nConfigure wlan0 interface\nFinally, tidy everything up\n02.Configure.sh This will:\nConfigure Wireshark\nMake the Terminal colourful\nDelete the gemini user\nRemove some directories\nAdd some directories\nAdd and configure the .ssh directory\nPopulate the .ssh directory\nAdd some scripts\nInstall Arduino from backup and configure\nInstall VNC from backup\nConfigure Conky \u0026amp; Compton\nAuto-hide the bottom panel\nAdd the ‘mount’ scripts to the menu\nHowever, Debian is not the only choice.\nThe Gemini PDA has a variety of operating systems available to use.\n(And I’ve been through most of them over the years.)\nI’m not going to cover how to re-flash a device (there are numerous articles about that on the interwebs) nor am I going to bemoan why there is no active development on any of these.\nAlternatives to Debian when upgrading the Gemini PDA If you are upgrading Debian on a Gemini PDA this is an overview of the current OS situation.\nOther OSs are available I wish that these were all straightforward to install, but they are not.\nAndroid is easily the easiest with a re-flash back to stock.\nHowever, Sailfish comes in a close second with PostmarketOS being the most complex to get up and running.\nAll have their idiosyncrasies though.\nAndroid Stuck on Android 8\nNo further updates including security updates\nSome current Apps no longer function under an OS from 2017\nThere seems to be no rooted Android image available so I was never able to have an unlocked boot-loader (meaning no alternatives like Lineage OS)\nSailfish Looks lovely but takes some getting used to\nHas an easy to use gesture based interface\nA nice RSS reader\nNo Android integration due to a lack of Alien Dalvik\nA lack of useful (to me) Apps and with some Portrait only\nMaps work but were awkward to get set up\nKali All of the Debian issues but with some cool tools\nNo monitor mode for this chip set so no fully functioning Wireshark\nPostmarketOS I compiled the image file but borked the flashing process, all my fault and I’ll return to this one at some point\nAll of the Gemini PDA code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/upgrading-debian-on-a-gemini-pda/","summary":"There is an upgrade path to Debian 10 but an \u0026lsquo;apt-get upgrade\u0026rsquo; will fail. This bash script will help.","title":"Upgrading Debian on a Gemini PDA"},{"content":"Not Battlestar Galactica. Most people associate a Larson Scanner with Battlestar Galactica (1978) and Knight Rider (1982) but there was an underrated sci-fi film in 1979 called The Black Hole , with a robot sidekick henchman called Maximilian .\nScrewedSculpts asked if I could create this iconic effect but for a version of Maximilian that he was building.\nFirst things first – chip choices The choice is between an Arduino Mini and an Arduino Nano.\nBoth would work and it’s a question of how to power the finished unit.\nYes these are clones, however the end product is going to live permanently in a robot shell with no future access.\nArduino Mini vs Arduino Nano\nLarson Scanner prototype These 10mm clear LEDs are crazy bright but the overall form factor could be too wide.\nArduino Mini Larson Breadboard\nLED comparison The 10mm LEDs would have a greater impact and are brighter overall but would take up too much horizontal space.\nLED Comparison\nA slight redesign So at this point it was decided to add in motion detection functionality.\nAnd then add a PIR sensor into the mix and make it responsive to nearby movement.\nThe plan being that when movement is detected it will trigger the scanner effect.\nI have zero CAD skills but I do own a pen.\nWho Needs EAGLE?\nA working PIR Using a pair of breadboards I can now trigger the scanner effect.\nThis particular PIR detector is (I think) the HC-SR505 with an effective range of 3m.\nAdded PIR Sensor\nLarson Scanner LEDs in place Now that the proof of concept is all working, time to dig out the toolbox.\nVeroBoard With LEDs\nLarson Scanner LEDs and resistors in place And then closely followed by adding all the resistors.\n(The underneath of the board has already had the appropriate tracks cut.)\nVeroBoard With LEDs And Resistors\nFinally, all wired up And then start to create a wiring loom.\nLED Board Wired\nTesting the Arduino The LED board has been wired up and liberal amounts of hot glue added to ensure that none of the wires break free.\nTesting to see that all of the LEDs still work.\nLED Board Test Rig\nTesting the Arduino and PIR Then, with a working LED board and a known working Arduino it’s time to add in the PIR module.\nThere are two sets of buttons now:\nA power button that when on, the scanner effect will run\nA sensor button that then on will only trigger the scanner effect when activated (assuming the power is on)\nLED Board Test Rig With PIR\nFinal board and all wrapped up Finally all working and wrapped up in cable tidy wrap.\nLED Board Completed With PIR All Wrapped Up\nI didn’t build the robot that this was going to live in; there is however a Facebook video of the finished piece .\nLarson Scanner Arduino sketch The scanner code came from the incredulist.blogspot .\nAlso, the PIR code attribution is in the comments.\nBoth of the above work fine in isolation, any errors after that are all mine.\nI hope this sketch also helps someone looking to do something similar.\nBack_And_Forth_3_With_Motion_Sensor.pde // // PORTDB_larsen2ways // // goes both ways: Back AND Forth :) // // 11 // 1098765432 const word dimbits[] = { 0b0000000011111100, // frame 1 0b0000000011111000, 0b0000000011110000, 0b0000000011100000, 0b0000000011000000, 0b0000000010000000, 0b0000000010000000, 0b0000000010000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000111111000, // frame 2 0b0000000111110000, 0b0000000111100000, 0b0000000111000000, 0b0000000110000000, 0b0000000100000000, 0b0000000100000000, 0b0000000100000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000001111110000, // frame 3 0b0000001111100000, 0b0000001111000000, 0b0000001110000000, 0b0000001100000000, 0b0000001000000000, 0b0000001000000000, 0b0000001000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000011111100000, // frame 4 0b0000011111000000, 0b0000011110000000, 0b0000011100000000, 0b0000011000000000, 0b0000010000000000, 0b0000010000000000, 0b0000010000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000111111000000, // frame 5 **** 0b0000111110000000, 0b0000111100000000, 0b0000111000000000, 0b0000110000000000, 0b0000100000000000, 0b0000100000000000, 0b0000100000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000111110000000, // frame 6 0b0000111100000000, 0b0000111000000000, 0b0000110000000000, 0b0000110000000000, 0b0000010000000000, 0b0000010000000000, 0b0000010000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000111100000000, // frame 7 0b0000111000000000, 0b0000111000000000, 0b0000111000000000, 0b0000011000000000, 0b0000001000000000, 0b0000001000000000, 0b0000001000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000111100000000, // frame 8 0b0000111100000000, 0b0000111100000000, 0b0000011100000000, 0b0000001100000000, 0b0000000100000000, 0b0000000100000000, 0b0000000100000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000111110000000, // frame 9 0b0000111110000000, 0b0000011110000000, 0b0000001110000000, 0b0000000110000000, 0b0000000010000000, 0b0000000010000000, 0b0000000010000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000111111000000, // frame 10 0b0000011111000000, 0b0000001111000000, 0b0000000111000000, 0b0000000011000000, 0b0000000001000000, 0b0000000001000000, 0b0000000001000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000011111100000, // frame 11 0b0000001111100000, 0b0000000111100000, 0b0000000011100000, 0b0000000001100000, 0b0000000000100000, 0b0000000000100000, 0b0000000000100000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000001111110000, // frame 12 0b0000000111110000, 0b0000000011110000, 0b0000000001110000, 0b0000000000110000, 0b0000000000010000, 0b0000000000010000, 0b0000000000010000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000111111000, // frame 13 0b0000000011111000, 0b0000000001111000, 0b0000000000111000, 0b0000000000011000, 0b0000000000001000, 0b0000000000001000, 0b0000000000001000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000011111100, // frame 14 *** 0b0000000001111100, 0b0000000000111100, 0b0000000000011100, 0b0000000000001100, 0b0000000000000100, 0b0000000000000100, 0b0000000000000100, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000001111100, // frame 15 0b0000000000111100, 0b0000000000011100, 0b0000000000001100, 0b0000000000001100, 0b0000000000001000, 0b0000000000001000, 0b0000000000001000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000000111100, // frame 16 0b0000000000011100, 0b0000000000011100, 0b0000000000011100, 0b0000000000011000, 0b0000000000010000, 0b0000000000010000, 0b0000000000010000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000000111100, // frame 17 0b0000000000111100, 0b0000000000111100, 0b0000000000111000, 0b0000000000110000, 0b0000000000100000, 0b0000000000100000, 0b0000000000100000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, // 1098765432 0b0000000001111100, // frame 18 0b0000000001111100, 0b0000000001111000, 0b0000000001110000, 0b0000000001100000, 0b0000000001000000, 0b0000000001000000, 0b0000000001000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000, 0b0000000000000000}; unsigned long duration = 0; unsigned long times_out = 7500; // frame\u0026#39;s time out \u0026#34;speed\u0026#34; // 500 for way fast // 50000 for way slow int idx; byte framepointer = 0; /* * ////////////////////////////////////// * //making sense of the sensor\u0026#39;s output * ////////////////////////////////////// * * Switches a LED according to the state of the sensors output pin. * Determines the beginning and end of continuous motion sequences. * * @author: Kristian Gohlke / krigoo (_) gmail (_) com / https://krx.at * @date: 3. September 2006 * * kr1 (cleft) 2006 * released under a creative commons \u0026#34;Attribution-NonCommercial-ShareAlike 2.0\u0026#34; license * https://creativecommons.org/licenses/by-nc-sa/2.0/de/ * * The sensor\u0026#39;s output pin goes to HIGH if motion is present. * However, even if motion is present it goes to LOW from time to time, * which might give the impression no motion is present. * This program deals with this issue by ignoring LOW-phases shorter than a given time, * assuming continuous motion is present during these phases. * */ ///////////////////////////// //VARS //the time we give the sensor to calibrate (10-60 secs according to the datasheet) //different sensor so smaller calibration time int calibrationTime = 5; //the time when the sensor outputs a low impulse long unsigned int lowIn; //the amount of milliseconds the sensor has to be low //before we assume all motion has stopped long unsigned int pause = 5000; boolean lockLow = true; boolean takeLowTime; int radPin = 12; //the digital pin connected to the sensor\u0026#39;s output //int ledPin = 13; ///////////////////////////// //SETUP void setup(){ //Larson //pinMode(13,OUTPUT); pinMode(2, OUTPUT); pinMode(3, OUTPUT); pinMode(4, OUTPUT); pinMode(5, OUTPUT); pinMode(6, OUTPUT); pinMode(7, OUTPUT); pinMode(8, OUTPUT); pinMode(9, OUTPUT); pinMode(10, OUTPUT); pinMode(11, OUTPUT); //PORTD \u0026amp;= 0b00000011; Serial.begin(9600); pinMode(radPin, INPUT); //pinMode(ledPin, OUTPUT); digitalWrite(radPin, LOW); //give the sensor some time to calibrate Serial.print(\u0026#34;calibrating sensor \u0026#34;); for(int i = 0; i \u0026lt; calibrationTime; i++){ Serial.print(\u0026#34;.\u0026#34;); delay(1000); } Serial.println(\u0026#34; done\u0026#34;); Serial.println(\u0026#34;SENSOR ACTIVE\u0026#34;); delay(50); } //////////////////////////// //LOOP void loop(){ if(digitalRead(radPin) == HIGH){ //digitalWrite(ledPin, HIGH); //the led visualizes the sensors output pin state if(lockLow){ //makes sure we wait for a transition to LOW before any further output is made: lockLow = false; Serial.println(\u0026#34;---\u0026#34;); Serial.print(\u0026#34;motion detected at \u0026#34;); Serial.print(millis()/1000); Serial.println(\u0026#34; sec\u0026#34;); delay(50); } // Larson code for (framepointer = 0; framepointer \u0026lt; 216; framepointer = (framepointer + 12)) // 0-11,12-33,24-35,36-47,48-59,60-71,72-83,84-95,96-107, // 108-119,120-131,132-143,144-155,156-167,168-179, // 180-191,192-203,204-215,216 { for(duration=0; duration\u0026lt;times_out; duration++) // times_out is the number of // frame repetitions { for(idx = framepointer; idx \u0026lt; (framepointer + 12); idx++) { commitPORTs(); } } } framepointer = 0; //delay(500); takeLowTime = true; } if(digitalRead(radPin) == LOW){ //digitalWrite(ledPin, LOW); //the led visualizes the sensors output pin state if(takeLowTime){ lowIn = millis(); //save the time of the transition from high to LOW takeLowTime = false; //make sure this is only done at the start of a LOW phase } //if the sensor is low for more than the given pause, //we assume that no more motion is going to happen if(!lockLow \u0026amp;\u0026amp; millis() - lowIn \u0026gt; pause){ //makes sure this block of code is only executed again after //a new motion sequence has been detected lockLow = true; Serial.print(\u0026#34;motion ended at \u0026#34;); //output Serial.print((millis() - pause)/1000); Serial.println(\u0026#34; sec\u0026#34;); delay(50); } } } // Larson array void commitPORTs () { PORTD = dimbits[idx]; PORTB = dimbits[idx]/256; } All of the Arduino code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/larson-scanner/","summary":"ScrewedSculpts asked if I could create this iconic effect but for a version of Maximilian that he was building.","title":"Larson Scanner"},{"content":"A Robot With A CCTV Camera ? That You Control via The Internet? The idea was to use a Pan / Tilt HAT and then add a camera to make a CCTV system, make it accessible / controllable from the Internet and contain it in something unusual looking; something that I wouldn’t mind having on a shelf (maybe a robot?).\nIt\u0026rsquo;s a good starting point in getting a pan / tilt Hat working and sending images to the Internet.\nAll of the bits for the robot CCTV build Some partially assembled pieces:\n1 x Pi 3A 1 x Pan / Tilt HAT 1 x Camera module Not shown are:\n1 x NeoPixel Stick A diffuser (which came with the NeoPixel Stick)\nRasPi3A and PanTilt Camera\nSome extra long nylon bolts were added so that the Pi could be mounted underneath the HAT.\nThese were far too long but needed.\nI could have trimmed them but as the finished unit was never going to be visible I left them alone.\nPi3A Board and Extension Screws\nYou can see that the bolts are too long but as I mentioned I decided to leave them untrimmed.\nAssembled Side View\nIt’s quite a squeeze but it all fits.\nTight Squeeze!\nThe fully assembled top view with the clearly visible NeoPixel Stick.\nThe stick was a late build decision but it does look good!\nIt was always an option that then provided me with the opportunity to write a neat menu system for the lights.\nAssembled Top View\nAs the whole thing is to be encased and impossible to adjust or repair, I made sure that the connectors wouldn’t budge.\nHot Glue Keeping Wires Secure\nDid I mention hot glue? Here again making sure that the nylon bolts on the camera module stay in place.\nHot Glue Keeping Camera Module Secure\nFinished The whole unit in its final resting place, standing tall!\nScrewbot Standing Tall\nBit of a close up of the Pan / Tilt unit and camera.\nIt’s a odd looking CCTV system for sure being housed in a recycled robot.\nA big thank-you to ScrewedSculpts for his work on the robot, I just did the nerdy bits.\nScrewbot Closeup\nMaking the robot move and operate the CCTV remotely There are quite a few guides on how to do this, and I also found some of them a bit too confusing.\nAfter a few bungled attempts then, I distilled them into a few simple commands.\nThe obvious starting place is to install the Pimoroni libraries:\ncurl https://get.pimoroni.com/pantilthat | bash\nAnd then check that it’s all working:\ncd /home/foo/Pimoroni/pantilthat/examples\u0026lt;br\u0026gt;\u0026lt;/br\u0026gt;python smooth.py\nThe RPi-Cam-Web-Interface is also needed:\nNOTE – Enabling the Camera\nBullseye OS has replaced the camera stack which stopped the raspimjpeg working.\nLegacy camera support should be enabled.\nDo this within raspi-config under Interface.\nIf this shows just “Enable camera” then update raspi-config itself from its menu item.\nThe interface option should now show enable Legacy camera support.\nThe Pi will reboot.\nThe install.sh script will detect a Bullseye OS, set the right PHP version and create a missing directory needed by this software to run.\nTo do this you will need to clone the code from github and enable and run the install script with the following:\ngit clone https://github.com/silvanmelchior/RPi_Cam_Web_Interface.git\u0026lt;br\u0026gt;\u0026lt;/br\u0026gt;cd RPi_Cam_Web_Interface\u0026lt;br\u0026gt;\u0026lt;/br\u0026gt;./install.sh\nConfigure accordingly with the cam folder at ‘/html/wherever-you-like’.\nOK \u0026amp; Wait for ‘Start’.\nLocally, at ‘http://Your-Pi-IP-Address/html/wherever-you-like’ there should be an image.\nAlso on the Pi, in a browser, at URL ‘http://127.0.0.1/html/wherever-you-like’ there should be an image.\nConfigure the pan and tilt Hat for RPi-Cam-Web-Interface This part can be confusing but if the following steps are taken, a working image should appear on a web page of your choice.\nFirstly, rename pipan_off to enable the browser direction arrows (needs a browser refresh):\ncd /var/www/html/wherever-you-like\u0026lt;br\u0026gt;\u0026lt;/br\u0026gt;sudo mv pipan_off pipan_on\nAdd a FIFO pipe:\ncd /var/www/html/wherever-you-like sudo mknod FIFO_pipan p sudo chmod 666 FIFO_pipan\nCreate a pipan-pipe.py on the Pi:\nnano pipan_pipe.py\nThe one I used came from this post: https://forums.pimoroni.com/t/rpi3-camery-pimioroni-adafruit-pan-tilt-rpi-cam-web-interfac/3263/13 The important line is this one:\npipein = open(\u0026quot;/var/www/html/wherever-you-like/FIFO_pipan\u0026quot;, 'r')\nSave it somewhere sensible (mine went in my Home folder).\nCheck that it’s working:\ncd /home/foo python pipan_pipe.py\nEdit /etc/rc.local and add the following line above the exit-command:\n(Change the path to the directory where you saved the pipan-file).\nsudo nano /etc/rc.local python /home/foo/pipan_pipe.py \u0026amp;\nIf you see this, then put the above lines ^^ above it.\n#START RASPIMJPEG SECTION\nFinally, it may need a reboot but the Pan / Tilt HAT should now be operable via a browser.\nConnecting to the outside world As the ‘RPi-Cam-Web-Interface’ comes with a web server, it can serve requests from the internet.\nHowever, on my router port 80 traffic goes to a web server where it determines where to send that request internally.\nAn easier route would be to direct port 80* traffic to the Pi where all of the above magic takes place.\n(This only applies if it was port 80 that was chosen to serve requests when installing ‘RPi-Cam-Web-Interface’.)\nAdding the NeoPixel stick menu I then added a bash menu to control the lights.\nAt the moment all the options link to the Pimoroni examples, apart from the ‘Sleep’ which turns the pixels off and returns the head back to its starting position.\nThe menu #!/bin/bash HEIGHT=15 WIDTH=40 CHOICE_HEIGHT=4 BACKTITLE=\u0026#34;robotcam Neopixel Lights\u0026#34; TITLE=\u0026#34;Neopixel Stick Examples\u0026#34; MENU=\u0026#34;Choose one of the following options:\u0026#34; OPTIONS=(1 \u0026#34;360 White\u0026#34; 2 \u0026#34;360 Rainbow\u0026#34; 3 \u0026#34;Smooth Sweep\u0026#34; 4 \u0026#34;Staggered Sweep\u0026#34; 5 \u0026#34;Sleep\u0026#34;) CHOICE=$(dialog --clear \\ --backtitle \u0026#34;$BACKTITLE\u0026#34; \\ --title \u0026#34;$TITLE\u0026#34; \\ --menu \u0026#34;$MENU\u0026#34; \\ $HEIGHT $WIDTH $CHOICE_HEIGHT \\ \u0026#34;${OPTIONS[@]}\u0026#34; \\ 2\u0026gt;\u0026amp;1 \u0026gt;/dev/tty) clear case $CHOICE in 1) python ~/Pimoroni/pantilthat/examples/grbw.py ;; 2) ~/Pimoroni/pantilthat/examples/neopixels.py ;; 3) ~/Pimoroni/pantilthat/examples/smooth.py ;; 4) ~/Pimoroni/pantilthat/examples/timeout.py ;; 5) ~/Pimoroni/pantilthat/examples/sleep.py ;; esac The ‘Sleep’ bit #!/usr/bin/env python import pantilthat pantilthat.light_mode(pantilthat.WS2812) pantilthat.light_type(pantilthat.GRBW) r, g, b, w = 0, 0, 0, 0 while True: for x in range(18): pantilthat.set_pixel(x, r, g, b, w) pantilthat.show() p = 0.00 t = 0.00 pantilthat.pan(p) pantilthat.tilt(t) I hope that the above helps anyone also looking to do something similar.\nIt’s not a straightforward process, especially getting the ‘RPi-Cam-Web-Interface’ up and running; it is however worth the effort.\nAnd Finally These are both excellent reading and flesh out the above a bit more:\nPimoroni Forums post elinux post All of the Pan-Tilt-HAT code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/robot-pan-tilt-hat-cctv/","summary":"An Internet controlled robot with a pan and tilt CCTV camera? What could go wrong?","title":"Robot Pan / Tilt Hat CCTV"},{"content":"More plotting. As I mentioned in a previous post, I was interested in data collection using an environment sensor, logging that info to a csv file and plotting a graph using gnuplot.\nHowever, the incorrectly labelled graph axes annoyed me so I set about repairing them.\nAlso, I’m not going to revisit how it all sits together as that was done in the previous post .\nIn summary there is:\nA python file that collects data from the Enviro pHAT (i.e. the enviroplot.py)\nA .csv file that the python file writes comma separated values to (i.e. the enviroplot.csv)\nThe gnuplot script that interpret the .csv rows and produce the .png graphs\ntemp_pressure_last24hours.plot\ntemp_pressure_last7days.plot\ntemp_pressure_all.plot\nAn .sh file that wraps the python file and the gnuplot scripts together (i.e. enviroplot.sh)\nA cron job in the Pi then executes the .sh file every 15 minutes (i.e. the pi crontab)\nA cron job in the server then runs every 15 minutes to collect all the .png files (i.e. the server crontab)\nThe problem The main issue with the graph axes was the underlying data in the .csv file (unsurprisingly).\nI thought I was being clever and splitting out the date and time into separate columns, as it turns out that wasn’t so wise.\n(I thought that later analysis such as ‘shown me the average temperatures by hour over time’ might be easier if I split these columns beforehand.)\nThe Python code would need a rewrite anyway to address my error but what about the existing info?\nI had to choose from:\nThe quick way – abandon all the previously collected data and start afresh with the date and time captured correctly\nThe time consuming way – concatenate the existing columns into a format gnuplot could understand easier\nObviously I chose the time consuming way and fired up Excel.\n(I also took the opportunity to rewrite the plotting scripts as I had discovered that gnuplot was capable of creating dual axes graphs.)\nThe data collection bit The Python code that writes to the .csv file now looks like this:\nenviroplot.py #!/usr/bin/env python import csv from datetime import datetime from envirophat import weather dt = datetime.now() DateTime= dt.strftime(\u0026#39;%d-%m-%Y %H:%M:%S\u0026#39;) temp = round(weather.temperature(), 2) baro = round(weather.pressure(unit = \u0026#39;hPa\u0026#39;), 2) csvWrite = DateTime, temp, baro csvFile = open(\u0026#39;/home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39;, \u0026#39;a\u0026#39;) with csvFile: writer = csv.writer(csvFile) writer.writerow(csvWrite) The created .csv file now looks like this, with a single column for Date and Time:\nThe logging bit enviroplot.csv (Example of data collected) DateTime temp baro 23-01-2019 22:15:01 23.12 998.17 23-01-2019 22:30:01 22.89 998.35 23-01-2019 22:45:02 22.54 998.41 23-01-2019 23:00:02 22.2 998.44 23-01-2019 23:15:01 21.9 998.46 The plotting part The gnuplot script that creates a 640×480 dual axes graph in .png format of temperature and pressure over the last 24 hours.\ntemp_pressure_last24hours.plot # Create new plot reset unset key set key off set terminal png medium size 640,480 # csv DateTime format set datafile separator \u0026#34;,\u0026#34; set timefmt \u0026#34;%d-%m-%Y %H:%M:%S\u0026#34; # Graph formatting set title \u0026#34;Temperature and Pressure Over the Last 24 Hours\u0026#34; # set grid # x axis formatting set xdata time set format x \u0026#34;%l%p\u0026#34; # y axis formatting set ytics 1 nomirror tc lt 1 set ylabel \u0026#34;Temperature\u0026#34; tc lt 1 # y2 axis formatting set y2tics 1 nomirror tc lt 2 set y2label \u0026#34;Pressure\u0026#34; tc lt 2 # Plot the data plot \u0026#39;\u0026lt; tail -n 90 /home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39; using 1:2 axes x1y1, \u0026#39;\u0026lt; tail -n 90 /home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39; using 1:3 axes x1y2 with lines The gnuplot script that creates a 640×480 dual axes graph in .png format of temperature and pressure over the last 7 days.\ntemp_pressure_last7days.plot # Create new plot reset unset key set key off set terminal png medium size 640,480 # csv DateTime format set datafile separator \u0026#34;,\u0026#34; set timefmt \u0026#34;%d-%m-%Y %H:%M:%S\u0026#34; # Graph formatting set title \u0026#34;Temperature and Pressure Over the Last 7 Days\u0026#34; # set grid # x axis formatting set xdata time set format x \u0026#34;%a\u0026#34; # y axis formatting set ytics 1 nomirror tc lt 1 set ylabel \u0026#34;Temperature\u0026#34; tc lt 1 # y2 axis formatting set y2tics 5 nomirror tc lt 2 set y2label \u0026#34;Pressure\u0026#34; tc lt 2 # Plot the data plot \u0026#39;\u0026lt; tail -n 672 /home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39; using 1:2 axes x1y1, \u0026#39;\u0026lt; tail -n 672 /home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39; using 1:3 axes x1y2 with lines The gnuplot script that creates a 640×480 dual axes graph in .png format of temperature and pressure over all recorded data.\ntemp_pressure_all.plot # Create new plot reset unset key set key off set terminal png medium size 1280,480 # csv DateTime format set datafile separator \u0026#34;,\u0026#34; set timefmt \u0026#34;%d-%m-%Y %H:%M:%S\u0026#34; # Graph formatting set title \u0026#34;Temperature and Pressure Over All Time\u0026#34; # set grid # x axis formatting set xdata time set format x \u0026#34;%b %y\u0026#34; # y axis formatting set ytics 10 nomirror tc lt 1 set ylabel \u0026#34;Temperature\u0026#34; tc lt 1 # y2 axis formatting set y2tics 10 nomirror tc lt 2 set y2label \u0026#34;Pressure\u0026#34; tc lt 2 # Plot the data plot \u0026#39;/home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39; using 1:2 axes x1y1, \u0026#39;/home/foo/Dev/RasPi/enviroplot/enviroplot.csv\u0026#39; using 1:3 axes x1y2 with lines Automating using cron The .sh file to pull together the enviroplot.py and gnuplot scripts.\nenviroplot.sh #!/bin/sh # This is the script that the cron job will execute # Run the python script to collect the variables python /home/foo/Dev/RasPi/enviroplot/enviroplot.py #### Create the graph images from gnuplot # \u0026#39;Sleep\u0026#39; is there to prevent a conflict sleep 10 gnuplot /home/foo/Dev/RasPi/enviroplot/temp_pressure_all.plot \u0026gt; /home/foo/Dev/RasPi/enviroplot/temp_pressure_all.png sleep 10 gnuplot /home/foo/Dev/RasPi/enviroplot/temp_pressure_last7days.plot \u0026gt; /home/foo/Dev/RasPi/enviroplot/temp_pressure_last7days.png sleep 10 gnuplot /home/foo/Dev/RasPi/enviroplot/temp_pressure_last24hours.plot \u0026gt; /home/foo/Dev/RasPi/enviroplot/temp_pressure_last24hours.png The pi crontab that executes every 15 minutes to run enviroplot.sh.\npi crontab # This is the script that runs the bash script to collect data and create graphs */15 * * * * /home/foo/Dev/Raspi/enviroplot/enviroplot.sh The cron job on the server has changed slightly as it now pulls in all of the .png files and not the named ones as per the previous version.\nserver crontab # Collect the graphs from the \u0026#39;piboard\u0026#39; machine */15 * * * * scp -i ~/.ssh/id_rsa_piboard foo@piboard:/home/foo/Dev/RasPi/enviroplot/*.png /var/www/html/ And finally! This is what the output looks like, snipped from a web page.\nNow that the visible range of data displayed has been shortened you can see that the temperature rises most evenings (when the heating comes on) and the following days weather can be predicted by looking at the pressure graphs.\nTemperature and Pressure Over the Last 24 Hours and Prior 7 Days\nAll of the Enviro pHAT code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/more-data-collection-logging-and-plotting/","summary":"Collecting data, logging to a csv file, plotting using gnuplot and sending to a web server.","title":"More Data Collection Logging and Plotting"},{"content":"Stop missing the bus! Show the timetable on an OLED display!\nI put together a Raspberry Pi and a tiny OLED display to show a bus timetable, showing the departure time of the next bus at my local stop; All built from a ‘Beautiful Soup’ web scrape and not using an API.\nOLED Bus Timetable Overview Not exactly a massive project but a fun introduction for me to the Pimoroni 1.2″ Mono OLED .\nInspired by this post , I thought I’d put that little OLED to some use and see if I could:\nScrape a website to determine when the next bus to town is departing\nThen write this info to the OLED\nThen keep on refreshing\nThe Scripts 22_next.py Shows the amount of time before the next departure from this bus stop.\n#!/usr/bin/env python3 # General import os import time # For the scraping from bs4 import BeautifulSoup import requests # For the oled from PIL import Image from PIL import ImageFont from PIL import ImageDraw from threading import Thread from luma.core.interface.serial import i2c from luma.core.render import canvas from luma.oled.device import sh1106 # Set up OLED oled = sh1106(i2c(port=1, address=0x3C), rotate=2, height=128, width=128) # Load fonts rr_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \u0026#39;fonts\u0026#39;, \u0026#39;Roboto_Regular.ttf\u0026#39;)) rb_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \u0026#39;fonts\u0026#39;, \u0026#39;Roboto_Black.ttf\u0026#39;)) rr_12 = ImageFont.truetype(rr_path, 12) rr_24 = ImageFont.truetype(rr_path, 24) # The main loop that writes to the OLED while True: # Page url to scrape url =\u0026#39;https://www.buses.co.uk/stops/149000006645\u0026#39; # Fetch the content from url page = requests.get(url, timeout=5) # Parse html soup = BeautifulSoup(page.content, \u0026#34;html.parser\u0026#34;) # Extract the html element where the next time expected is stored time_expected = soup.find(class_=\u0026#39;single-visit__time--expected\u0026#39;) # Strip extraneous html next_bus = time_expected.text.strip() # Start to draw to the display background = Image.open(\u0026#34;/home/foo/Dev/Raspi/busses/images/bus.png\u0026#34;).convert(oled.mode) draw = ImageDraw.ImageDraw(background) # Draw the top line draw.rectangle([(0, 0), (128, 20)], fill=\u0026#34;black\u0026#34;) draw.line([(0, 20), (128, 20)], fill=\u0026#34;white\u0026#34;) # Draw the text draw.rectangle([(0, 108), (128, 128)], fill=\u0026#34;black\u0026#34;) draw.text((10, 40), \u0026#34;The next bus is in\u0026#34;, fill=\u0026#34;white\u0026#34;, font=rr_12) draw.text((20, 60), next_bus, fill=\u0026#34;white\u0026#34;, font=rr_24) # Draw the bottom line draw.rectangle([(0, 108), (128, 128)], fill=\u0026#34;black\u0026#34;) draw.line([(0, 108), (128, 108)], fill=\u0026#34;white\u0026#34;) # Display on the OLED oled.display(background) time.sleep(0.05) 22_deps.py Shows the upcoming departures from this bus stop.\n#!/usr/bin/env python3 # General import os import time # For the scraping from bs4 import BeautifulSoup import requests # For the oled from PIL import Image from PIL import ImageFont from PIL import ImageDraw from threading import Thread from luma.core.interface.serial import i2c from luma.core.render import canvas from luma.oled.device import sh1106 # Set up OLED oled = sh1106(i2c(port=1, address=0x3C), rotate=2, height=128, width=128) # Load fonts rr_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \u0026#39;fonts\u0026#39;, \u0026#39;Roboto_Regular.ttf\u0026#39;)) rb_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \u0026#39;fonts\u0026#39;, \u0026#39;Roboto_Black.ttf\u0026#39;)) rr_12 = ImageFont.truetype(rr_path, 12) rr_24 = ImageFont.truetype(rr_path, 24) # The main loop that writes to the OLED while True: # Page url to scrape url =\u0026#39;https://www.buses.co.uk/stops/149000006645\u0026#39; # Fetch the content from url page = requests.get(url, timeout=5) # Parse html soup = BeautifulSoup(page.content, \u0026#34;html.parser\u0026#34;) # Extract the html element where the next times expected are stored for bus_times in soup.findAll(\u0026#39;div\u0026#39;, attrs={\u0026#34;class\u0026#34;:\u0026#34;single-visit__time single-visit__time--expected\u0026#34;}): bus_deps = bus_times.text.strip() # Start to draw to the display background = Image.open(\u0026#34;/home/foo/Dev/Raspi/busses/images/bus.png\u0026#34;).convert(oled.mode) draw = ImageDraw.ImageDraw(background) # Draw the top line draw.rectangle([(0, 0), (128, 20)], fill=\u0026#34;black\u0026#34;) draw.line([(0, 20), (128, 20)], fill=\u0026#34;white\u0026#34;) # Draw the text draw.rectangle([(0, 108), (128, 128)], fill=\u0026#34;black\u0026#34;) draw.text((10, 40), \u0026#34;The next bus is in\u0026#34;, fill=\u0026#34;white\u0026#34;, font=rr_12) draw.text((20, 60), bus_deps, fill=\u0026#34;white\u0026#34;, font=rr_24) # Draw the bottom line draw.rectangle([(0, 108), (128, 128)], fill=\u0026#34;black\u0026#34;) draw.line([(0, 108), (128, 108)], fill=\u0026#34;white\u0026#34;) # Display on the OLED oled.display(background) time.sleep(0.05) continue Well, it works, so now I don’t have to keep looking at the bus app on my phone in a morning…\nOLED Display Showing the Next Bus Departure\nHope this helps someone looking for a simple way to write basic text to the display.\n(It doesn’t have to be a bus timetable that can be written to the OLED display, maybe tide tables or how long until the bin men arrive?)\nAll of the Mono OLED code examples can also be found here .\n","permalink":"https://boffinsblog.github.io/posts/oled-bus-timetable/","summary":"Show the bus timetable on an OLED display!","title":"OLED Bus Timetable"},{"content":"The good kind of plotting I was interested in data collection using an environment sensor, logging that info to a csv file and plotting a graph using gnuplot.\nI have a couple of RaspberryPi HATs and pHATs knocking about and wondered how to collect data from one, and log the data it to a local file and then present that info in an understandable way.\nThe routine below should be a good jumping off point for people to start with and does the following:\nUses python to collect data from a sensor (in my case an Enviro pHAT)\nThen, writes the python output to a comma separated file\nUses gnuplot to turn slices of the .csv file in to graphs, stored locally\nInstructs a web server on a different machine to pull those files into the /var/www folder\nA web page then easily serves these, as they always have the same name.\nSo, putting all of this together:\nThe data collection bit The Enviro pHAT sits on top of a pi Zero WH (named phats). It uses ssh keys for authentication.\nThe pi is connected to the internal network (and therefore the web server) and has a known IP address.\nThe web server is named sheevaplug, it has a single user with no rights allocated. The user has to su to get rights.\nOn the pi there are a few files:\nThe python file that collects data from the Enviro pHAT (enviroplot.py)\nThe .csv file that the python file writes comma separated values to (enviroplot.csv)\nThe gnuplot script(s) that interpret the .csv rows and produce the .png graphs (temp_range.plot, temp_recent.plot)\nThe sh file that wraps the python file and the gnuplot scripts together (enviroplot.sh)\nA cron job then executes the sh file every 15 minutes\nThe logging bit enviroplot.py #!/usr/bin/env python import csv from datetime import datetime from envirophat import weather dt = datetime.now() full_date = dt.strftime(’%Y-%m-%d’) full_time = dt.strftime(’%H:%M:%S’) full_day = dt.strftime(’%A’) year = dt.year month = dt.month date = dt.day hour = dt.hour temp = round(weather.temperature(), 2) baro = round(weather.pressure(unit = ‘hPa’), 2) csvWrite = full_date, full_time, full_day, year, month, date, hour, temp, baro csvFile = open(’/home/foo/Dev/Raspi/enviroplot/enviroplot.csv’, ‘a’) with csvFile: writer = csv.writer(csvFile) writer.writerow(csvWrite) The important bits here are:\nround(xxxx, 2) rounds to two decimal places\nThe ‘a’ at the end of the csvFile line means append. The .csv file must exist first, even if it is empty to start with\nThe logging output enviroplot.csv (Example of data collected) full_date\tfull_time\tfull_day\tyear\tmonth\tdate\thour\ttemp\tbaro 2019-01-23\t22:15:01\tWednesday\t2019\t1\t23\t22\t23.12\t998.17 2019-01-23\t22:30:01\tWednesday\t2019\t1\t23\t22\t22.89\t998.35 2019-01-23\t22:45:02\tWednesday\t2019\t1\t23\t22\t22.54\t998.41 2019-01-23\t23:00:02\tWednesday\t2019\t1\t23\t23\t22.2\t998.44 2019-01-23\t23:15:01\tWednesday\t2019\t1\t23\t23\t21.9\t998.46 I have deliberately collected the full date, full time, day, year, month, date, hour separately as I may want to analyse these elements later and don’t want to have to dissect the .csv string.\n(The final two values are temperature and barometric pressure.)\nThe plotting part The following two .plot examples are the gnuplot scripts.\nGnuplot is an incredibly versatile open source plotting piece of software and is available in the repos.\nsudo apt install gnuplot\nIt is capable of creating 2D and 3D graphs and outputting to a variety of formats.\nYou can even plot ASCII graphs in the terminal with it!\nYou can create a graph with as little as one line, specifying simply where the input data is.\nI’m still learning, so you may have better looking outputs with more practice.\ntemp_range.plot reset unset key set key off set terminal png set datafile separator \u0026#34;,\u0026#34; set xdata time set timefmt \u0026#34;%Y-%m-%dT%H:%M:%S\u0026#34; set format x \u0026#34;%d/%m\u0026#34; set xlabel \u0026#34;Date (day/month)\u0026#34; set ylabel \u0026#34;Temp\u0026#34; set style data points set title \u0026#34;Temp Range\u0026#34; set grid plot \u0026#39;/home/foo/Dev/Raspi/enviroplot/enviroplot.csv\u0026#39; using 1:8 The important bits here are:\n‘plot’ i.e. the location of the .csv file containing data\n‘Using 1:8’ i.e. use data columns 1 \u0026amp; 8\ntemp_recent.plot reset unset key set key off set terminal png set datafile separator \u0026#34;,\u0026#34; set xdata time set timefmt \u0026#34;%Y-%m-%dT%H:%M:%S\u0026#34; set format x \u0026#34;%d/%m\u0026#34; set xlabel \u0026#34;Date (day/month)\u0026#34; set ylabel \u0026#34;Temp\u0026#34; set style data line set title \u0026#34;Recent Temp\u0026#34; set grid plot \u0026#39;\u0026lt; tail -n 672 /home/foo/Dev/Raspi/enviroplot/enviroplot.csv\u0026#39; using 1\u0026amp;2:8 The important bits here are:\n‘tail -n 672’ i.e. take the last 672 data points ( 1 x week at 15 minute intervals)\n‘using 1\u0026amp;2:8’ i.e. concatenate columns 1 \u0026amp; 2 as one data column and column 8 as the other\nAutomating using cron enviroplot.sh #!/bin/sh # This is the script that the cron job will execute # Run the python script to collect the variables python /home/foo/Dev/Raspi/enviroplot/enviroplot.py #### Create the graph images from gnuplot # \u0026#39;Sleep\u0026#39; is there to prevent a conflict gnuplot /home/foo/Dev/Raspi/enviroplot/temp_range.plot \u0026gt; /home/foo/Dev/Raspi/enviroplot/temp_range.png sleep 5 gnuplot /home/foo/Dev/Raspi/enviroplot/temp_recent.plot \u0026gt; /home/foo/Dev/Raspi/enviroplot/temp_recent.png The important bits here are:\nRuns the python script that writes the Enviro pHAT data to the .csv file\nCalls the gnuplot file(s) and outputs it to a named .png file i.e. creates the graph image file(s)\n‘Sleep’ is there as cron will execute both commands at the same time; better to have a slight delay and wait for the first command to finish\nphats crontab # m h dom mon dow command # This is the script that runs the bash script to collect data and create graphs */15 * * * * /home/foo/Dev/Raspi/enviroplot/enviroplot.sh The cron job that runs every 15 minutes to execute the above enviroplot.sh script.\nThe above will get you a working pair of graphs that are refreshed every 15 minutes.\nBut they will be on the pi which may be headless or unattached to any sort of display.\nMuch better to put the images on the interweb where you can watch your house getting hotter when it’s burning.\nThe next two files are on the web server and comprise of:\nA sh script to pull the files from the pi over the local network\nA cron job to do that every 15 minutes\nServing it all up I have to pull the files into the web server, rather than push the files from the pi because, as I said earlier the server only has a single user with no rights and I couldn’t find a way to connect to the server using ssh, then pass a password to get rights. (Probably is a way, but too late now.)\nget_graphs.sh #!/bin/sh # This is the script that the cron job will execute # Collect the graphs from the \u0026#39;phats\u0026#39; machine # \u0026#39;Sleep\u0026#39; is there to prevent a conflict scp -i ~/.ssh/id_rsa_phats foo@phats:/home/foo/Dev/Raspi/enviroplot/temp_range.png /var/www/html/ sleep 5 scp -i ~/.ssh/id_rsa_phats foo@phats:/home/foo/Dev/Raspi/enviroplot/temp_recent.png /var/www/html/ The important bits here are:\nscp is secure copy and allows use of ssh keys\nAlso, I’m using ssh keys for authentication, this prevents having to type my password repeatedly and offers secure copying and logins\n‘Sleep’ is there as cron will execute both commands at the same time; better to have a slight delay and wait for the first command to finish\nsheevaplug crontab # m h dom mon dow command # This is the script that runs the bash script to collect the graphs from the \u0026#39;phats\u0026#39; machine */15 * * * * sh /var/www/html/get_graphs.sh The cron job that runs every 15 minutes to execute the above get_graphs.sh script.\nAnd finally! This is what the output looks like, snipped from a web page.\nTemperature Over the Last Fortnight and All Time\nAlso, I know the axis on the first graph looks a bit odd, as I said I’m just starting out with gnuplot.\nThis method has now been updated , with corrected graphs axes on a dual axis chart, showing temperature and pressure over both the last week and the prior 24 hours.\n","permalink":"https://boffinsblog.github.io/posts/data-collection-logging-and-plotting/","summary":"Data collection using an environment sensor, logging that info to a csv file and plotting a graph using gnuplot.","title":"Data Collection, Logging and Plotting"},{"content":"** WARNING ** The following post contains images of neglect.\nThose of a nervous disposition should read no further.\nMore Pac-Man joy Many years ago I chipped in to the first ever UK Kickstarter, the Picade from Pimoroni, so old it was powered by a RaspberryPi Model B; I decided to dig it out of the cupboard of forgotten tech, dust it off and start refurbishing it.\nBits and pieces used RaspberryPi 3\nPac-Man imagery by @Wheelhorse from this mega-thread and vinyl printed by a local printer Illuminated buttons and joystick ball\nHowever I probably ordered too many buttons…\nPicade loom (Edit: no longer stocked)\nThe sturdier wiring on the new loom is a lot less fragile than the original KickStarter loom\nOn/Off Shim A couple of reasons to add one of these nifty boards, more details further down\nMomentary Push Switch Then main reason to add one of these was to simplify the powering-down process. I also needed one to fit a pre-drilled hole in the rear of the cabinet\nShiny Trim (Edit: no longer stocked)\nFor bling !\n6mm and 12mm cable tidy because I like tidy\nWire and crimps for wiring harnesses because I needed to make some adjustments to where the power went to\nPatience, and I mean lots of patience\nTake one dusty, neglected Picade in need of refurbishing It’s looking a little forlorn but elbow grease and chemicals will help; it’s time for a transformation.\nA Dusty Front View of a Picade\nNot exactly a good advertisement for my dusting skills but it does show that an update is long overdue.\nA Dusty Rear View of a Picade\nOh look, an ancient RaspberryPi Model B but in an original Pimoroni Pibow.\nI’m going to take this opportunity to upgrade that too but first everything needs disassembling.\nAnother Dusty Rear View of a Picade\nAlso, I’m sure there were creatures living in there.\nIt’s amazing how much dust actually collects in there even though it has been living in a cupboard.\nA Dusty Inside View of a Picade\nYou can also see the ghostly outlines of long dead cables.\n(And a possible dead creature carcass next to the middle screw terminal on the left.)\nAnother Dusty Inside View of a Picade\nFirst ablutions Shiny, you can almost see your reflection in there (but not quite).\nHowever, it’s amazing what hot soapy water, some sugar soap and WD40 can do.\nClean Inside View of a Picade\nI even took the Pibow apart and cleaned that otherwise it would have looked incongruous.\nHowever, it turns out that was a pointless exercise as the new RaspberryPi wouldn’t fit into the old Pibow.\nClean Rear View\nAt this point everything still worked.\nThe screen was lit and the Pi was booting.\nBut there was still that strange buzzing sound.\nSo I decided to reflash RetroPie .\nStill no joy, still buzzing.\nSo I decided to update and flash the firmware, MacGyver style.\nRaspi Rats Nest\nOops.\nOf course I borked the firmware.\nSo, time to disassemble completely, clean again, and give my Picade a fresh coat of LED and vinyl love.\nSecond ablutions After complete disassembly, much sugar-soap and WD40 the MDF was looking in better shape.\nClean MDF Parts\nCutting the Pac-Man vinyl for the case I had decided to vinyl wrap the Picade.\nThat however, was a mixed blessing.\nThe Pac-Man wrap looked fantastic but cutting all of the pieces by hand was a chore.\nPac-Man Vinyl Side View\nIt looks fantastic but I can’t emphasise enough how much of a chore this was.\nVinyl Pac-Man Bezel and Controls\nIf I ever do this again, I’d opt for contour cutting to save a few hours.\n(But that would have driven up the cost quite dramatically.)\nVinyl \u0026amp; MDF Pac-Man Side View\nReassembly I reassembled the carcass without a hitch.\nI’m not sure about cutting out the crenels though.\nI’m undecided whether I like this look or not.\nStarting Cabinet Reassembly\nThe marquee is looking good (more about this later).\nStarting Marquee Reassembly\nI even managed to not break the screen whilst disassembling, cleaning and reassembling.\nReassembled Cabinet\nAdding an ‘On-Off Shim’ to the RaspberryPi There are a couple of reasons I added an ‘On/Off Shim’:\nA neat button that performs a clean shutdown\nA way of running the 5V power and ground looms to the LED buttons\nOn/Off Shim\nAlso, ‘Hot Glue’ is related to ‘WD40’ and ‘Gaffer Tape’, right?\nOn/Off Shim in Place\nButton wiring – part 1 Wiring up the LED power and ground looms first.\nAlso, it’s a good job the anode was painted or else I’d never have figured out how to wire the four terminals up.\nFor reference:\nThis is a ‘Picade Maxi’, so has 8 buttons on top.\n6 x Blue buttons for the arcade controls\n2 x red buttons for the volume controls\nTidy Button Wiring\nPicade first boot! Not doing any more wiring unless the damn thing boots !\nI’m keen to start playing some Pac-Man but I really need to make sure that it boots OK.\nFirst Boot !\nButton wiring – part 2 Connecting up the buttons to the Picade control board.\nButton Wiring Close Up\nPicade cable tidy The cable tidy proved to be a mixed blessing.\nWhilst it kept everything tidy it also introduced a slight amount of resistance to the bound wires.\nThat meant that I had to take a bit of care when affixing the console to the cabinet to ensure that the tidied bundles of wires were positioned correctly and not pressing on the underside of the illuminated buttons as this inhibited the button action.\nConsole Wiring\nTesting the buttons Those wires hanging over the top of the cabinet are where the +/-5V from the ‘On/Off Shim’ has been broken out into three separate strands.\nOne strand is for the the eight buttons on the top of the console\nOne strand is for the side and front buttons if they are ever swapped for LED buttons\nThe third strand is to potentially power an illuminated marquee if I ever get around to that\nButton Test\nBreaking out the power (Before being wired up and squished into the cabinet.)\nI deleiberately added extra power strands as I plan in the future to add even more LED goodness.\nOn/Off Shim In Rear Door\nEverything works, so change it All booting and working.\nSo, time to tinker.\nBoot With Buttons Lit\nPicade button swapping I swapped out the yellow buttons for LED ones.\n(Actually, I broke one of the yellow lugs that hold the button in place.)\nSo, all I needed to do was make a positive daisy-chain and a ground daisy-chain.\nThen wire those to the already broken out +/-5V from the ‘On/Off Shim’.\nYou can just about make out the daisy-chain, covered in cable-tidy before it heads off under the screen to the Shim.\nChanging To Yellow Lit Buttons\nPicade rear wiring Lots of tidy cables here, it took a bit of time but I think it was worth it.\nWiring View From The Rear\nI just about manage to fit all of that extra plastic in.\nEven though it looks pretty, I wouldn’t do it again because:\nIt was time consuming\nIt added unnecessary resistance to the loops of wires\nAnother Wiring View From The Rear\nFinishing up I went through updating the firmware ; without breaking anything this time.\nI managed to sort out the sound.\nAnd that was pretty much it, everything else worked straight away, out of the box.\nThe finished Picade with unlit buttons Looks good now that it’s all finished and the trim is in place.\nNot sure how long the trim will stay stuck for though, it doesn’t seem super sticky.\n(Edit: I had to eventually superglue the trim in place as it lifted over time.)\nCompleted Unlit Front View\nI made a conscious decision to leave the black buttons in place.\nThey just look right next to the yellow vinyl.\nSide view of the refurbished Picade Completed Unlit Side View\nSpace-Invaders detail on the refurbished Picade I do love the ‘Space Invader’ detail on the rear.\nCompleted Unlit Rear View\nIt looks good from the back with the ‘Invader’ lit.\nCompleted Lit Rear View\nSome of those Picade buttons don’t look OK The button stickers are lifting slightly.\nI know these buttons are a few millimeters less in diameter than the originals.\nThey are also slightly more convex.\nEtching, instead of stickers may be in order.\nCompleted Unlit Console View\nThe lifting stickers are not too noticeable once the power is on.\nCompleted Lit Console View\nThe yellow illuminated buttons next to the graphics by @Wheelhorse look fantastic.\nCompleted Lit Console Front View\nGratuitous Picade glamour shots after the overhaul Still unsure about those cutaways.\nIf I had to revamp this thing again I’d think about covering the crenelations but that might remove some of the ‘home-made’ charm so I’m not sure.\nCompleted Lit Side View\nAll work was overseen by Professor Screwed.\nCompleted Lit Front View\nAnother shot of the front.\nAnother Completed Lit Front View\nAnd yet another…\nYet Another Completed Lit Front View\nIt is a bit of a transformation but took quite a bit of time.\nBut it does actually work properly now !\nCompleted Lit Side View\nNext steps Load it full of games and then play Pac-Man obviously.\nPlay even more than ‘Pac-Man’ on it !!!\nThings I’d do differently Cough up the extra cash and get the vinyl contour cut next time My poor knife skills are not too noticeable, however I know the mistakes are there.\nCompleted Bad Knife Skills\nCut with more care The image insert needs to extend to the end of the marquee and then it would be a perfect fit.\nMy mistake, but not a biggie and if I add lights up there it’ll need to be opaque anyway.\nCompleted Marquee Short Banner\nNot use unnecessary cable tidy No matter however tidy it all looks.\nDon’t buy cheap tools from Amazon They are a false economy however quickly they get delivered.\n(I’m looking at you crimping tool.)\nFuture Picade mods Because nothing is every truly finished.\nIlluminated Marquee. I’ll probably run the remaining +/-5V from the Shim to a set of lights behind the marquee but for now I’m done tinkering.\nMote. I will find a use for those Mote strips I have in a box somewhere and then there will be even more LED goodness.\nEtched buttons. The stickers will come off, probably sooner rather than later, so I think I’m going to etch them.\nFinished (?) I thinks that’s all for now but you never know when inspiration will strike!\n","permalink":"https://boffinsblog.github.io/posts/refurbishing-a-picade/","summary":"Those of a nervous disposition should read no further.","title":"Refurbishing a Picade"},{"content":"Shouting at a box! So, I finally got my RaspberryPi powered GoogleAIY box to talk nicely to both Google and a Scroll pHAT.\nGoogleAIY a RaspberryPi and a Scroll pHAT Overview I decided to find it a permanent home, but it was in pieces.\nSo I thought that I’d put it all together.\nIn a gorgeous blue Perspex cube.\nAll the bits A Raspberry Pi 3 (not the new one)\nA Google AIY kit from the good ship Pimoroni A FullpHAT from RasPiO (shop no longer online)\nA lovely box from ModMyPi. (Edit: this store has now closed.)\nA proper GPIO extender cable from Cyntech Scroll pHAT – Found in a box and no longer stocked by Pimoroni\nHeaders and standoffs, again found in a box\nAll The Pieces\nThe full pHAT board and headers I soldered a female header in the middle of the full pHAT board so that the GPIO cable can fit relatively flush to the surface of the PCB.\nI know I could have put the female end of the GPIO cable to a male header but my Scroll pHAT had already been soldered with male headers.\nI’m not sure if this will create any future problems but I guess I’m going to find out.\nFull pHAT Board Headers\nExtending the headers There was a few millimeters of space short between the boards so I had to extend the headers and standoffs.\nBecause I only had a header with extra long pins I had to pad the top of each standoff with a bolt to meet the bottom of the GoogleAIY board.\n(The GoogleAIY board is the ‘VoiceHAT’ board in the top right and is yet to be place on top of the full pHAT board.)\nExtending The Headers And GPIO Cable\nA RaspberryPi, a GoogleAIY Board and Scroll pHAT all connected If you look closely you can see the gap between the top of the standoffs and the bottom of the top ‘VoiceHAT’ board.\nI could have trimmed all of the header pins by a bit but didn’t want the extra hassle.\nI’m sure it’ll all be fine.\nVoice HAT And Scroll pHAT\nA RaspberryPi nestling under a GoogleAIY VoiceHAT talking to a Scroll pHAT A side view of the assembled boards before squeezing them into the case.\nThere is quite a collection of boards being stacked atop each other but it all seems stable enough.\nBridging The Gap Between Headers\nTesting the RaspberryPi and GoogleAIY VoiceHAT To make sure that both the AIY and Scroll pHAT processes were working, the whole thing has been powered up and is connected to the network via WiFi.\nVNC is running on the laptop so I can see the GoogleAIY desktop.\n(There is a handy shortcut to ‘check_audio.py’ file on the desktop.)\nThe result: I get to hear and respond to the Google voice prompts and can check both the microphone input and speaker output.\nTesting Testing\nWill it all fit? There’s not a lot of room in there especially as the LED button has to be squeezed in as well.\nBut I’m sure it will all fit.\nWill It All Fit?\nA longer Scroll pHAT cable? The Hat isn’t sitting quite where I want it to, so it’s time to disassemble and fit a longer GPIO cable.\nIt was all a bit of a squeeze and of course I managed to ding one of the acrylic clips.\n(The people at ModMyPi sent me a replacement FOC, which was really kind of them.)\nLonger Cable Needed\nFinally, my RaspberryPi GoogleAIY box talking to both Google and a Scroll pHAT and blinking away! And here is the finished article running the test-all.py script.\nFinished !\nThe Scroll pHAT code At the moment I’m running a very simple clock script but in the future maybe I could run some animations ?\n#!/usr/bin/env python import sys import time import scrollphat brightness = scrollphat.set_brightness(5) while True: try: scrollphat.write_string(time.strftime(\u0026#34;%H:%M \u0026#34;)) scrollphat.scroll(1) time.sleep(0.5) except KeyboardInterrupt: scrollphat.clear() sys.exit(-1) Also, all of the Scroll pHAT code examples can also be found at .\nUpdate to ‘GoogleAIY a RaspberryPi and a Scroll pHAT’ : May 2023 This has now been running for 7 years flawlessly.\nThe only things I have ever done are to re-flash it once for a Debian update (to Buster).\nThen update the software occasionally with an apt-get update\nHowever there are a couple of things that I’ll tinker around with eventually:\nTurn off the flashing blue light whilst it is in listening mode, it get very annoying\nSomehow activate a playlist, the speaker seems as if it would cope OK-ish with basic audio out\n","permalink":"https://boffinsblog.github.io/posts/google-aiy-a-raspberrypi-and-a-scroll-phat/","summary":"I got my RaspberryPi powered GoogleAIY box to talk nicely to both Google and a Scroll pHAT.","title":"GoogleAIY a RaspberryPi and a Scroll pHAT"},{"content":"Asking the important questions Based on my post about logging and plotting the temperature, it\u0026rsquo;s easy to spot if the house is on fire.\nTemperate and Pressure Over the Last 24 Hours Temperate and Pressure Over the Last 7 Days Temperate and Pressure Over All Time ","permalink":"https://boffinsblog.github.io/page/is-my-house-burning/","summary":"Is My House Burning?","title":"Is My House Burning?"}]